{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
=======
   "execution_count": 1,
>>>>>>> c7d8f121b7480536c36d2f93407d48d1fe71c46d
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foldername : 0 - 20 파일\n",
      "Foldername : 1 - 20 파일\n",
      "Foldername : 2 - 20 파일\n",
      "Foldername : 3 - 20 파일\n",
      "Foldername : 4 - 20 파일\n",
      "Foldername : 5 - 20 파일\n",
      "X_data : (64734, 20)\n",
      "Y_label : (64734, 6)\n",
      "6 개의 클래스!!\n",
      "X_train : (48550, 20)\n",
      "Y_train : (48550, 6)\n",
      "X_test : (16184, 20)\n",
      "Y_test : (16184, 6)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import pyaudio #마이크를 사용하기 위한 라이브러리\n",
    "import matplotlib.pyplot as plt #그래프\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "DATA_PATH = \"./data\"\n",
    "X_train = []\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "tf_classes = 0\n",
    "\n",
    "def load_wave_generator(path): \n",
    "       \n",
    "    batch_waves = []\n",
    "    labels = []\n",
    "    X_data = []\n",
    "    Y_label = []    \n",
    "    global X_train, X_test, Y_train, Y_test, tf_classes\n",
    "    \n",
    "    folders = os.listdir(path)\n",
    "\n",
    "    for folder in folders:\n",
    "        if not os.path.isdir(path):continue #폴더가 아니면 continue                   \n",
    "        files = os.listdir(path+\"/\"+folder)        \n",
    "        print(\"Foldername :\",folder,\"-\",len(files),\"파일\")\n",
    "        #폴더 이름과 그 폴더에 속하는 파일 갯수 출력\n",
    "        for wav in files:\n",
    "            if not wav.endswith(\".wav\"):continue\n",
    "            else:               \n",
    "                #print(\"Filename :\",wav)#.wav 파일이 아니면 continue\n",
    "                y, sr = librosa.load(path+\"/\"+folder+\"/\"+wav)\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "              \n",
    "                X_data.extend(mfcc)\n",
    "               # print(len(mfcc))\n",
    "                \n",
    "                label = [0 for i in range(len(folders))]\n",
    "                label[tf_classes] = 1\n",
    "                \n",
    "                for i in range(len(mfcc)):\n",
    "                    Y_label.append(label)\n",
    "                #print(Y_label)\n",
    "        tf_classes = tf_classes+1\n",
    "    #end loop\n",
    "    print(\"X_data :\",np.shape(X_data))\n",
    "    print(\"Y_label :\",np.shape(Y_label))\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(np.array(X_data), np.array(Y_label))\n",
    "\n",
    "    xy = (X_train, X_test, Y_train, Y_test)\n",
    "    np.save(\"./data.npy\",xy)\n",
    "\n",
    "load_wave_generator(DATA_PATH)\n",
    "\n",
    "\n",
    "print(tf_classes,\"개의 클래스!!\")\n",
    "print(\"X_train :\",np.shape(X_train))\n",
    "print(\"Y_train :\",np.shape(Y_train))\n",
    "print(\"X_test :\",np.shape(X_test))\n",
    "print(\"Y_test :\",np.shape(Y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 6,
>>>>>>> c7d8f121b7480536c36d2f93407d48d1fe71c46d
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 2.162456155\n",
      "Epoch: 0001 cost = 1.912202418\n",
      "Epoch: 0002 cost = 1.860998034\n",
      "Epoch: 0003 cost = 1.841403484\n",
      "Epoch: 0004 cost = 1.828666687\n",
      "Epoch: 0005 cost = 1.823105276\n",
      "Epoch: 0006 cost = 1.817907512\n",
      "Epoch: 0007 cost = 1.813428342\n",
      "Epoch: 0008 cost = 1.811275125\n",
      "Epoch: 0009 cost = 1.807278156\n",
      "Epoch: 0010 cost = 1.802316904\n",
      "Epoch: 0011 cost = 1.800757885\n",
      "Epoch: 0012 cost = 1.793956816\n",
      "Epoch: 0013 cost = 1.787991464\n",
      "Epoch: 0014 cost = 1.778402567\n",
      "Epoch: 0015 cost = 1.770873129\n",
      "Epoch: 0016 cost = 1.756152093\n",
      "Epoch: 0017 cost = 1.742222488\n",
      "Epoch: 0018 cost = 1.724554360\n",
      "Epoch: 0019 cost = 1.713430047\n",
      "Epoch: 0020 cost = 1.702734113\n",
      "Epoch: 0021 cost = 1.695397854\n",
      "Epoch: 0022 cost = 1.687533736\n",
      "Epoch: 0023 cost = 1.681050062\n",
      "Epoch: 0024 cost = 1.672319949\n",
      "Epoch: 0025 cost = 1.666085601\n",
      "Epoch: 0026 cost = 1.659467041\n",
      "Epoch: 0027 cost = 1.650155902\n",
      "Epoch: 0028 cost = 1.640081108\n",
      "Epoch: 0029 cost = 1.634667397\n",
      "Epoch: 0030 cost = 1.620991051\n",
      "Epoch: 0031 cost = 1.601689279\n",
      "Epoch: 0032 cost = 1.584913552\n",
      "Epoch: 0033 cost = 1.563389897\n",
      "Epoch: 0034 cost = 1.546086133\n",
      "Epoch: 0035 cost = 1.523791790\n",
      "Epoch: 0036 cost = 1.502641022\n",
      "Epoch: 0037 cost = 1.482270241\n",
      "Epoch: 0038 cost = 1.464087486\n",
      "Epoch: 0039 cost = 1.445142388\n",
      "Epoch: 0040 cost = 1.428625047\n",
      "Epoch: 0041 cost = 1.414613068\n",
      "Epoch: 0042 cost = 1.389424384\n",
      "Epoch: 0043 cost = 1.376640439\n",
      "Epoch: 0044 cost = 1.353186071\n",
      "Epoch: 0045 cost = 1.338006079\n",
      "Epoch: 0046 cost = 1.315107226\n",
      "Epoch: 0047 cost = 1.295447230\n",
      "Epoch: 0048 cost = 1.276891351\n",
      "Epoch: 0049 cost = 1.256879151\n",
      "Epoch: 0050 cost = 1.238395751\n",
      "Epoch: 0051 cost = 1.216108203\n",
      "Epoch: 0052 cost = 1.199053884\n",
      "Epoch: 0053 cost = 1.186497033\n",
      "Epoch: 0054 cost = 1.168317020\n",
      "Epoch: 0055 cost = 1.155148566\n",
      "Epoch: 0056 cost = 1.136558533\n",
      "Epoch: 0057 cost = 1.121679962\n",
      "Epoch: 0058 cost = 1.108604789\n",
      "Epoch: 0059 cost = 1.088398457\n",
      "Epoch: 0060 cost = 1.075206399\n",
      "Epoch: 0061 cost = 1.057998300\n",
      "Epoch: 0062 cost = 1.046340525\n",
      "Epoch: 0063 cost = 1.029773712\n",
      "Epoch: 0064 cost = 1.019665956\n",
      "Epoch: 0065 cost = 1.003380597\n",
      "Epoch: 0066 cost = 0.987692326\n",
      "Epoch: 0067 cost = 0.975809187\n",
      "Epoch: 0068 cost = 0.959769279\n",
      "Epoch: 0069 cost = 0.952131301\n",
      "Epoch: 0070 cost = 0.940452129\n",
      "Epoch: 0071 cost = 0.921086073\n",
      "Epoch: 0072 cost = 0.905325890\n",
      "Epoch: 0073 cost = 0.890175164\n",
      "Epoch: 0074 cost = 0.874192834\n",
      "Epoch: 0075 cost = 0.858830541\n",
      "Epoch: 0076 cost = 0.842758119\n",
      "Epoch: 0077 cost = 0.828051537\n",
      "Epoch: 0078 cost = 0.813095659\n",
      "Epoch: 0079 cost = 0.796023965\n",
      "Epoch: 0080 cost = 0.787162304\n",
      "Epoch: 0081 cost = 0.766498476\n",
      "Epoch: 0082 cost = 0.749727309\n",
      "Epoch: 0083 cost = 0.739493668\n",
      "Epoch: 0084 cost = 0.722319007\n",
      "Epoch: 0085 cost = 0.706936747\n",
      "Epoch: 0086 cost = 0.693224281\n",
      "Epoch: 0087 cost = 0.681966215\n",
      "Epoch: 0088 cost = 0.671577513\n",
      "Epoch: 0089 cost = 0.655295074\n",
      "Epoch: 0090 cost = 0.638978720\n",
      "Epoch: 0091 cost = 0.632710636\n",
      "Epoch: 0092 cost = 0.624992877\n",
      "Epoch: 0093 cost = 0.617479116\n",
      "Epoch: 0094 cost = 0.602448404\n",
      "Epoch: 0095 cost = 0.596873313\n",
      "Epoch: 0096 cost = 0.595098495\n",
      "Epoch: 0097 cost = 0.578449905\n",
      "Epoch: 0098 cost = 0.571543843\n",
      "Epoch: 0099 cost = 0.567800939\n",
      "Epoch: 0100 cost = 0.559286892\n",
      "Epoch: 0101 cost = 0.553914756\n",
      "Epoch: 0102 cost = 0.544114828\n",
      "Epoch: 0103 cost = 0.540425330\n",
      "Epoch: 0104 cost = 0.533038944\n",
      "Epoch: 0105 cost = 0.533998013\n",
      "Epoch: 0106 cost = 0.524257749\n",
      "Epoch: 0107 cost = 0.518671960\n",
      "Epoch: 0108 cost = 0.516256988\n",
      "Epoch: 0109 cost = 0.507688820\n",
      "Epoch: 0110 cost = 0.500279039\n",
      "Epoch: 0111 cost = 0.498276353\n",
      "Epoch: 0112 cost = 0.494748622\n",
      "Epoch: 0113 cost = 0.489067823\n",
      "Epoch: 0114 cost = 0.489689589\n",
      "Epoch: 0115 cost = 0.481739625\n",
      "Epoch: 0116 cost = 0.471573219\n",
      "Epoch: 0117 cost = 0.474654302\n",
      "Epoch: 0118 cost = 0.473153397\n",
      "Epoch: 0119 cost = 0.466660574\n",
      "Epoch: 0120 cost = 0.465406671\n",
      "Epoch: 0121 cost = 0.458557174\n",
      "Epoch: 0122 cost = 0.457336247\n",
      "Epoch: 0123 cost = 0.454496309\n",
      "Epoch: 0124 cost = 0.445772871\n",
      "Epoch: 0125 cost = 0.446613163\n",
      "Epoch: 0126 cost = 0.441311806\n",
      "Epoch: 0127 cost = 0.441893265\n",
      "Epoch: 0128 cost = 0.439753950\n",
      "Epoch: 0129 cost = 0.438562453\n",
      "Epoch: 0130 cost = 0.429120198\n",
      "Epoch: 0131 cost = 0.430930436\n",
      "Epoch: 0132 cost = 0.427777141\n",
      "Epoch: 0133 cost = 0.426215097\n",
      "Epoch: 0134 cost = 0.421841547\n",
      "Epoch: 0135 cost = 0.414835289\n",
      "Epoch: 0136 cost = 0.422461346\n",
      "Epoch: 0137 cost = 0.414506152\n",
      "Epoch: 0138 cost = 0.414886057\n",
      "Epoch: 0139 cost = 0.414834172\n",
      "Epoch: 0140 cost = 0.411203399\n",
      "Epoch: 0141 cost = 0.410626978\n",
      "Epoch: 0142 cost = 0.406088665\n",
      "Epoch: 0143 cost = 0.408406809\n",
      "Epoch: 0144 cost = 0.402308494\n",
      "Epoch: 0145 cost = 0.401288450\n",
      "Epoch: 0146 cost = 0.400449157\n",
      "Epoch: 0147 cost = 0.398961782\n",
      "Epoch: 0148 cost = 0.400161549\n",
      "Epoch: 0149 cost = 0.394862756\n",
      "Epoch: 0150 cost = 0.393760502\n",
      "Epoch: 0151 cost = 0.392286941\n",
      "Epoch: 0152 cost = 0.388359383\n",
      "Epoch: 0153 cost = 0.389286175\n",
      "Epoch: 0154 cost = 0.391052112\n",
      "Epoch: 0155 cost = 0.385397211\n",
      "Epoch: 0156 cost = 0.383855805\n",
      "Epoch: 0157 cost = 0.382398993\n",
      "Epoch: 0158 cost = 0.380000740\n",
      "Epoch: 0159 cost = 0.380869806\n",
      "Epoch: 0160 cost = 0.379655346\n",
      "Epoch: 0161 cost = 0.376097754\n",
      "Epoch: 0162 cost = 0.377378479\n",
      "Epoch: 0163 cost = 0.379104570\n",
      "Epoch: 0164 cost = 0.375478968\n",
      "Epoch: 0165 cost = 0.374156952\n",
      "Epoch: 0166 cost = 0.370835930\n",
      "Epoch: 0167 cost = 0.369597822\n",
      "Epoch: 0168 cost = 0.367379174\n",
      "Epoch: 0169 cost = 0.368464917\n",
      "Epoch: 0170 cost = 0.363620058\n",
      "Epoch: 0171 cost = 0.366076872\n",
      "Epoch: 0172 cost = 0.362088203\n",
      "Epoch: 0173 cost = 0.362126842\n",
      "Epoch: 0174 cost = 0.361801922\n",
      "Epoch: 0175 cost = 0.360500902\n",
      "Epoch: 0176 cost = 0.357528359\n",
      "Epoch: 0177 cost = 0.356747344\n",
      "Epoch: 0178 cost = 0.358469039\n",
      "Epoch: 0179 cost = 0.359307259\n",
      "Epoch: 0180 cost = 0.354927674\n",
      "Epoch: 0181 cost = 0.353668988\n",
      "Epoch: 0182 cost = 0.352841601\n",
      "Epoch: 0183 cost = 0.350711584\n",
      "Epoch: 0184 cost = 0.350336328\n",
      "Epoch: 0185 cost = 0.349979535\n",
      "Epoch: 0186 cost = 0.349820837\n",
      "Epoch: 0187 cost = 0.348075777\n",
      "Epoch: 0188 cost = 0.349065319\n",
      "Epoch: 0189 cost = 0.341296136\n",
      "Epoch: 0190 cost = 0.344590157\n",
      "Epoch: 0191 cost = 0.342734471\n",
      "Epoch: 0192 cost = 0.339876845\n",
      "Epoch: 0193 cost = 0.342092782\n",
      "Epoch: 0194 cost = 0.340133801\n",
      "Epoch: 0195 cost = 0.339283422\n",
      "Epoch: 0196 cost = 0.341790378\n",
      "Epoch: 0197 cost = 0.336505368\n",
      "Epoch: 0198 cost = 0.338646814\n",
      "Epoch: 0199 cost = 0.335505560\n",
      "Accuracy:  0.9054004\n",
      "Learning Finished!\n"
=======
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'set_random_seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-552bce962e3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m777\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtraining_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'set_random_seed'"
>>>>>>> c7d8f121b7480536c36d2f93407d48d1fe71c46d
     ]
    }
   ],
   "source": [
    "##################  화자인식 NN 버전 ##################\n",
    "X_train, X_test, Y_train, Y_test = np.load(\"./data.npy\")\n",
    "X_train = X_train.astype(\"float\")\n",
    "X_test = X_test.astype(\"float\")\n",
    "\n",
<<<<<<< HEAD
    "# v1\n",
    "tf.reset_default_graph() # 기존에 생성된 graph를 모두 삭제하고, reset시켜 중복되는 것을 막아준다. \n",
    "                         # context가 유지되는 주피터에서는 사용해야한다.\n",
=======
    "tf.compat.v1.reset_default_graph() \n",
>>>>>>> c7d8f121b7480536c36d2f93407d48d1fe71c46d
    "tf.set_random_seed(777)\n",
    "learning_rate = 0.001\n",
    "training_epochs = 200\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "sd = 1 / np.sqrt(13) # standard deviation 표준편차(표본표준편차라 1/root(n))\n",
    "\n",
    "#mfcc의 기본은 20\n",
    "# 20ms일 때216은 각 mfcc feature의 열이 216\n",
    "X = tf.placeholder(tf.float32, [None, 20])\n",
    "Y = tf.placeholder(tf.float32, [None, tf_classes])\n",
    "\n",
    "#1차 히든레이어\n",
    "W1 = tf.get_variable(\"w1\", shape=[20, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b1\")\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1) # 1차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "# 2차 히든 레이어\n",
    "W2 = tf.get_variable(\"w2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b2\")\n",
    "L2 = tf.nn.tanh(tf.matmul(L1, W2) + b2) # 2차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "\n",
    "# 3차 히든 레이어\n",
    "W3 = tf.get_variable(\"w3\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b3\")\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3) # 3차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "# 4차 히든 레이어\n",
    "W4 = tf.get_variable(\"w4\", shape=[256, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b4\")\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4) # 4차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
    "\n",
    "# 5차 히든 레이어\n",
    "W5 = tf.get_variable(\"w5\", shape=[128, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b5\")\n",
    "L5 = tf.nn.relu(tf.matmul(L4, W5) + b5) # 5차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L5 = tf.nn.dropout(L5, keep_prob = keep_prob)\n",
    "\n",
    "# 6차 히든 레이어\n",
    "W6 = tf.get_variable(\"w6\", shape=[128, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b6\")\n",
    "L6 = tf.nn.relu(tf.matmul(L5, W6) + b6) # 6차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L6 = tf.nn.dropout(L6, keep_prob = keep_prob)\n",
    "\n",
    "# 7차 히든 레이어\n",
    "W7 = tf.get_variable(\"w7\", shape=[128, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b7\")\n",
    "L7 = tf.nn.relu(tf.matmul(L6, W7) + b7) # 7차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L7 = tf.nn.dropout(L7, keep_prob = keep_prob)\n",
    "\n",
    "# 최종 레이어\n",
    "W8 = tf.get_variable(\"w8\", shape=[128, tf_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([tf_classes], mean=0, stddev=sd), name=\"b8\")\n",
    "hypothesis = tf.matmul(L7, W8) + b8\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "\n",
    "\n",
    "batch_size=1\n",
    "x_len = len(X_train)\n",
    "\n",
    "#짝수\n",
    "if(x_len%2==0):\n",
    "    batch_size = 2\n",
    "elif(x_len%3==0):\n",
    "    batch_size = 3\n",
    "elif(x_len%4==0):\n",
    "    batch_size = 4\n",
    "else:\n",
    "    batch_size = 1\n",
    "\n",
    "split_X = np.split(X_train,batch_size)\n",
    "split_Y = np.split(Y_train,batch_size)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.335050747\n",
      "Epoch: 0001 cost = 0.335953116\n",
      "Epoch: 0002 cost = 0.334738567\n",
      "Epoch: 0003 cost = 0.331572860\n",
      "Epoch: 0004 cost = 0.332948655\n",
      "Epoch: 0005 cost = 0.331598848\n",
      "Epoch: 0006 cost = 0.331848577\n",
      "Epoch: 0007 cost = 0.330937520\n",
      "Epoch: 0008 cost = 0.333299801\n",
      "Epoch: 0009 cost = 0.328576714\n",
      "Epoch: 0010 cost = 0.324837461\n",
      "Epoch: 0011 cost = 0.322993219\n",
      "Epoch: 0012 cost = 0.323713928\n",
      "Epoch: 0013 cost = 0.326472074\n",
      "Epoch: 0014 cost = 0.322893634\n",
      "Epoch: 0015 cost = 0.323014125\n",
      "Epoch: 0016 cost = 0.323369890\n",
      "Epoch: 0017 cost = 0.319723368\n",
      "Epoch: 0018 cost = 0.317210168\n",
      "Epoch: 0019 cost = 0.319675833\n",
      "Epoch: 0020 cost = 0.321207285\n",
      "Epoch: 0021 cost = 0.320268795\n",
      "Epoch: 0022 cost = 0.318391904\n",
      "Epoch: 0023 cost = 0.316702217\n",
      "Epoch: 0024 cost = 0.317061111\n",
      "Epoch: 0025 cost = 0.318572730\n",
      "Epoch: 0026 cost = 0.313547224\n",
      "Epoch: 0027 cost = 0.312528417\n",
      "Epoch: 0028 cost = 0.313244760\n",
      "Epoch: 0029 cost = 0.310446948\n",
      "Epoch: 0030 cost = 0.314588174\n",
      "Epoch: 0031 cost = 0.308138385\n",
      "Epoch: 0032 cost = 0.310060352\n",
      "Epoch: 0033 cost = 0.311950624\n",
      "Epoch: 0034 cost = 0.314539015\n",
      "Epoch: 0035 cost = 0.307960734\n",
      "Epoch: 0036 cost = 0.304464683\n",
      "Epoch: 0037 cost = 0.308552206\n",
      "Epoch: 0038 cost = 0.310864255\n",
      "Epoch: 0039 cost = 0.307113826\n",
      "Epoch: 0040 cost = 0.310083061\n",
      "Epoch: 0041 cost = 0.305452645\n",
      "Epoch: 0042 cost = 0.304986462\n",
      "Epoch: 0043 cost = 0.305017680\n",
      "Epoch: 0044 cost = 0.307081535\n",
      "Epoch: 0045 cost = 0.301689565\n",
      "Epoch: 0046 cost = 0.300430313\n",
      "Epoch: 0047 cost = 0.301270291\n",
      "Epoch: 0048 cost = 0.299207211\n",
      "Epoch: 0049 cost = 0.303332701\n",
      "Epoch: 0050 cost = 0.304919928\n",
      "Epoch: 0051 cost = 0.297584474\n",
      "Epoch: 0052 cost = 0.303412199\n",
      "Epoch: 0053 cost = 0.301479369\n",
      "Epoch: 0054 cost = 0.296995744\n",
      "Epoch: 0055 cost = 0.294390127\n",
      "Epoch: 0056 cost = 0.297912434\n",
      "Epoch: 0057 cost = 0.295415506\n",
      "Epoch: 0058 cost = 0.295541018\n",
      "Epoch: 0059 cost = 0.297455341\n",
      "Epoch: 0060 cost = 0.297879308\n",
      "Epoch: 0061 cost = 0.294534773\n",
      "Epoch: 0062 cost = 0.294574976\n",
      "Epoch: 0063 cost = 0.294402733\n",
      "Epoch: 0064 cost = 0.295066804\n",
      "Epoch: 0065 cost = 0.290434808\n",
      "Epoch: 0066 cost = 0.294822484\n",
      "Epoch: 0067 cost = 0.292280659\n",
      "Epoch: 0068 cost = 0.292911842\n",
      "Epoch: 0069 cost = 0.293529004\n",
      "Epoch: 0070 cost = 0.288875744\n",
      "Epoch: 0071 cost = 0.292281896\n",
      "Epoch: 0072 cost = 0.287766322\n",
      "Epoch: 0073 cost = 0.287592635\n",
      "Epoch: 0074 cost = 0.285870239\n",
      "Epoch: 0075 cost = 0.287197053\n",
      "Epoch: 0076 cost = 0.290343940\n",
      "Epoch: 0077 cost = 0.288631603\n",
      "Epoch: 0078 cost = 0.287552938\n",
      "Epoch: 0079 cost = 0.287896141\n",
      "Epoch: 0080 cost = 0.284642443\n",
      "Epoch: 0081 cost = 0.281354785\n",
      "Epoch: 0082 cost = 0.285556838\n",
      "Epoch: 0083 cost = 0.285332486\n",
      "Epoch: 0084 cost = 0.284171194\n",
      "Epoch: 0085 cost = 0.284552738\n",
      "Epoch: 0086 cost = 0.285675913\n",
      "Epoch: 0087 cost = 0.283286154\n",
      "Epoch: 0088 cost = 0.284266308\n",
      "Epoch: 0089 cost = 0.282158226\n",
      "Epoch: 0090 cost = 0.279745758\n",
      "Epoch: 0091 cost = 0.278730199\n",
      "Epoch: 0092 cost = 0.280242041\n",
      "Epoch: 0093 cost = 0.277163520\n",
      "Epoch: 0094 cost = 0.280577078\n",
      "Epoch: 0095 cost = 0.277464196\n",
      "Epoch: 0096 cost = 0.277093798\n",
      "Epoch: 0097 cost = 0.275471523\n",
      "Epoch: 0098 cost = 0.276854962\n",
      "Epoch: 0099 cost = 0.276043490\n",
      "Epoch: 0100 cost = 0.273440942\n",
      "Epoch: 0101 cost = 0.276667416\n",
      "Epoch: 0102 cost = 0.276453078\n",
      "Epoch: 0103 cost = 0.276039377\n",
      "Epoch: 0104 cost = 0.276996598\n",
      "Epoch: 0105 cost = 0.272173271\n",
      "Epoch: 0106 cost = 0.275720760\n",
      "Epoch: 0107 cost = 0.272877634\n",
      "Epoch: 0108 cost = 0.274522915\n",
      "Epoch: 0109 cost = 0.273156196\n",
      "Epoch: 0110 cost = 0.274335191\n",
      "Epoch: 0111 cost = 0.270193785\n",
      "Epoch: 0112 cost = 0.275113598\n",
      "Epoch: 0113 cost = 0.267596260\n",
      "Epoch: 0114 cost = 0.271740884\n",
      "Epoch: 0115 cost = 0.266705334\n",
      "Epoch: 0116 cost = 0.274329841\n",
      "Epoch: 0117 cost = 0.273659945\n",
      "Epoch: 0118 cost = 0.270103768\n",
      "Epoch: 0119 cost = 0.267883807\n",
      "Epoch: 0120 cost = 0.267714143\n",
      "Epoch: 0121 cost = 0.269723162\n",
      "Epoch: 0122 cost = 0.270437896\n",
      "Epoch: 0123 cost = 0.267310306\n",
      "Epoch: 0124 cost = 0.271193087\n",
      "Epoch: 0125 cost = 0.266374886\n",
      "Epoch: 0126 cost = 0.267656147\n",
      "Epoch: 0127 cost = 0.268523782\n",
      "Epoch: 0128 cost = 0.266477108\n",
      "Epoch: 0129 cost = 0.269216076\n",
      "Epoch: 0130 cost = 0.265194103\n",
      "Epoch: 0131 cost = 0.264605850\n",
      "Epoch: 0132 cost = 0.267287895\n",
      "Epoch: 0133 cost = 0.265134439\n",
      "Epoch: 0134 cost = 0.264176294\n",
      "Epoch: 0135 cost = 0.263330802\n",
      "Epoch: 0136 cost = 0.258727729\n",
      "Epoch: 0137 cost = 0.261978984\n",
      "Epoch: 0138 cost = 0.262319073\n",
      "Epoch: 0139 cost = 0.263492018\n",
      "Epoch: 0140 cost = 0.262584731\n",
      "Epoch: 0141 cost = 0.261064038\n",
      "Epoch: 0142 cost = 0.259281322\n",
      "Epoch: 0143 cost = 0.261515558\n",
      "Epoch: 0144 cost = 0.264240891\n",
      "Epoch: 0145 cost = 0.258896276\n",
      "Epoch: 0146 cost = 0.260972247\n",
      "Epoch: 0147 cost = 0.256517708\n",
      "Epoch: 0148 cost = 0.262368500\n",
      "Epoch: 0149 cost = 0.259141341\n",
      "Epoch: 0150 cost = 0.260886967\n",
      "Epoch: 0151 cost = 0.257601231\n",
      "Epoch: 0152 cost = 0.257278696\n",
      "Epoch: 0153 cost = 0.260993153\n",
      "Epoch: 0154 cost = 0.258578509\n",
      "Epoch: 0155 cost = 0.256533846\n",
      "Epoch: 0156 cost = 0.256326556\n",
      "Epoch: 0157 cost = 0.257266551\n",
      "Epoch: 0158 cost = 0.257465482\n",
      "Epoch: 0159 cost = 0.256274760\n",
      "Epoch: 0160 cost = 0.258141786\n",
      "Epoch: 0161 cost = 0.253620028\n",
      "Epoch: 0162 cost = 0.255769446\n",
      "Epoch: 0163 cost = 0.254217699\n",
      "Epoch: 0164 cost = 0.254848927\n",
      "Epoch: 0165 cost = 0.255971372\n",
      "Epoch: 0166 cost = 0.255697235\n",
      "Epoch: 0167 cost = 0.256127462\n",
      "Epoch: 0168 cost = 0.250230290\n",
      "Epoch: 0169 cost = 0.256918475\n",
      "Epoch: 0170 cost = 0.252802528\n",
      "Epoch: 0171 cost = 0.250446662\n",
      "Epoch: 0172 cost = 0.251427084\n",
      "Epoch: 0173 cost = 0.252212569\n",
      "Epoch: 0174 cost = 0.249922052\n",
      "Epoch: 0175 cost = 0.250889778\n",
      "Epoch: 0176 cost = 0.251048848\n",
      "Epoch: 0177 cost = 0.252966776\n",
      "Epoch: 0178 cost = 0.253429756\n",
      "Epoch: 0179 cost = 0.252900615\n",
      "Epoch: 0180 cost = 0.249331065\n",
      "Epoch: 0181 cost = 0.251351967\n",
      "Epoch: 0182 cost = 0.251005828\n",
      "Epoch: 0183 cost = 0.248179995\n",
      "Epoch: 0184 cost = 0.246052541\n",
      "Epoch: 0185 cost = 0.247767694\n",
      "Epoch: 0186 cost = 0.247777298\n",
      "Epoch: 0187 cost = 0.248146676\n",
      "Epoch: 0188 cost = 0.248362578\n",
      "Epoch: 0189 cost = 0.245674416\n",
      "Epoch: 0190 cost = 0.247702315\n",
      "Epoch: 0191 cost = 0.250636011\n",
      "Epoch: 0192 cost = 0.248965621\n",
      "Epoch: 0193 cost = 0.244998820\n",
      "Epoch: 0194 cost = 0.243898965\n",
      "Epoch: 0195 cost = 0.248020127\n",
      "Epoch: 0196 cost = 0.248254493\n",
      "Epoch: 0197 cost = 0.245689206\n",
      "Epoch: 0198 cost = 0.244621806\n",
      "Epoch: 0199 cost = 0.245555863\n",
      "Accuracy:  0.9195502\n",
      "Learning Finished!\n"
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-760eb6b7f7a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#학습만 반복 코스트 보며 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mavg_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mbatch_xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_epochs' is not defined"
>>>>>>> c7d8f121b7480536c36d2f93407d48d1fe71c46d
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.322934777\n",
      "Epoch: 0001 cost = 0.322670728\n",
      "Epoch: 0002 cost = 0.318660885\n",
      "Epoch: 0003 cost = 0.320851028\n",
      "Epoch: 0004 cost = 0.317289948\n",
      "Epoch: 0005 cost = 0.320301950\n",
      "Epoch: 0006 cost = 0.319773078\n",
      "Epoch: 0007 cost = 0.320009947\n",
      "Epoch: 0008 cost = 0.320991546\n",
      "Epoch: 0009 cost = 0.317613274\n",
      "Epoch: 0010 cost = 0.318127424\n",
      "Epoch: 0011 cost = 0.315550268\n",
      "Epoch: 0012 cost = 0.319107503\n",
      "Epoch: 0013 cost = 0.317370921\n",
      "Epoch: 0014 cost = 0.313254893\n",
      "Epoch: 0015 cost = 0.315644294\n",
      "Epoch: 0016 cost = 0.315298170\n",
      "Epoch: 0017 cost = 0.314234465\n",
      "Epoch: 0018 cost = 0.312997371\n",
      "Epoch: 0019 cost = 0.314338624\n",
      "Epoch: 0020 cost = 0.310044199\n",
      "Epoch: 0021 cost = 0.314489067\n",
      "Epoch: 0022 cost = 0.311609298\n",
      "Epoch: 0023 cost = 0.312478632\n",
      "Epoch: 0024 cost = 0.315486908\n",
      "Epoch: 0025 cost = 0.311447442\n",
      "Epoch: 0026 cost = 0.311211944\n",
      "Epoch: 0027 cost = 0.314655095\n",
      "Epoch: 0028 cost = 0.310910374\n",
      "Epoch: 0029 cost = 0.310225308\n",
      "Epoch: 0030 cost = 0.310463786\n",
      "Epoch: 0031 cost = 0.310570329\n",
      "Epoch: 0032 cost = 0.309506148\n",
      "Epoch: 0033 cost = 0.307101309\n",
      "Epoch: 0034 cost = 0.308593959\n",
      "Epoch: 0035 cost = 0.310139447\n",
      "Epoch: 0036 cost = 0.306854308\n",
      "Epoch: 0037 cost = 0.309040040\n",
      "Epoch: 0038 cost = 0.308572710\n",
      "Epoch: 0039 cost = 0.305582196\n",
      "Epoch: 0040 cost = 0.307739854\n",
      "Epoch: 0041 cost = 0.306049436\n",
      "Epoch: 0042 cost = 0.303922445\n",
      "Epoch: 0043 cost = 0.305205852\n",
      "Epoch: 0044 cost = 0.305797279\n",
      "Epoch: 0045 cost = 0.304443330\n",
      "Epoch: 0046 cost = 0.300598085\n",
      "Epoch: 0047 cost = 0.303924114\n",
      "Epoch: 0048 cost = 0.305007756\n",
      "Epoch: 0049 cost = 0.302245378\n",
      "Epoch: 0050 cost = 0.299552679\n",
      "Epoch: 0051 cost = 0.303423196\n",
      "Epoch: 0052 cost = 0.302711695\n",
      "Epoch: 0053 cost = 0.303511530\n",
      "Epoch: 0054 cost = 0.305139184\n",
      "Epoch: 0055 cost = 0.301340163\n",
      "Epoch: 0056 cost = 0.300187588\n",
      "Epoch: 0057 cost = 0.300530583\n",
      "Epoch: 0058 cost = 0.298943222\n",
      "Epoch: 0059 cost = 0.302570254\n",
      "Epoch: 0060 cost = 0.297014594\n",
      "Epoch: 0061 cost = 0.296111226\n",
      "Epoch: 0062 cost = 0.297818452\n",
      "Epoch: 0063 cost = 0.297422498\n",
      "Epoch: 0064 cost = 0.299629629\n",
      "Epoch: 0065 cost = 0.296583116\n",
      "Epoch: 0066 cost = 0.302914858\n",
      "Epoch: 0067 cost = 0.295130461\n",
      "Epoch: 0068 cost = 0.298313349\n",
      "Epoch: 0069 cost = 0.296788633\n",
      "Epoch: 0070 cost = 0.298589975\n",
      "Epoch: 0071 cost = 0.296415210\n",
      "Epoch: 0072 cost = 0.293774635\n",
      "Epoch: 0073 cost = 0.294128865\n",
      "Epoch: 0074 cost = 0.295361578\n",
      "Epoch: 0075 cost = 0.294690758\n",
      "Epoch: 0076 cost = 0.293974251\n",
      "Epoch: 0077 cost = 0.293643236\n",
      "Epoch: 0078 cost = 0.298087746\n",
      "Epoch: 0079 cost = 0.295629501\n",
      "Epoch: 0080 cost = 0.294225901\n",
      "Epoch: 0081 cost = 0.292902768\n",
      "Epoch: 0082 cost = 0.297125578\n",
      "Epoch: 0083 cost = 0.293547481\n",
      "Epoch: 0084 cost = 0.295660585\n",
      "Epoch: 0085 cost = 0.292885989\n",
      "Epoch: 0086 cost = 0.297249973\n",
      "Epoch: 0087 cost = 0.294648767\n",
      "Epoch: 0088 cost = 0.287955195\n",
      "Epoch: 0089 cost = 0.289085448\n",
      "Epoch: 0090 cost = 0.291488707\n",
      "Epoch: 0091 cost = 0.288959026\n",
      "Epoch: 0092 cost = 0.291026980\n",
      "Epoch: 0093 cost = 0.292240709\n",
      "Epoch: 0094 cost = 0.289955258\n",
      "Epoch: 0095 cost = 0.291245490\n",
      "Epoch: 0096 cost = 0.289942890\n",
      "Epoch: 0097 cost = 0.291269809\n",
      "Epoch: 0098 cost = 0.284869552\n",
      "Epoch: 0099 cost = 0.290075421\n",
      "Epoch: 0100 cost = 0.288883090\n",
      "Epoch: 0101 cost = 0.286959618\n",
      "Epoch: 0102 cost = 0.287251025\n",
      "Epoch: 0103 cost = 0.290970981\n",
      "Epoch: 0104 cost = 0.285404265\n",
      "Epoch: 0105 cost = 0.288541317\n",
      "Epoch: 0106 cost = 0.290823072\n",
      "Epoch: 0107 cost = 0.287067652\n",
      "Epoch: 0108 cost = 0.285226852\n",
      "Epoch: 0109 cost = 0.286914349\n",
      "Epoch: 0110 cost = 0.284144610\n",
      "Epoch: 0111 cost = 0.281449735\n",
      "Epoch: 0112 cost = 0.284994245\n",
      "Epoch: 0113 cost = 0.285385728\n",
      "Epoch: 0114 cost = 0.283858478\n",
      "Epoch: 0115 cost = 0.283624649\n",
      "Epoch: 0116 cost = 0.282833248\n",
      "Epoch: 0117 cost = 0.279769838\n",
      "Epoch: 0118 cost = 0.285968870\n",
      "Epoch: 0119 cost = 0.282612860\n",
      "Epoch: 0120 cost = 0.284065872\n",
      "Epoch: 0121 cost = 0.283072859\n",
      "Epoch: 0122 cost = 0.282598913\n",
      "Epoch: 0123 cost = 0.282177031\n",
      "Epoch: 0124 cost = 0.280721515\n",
      "Epoch: 0125 cost = 0.282838374\n",
      "Epoch: 0126 cost = 0.281939059\n",
      "Epoch: 0127 cost = 0.282012582\n",
      "Epoch: 0128 cost = 0.283037484\n",
      "Epoch: 0129 cost = 0.284359068\n",
      "Epoch: 0130 cost = 0.281367034\n",
      "Epoch: 0131 cost = 0.283868432\n",
      "Epoch: 0132 cost = 0.283367544\n",
      "Epoch: 0133 cost = 0.280500203\n",
      "Epoch: 0134 cost = 0.282235801\n",
      "Epoch: 0135 cost = 0.282310694\n",
      "Epoch: 0136 cost = 0.275336742\n",
      "Epoch: 0137 cost = 0.281782538\n",
      "Epoch: 0138 cost = 0.279318064\n",
      "Epoch: 0139 cost = 0.282228678\n",
      "Epoch: 0140 cost = 0.276245445\n",
      "Epoch: 0141 cost = 0.277756363\n",
      "Epoch: 0142 cost = 0.279985040\n",
      "Epoch: 0143 cost = 0.276967108\n",
      "Epoch: 0144 cost = 0.278414547\n",
      "Epoch: 0145 cost = 0.278321445\n",
      "Epoch: 0146 cost = 0.278022200\n",
      "Epoch: 0147 cost = 0.277557284\n",
      "Epoch: 0148 cost = 0.274160445\n",
      "Epoch: 0149 cost = 0.275767654\n",
      "Epoch: 0150 cost = 0.279862702\n",
      "Epoch: 0151 cost = 0.275977582\n",
      "Epoch: 0152 cost = 0.277421564\n",
      "Epoch: 0153 cost = 0.278046221\n",
      "Epoch: 0154 cost = 0.276833951\n",
      "Epoch: 0155 cost = 0.278156132\n",
      "Epoch: 0156 cost = 0.277344823\n",
      "Epoch: 0157 cost = 0.273372561\n",
      "Epoch: 0158 cost = 0.269545823\n",
      "Epoch: 0159 cost = 0.275260270\n",
      "Epoch: 0160 cost = 0.275655985\n",
      "Epoch: 0161 cost = 0.274313748\n",
      "Epoch: 0162 cost = 0.274405837\n",
      "Epoch: 0163 cost = 0.272889584\n",
      "Epoch: 0164 cost = 0.274842799\n",
      "Epoch: 0165 cost = 0.272488743\n",
      "Epoch: 0166 cost = 0.274388522\n",
      "Epoch: 0167 cost = 0.278026491\n",
      "Epoch: 0168 cost = 0.270215839\n",
      "Epoch: 0169 cost = 0.273088127\n",
      "Epoch: 0170 cost = 0.272405773\n",
      "Epoch: 0171 cost = 0.268912762\n",
      "Epoch: 0172 cost = 0.276509345\n",
      "Epoch: 0173 cost = 0.270172715\n",
      "Epoch: 0174 cost = 0.271972686\n",
      "Epoch: 0175 cost = 0.267102450\n",
      "Epoch: 0176 cost = 0.272270501\n",
      "Epoch: 0177 cost = 0.271697253\n",
      "Epoch: 0178 cost = 0.270587236\n",
      "Epoch: 0179 cost = 0.270797282\n",
      "Epoch: 0180 cost = 0.271925390\n",
      "Epoch: 0181 cost = 0.267759323\n",
      "Epoch: 0182 cost = 0.268808007\n",
      "Epoch: 0183 cost = 0.268212050\n",
      "Epoch: 0184 cost = 0.270326167\n",
      "Epoch: 0185 cost = 0.269777507\n",
      "Epoch: 0186 cost = 0.267769158\n",
      "Epoch: 0187 cost = 0.268289953\n",
      "Epoch: 0188 cost = 0.265795648\n",
      "Epoch: 0189 cost = 0.268376678\n",
      "Epoch: 0190 cost = 0.267997533\n",
      "Epoch: 0191 cost = 0.269217402\n",
      "Epoch: 0192 cost = 0.269113392\n",
      "Epoch: 0193 cost = 0.267214447\n",
      "Epoch: 0194 cost = 0.265827179\n",
      "Epoch: 0195 cost = 0.271164954\n",
      "Epoch: 0196 cost = 0.264675885\n",
      "Epoch: 0197 cost = 0.268078834\n",
      "Epoch: 0198 cost = 0.266942412\n",
      "Epoch: 0199 cost = 0.267695695\n",
      "Accuracy:  0.91114813\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정 2\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./SpeakerRecognition-0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
=======
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'Saver'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-084227b12ebf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./my_voice_model2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'Saver'"
     ]
>>>>>>> c7d8f121b7480536c36d2f93407d48d1fe71c46d
    }
   ],
   "source": [
    "W100 = tf.Variable(tf.random_normal([1]), name='weight') # 저장할 w 생성\n",
    "saver = tf.train.Saver() # saver 객체 받음\n",
    "sess = tf.Session() # session 객체 받음\n",
    "sess.run(tf.global_variables_initializer()) # session 초기화\n",
    "saver.save(sess, './SpeakerRecognition', global_step=0) # global_step = 0 : 0번째 학습모델 저장"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 5,
>>>>>>> c7d8f121b7480536c36d2f93407d48d1fe71c46d
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "(536, 20)\n",
      "(536, 6)\n",
      "predict\n",
      "4    411\n",
      "3     51\n",
      "2     37\n",
      "1     20\n",
      "0     15\n",
      "5      2\n",
      "dtype: int64\n",
      "Accuracy:  0.76679105\n"
=======
      "(533, 13)\n",
      "(533, 5)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hypothesis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a31543626581>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#print(\"Label :\",sess.run(tf.argmax(Y_test,1)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mcorrect_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"predict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hypothesis' is not defined"
>>>>>>> c7d8f121b7480536c36d2f93407d48d1fe71c46d
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(\"./test_임찬주.wav\")\n",
    "\n",
    "X_test = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "\n",
    "'''\n",
    "0 정유경\n",
    "1 배철수\n",
    "2 이윤진\n",
    "3 강정윤\n",
    "4 임찬주\n",
    "5 한예진\n",
    "'''\n",
    "label = [0 for i in range(6)]#class가 3개이니까 y_test만드는 과정\n",
    "label[4] = 1\n",
    "Y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    Y_test.append(label)\n",
    "\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"predict\")\n",
    "print(pd.value_counts(pd.Series(sess.run(tf.argmax(hypothesis, 1),\n",
    "                                    feed_dict={X: X_test, keep_prob:1}))))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
