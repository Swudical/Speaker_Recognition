{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foldername : 0 - 35 파일\n",
      "Foldername : 1 - 20 파일\n",
      "Foldername : 2 - 35 파일\n",
      "Foldername : 3 - 35 파일\n",
      "Foldername : 4 - 35 파일\n",
      "X_data : (86058, 13)\n",
      "Y_label : (86058, 5)\n",
      "5 개의 클래스!!\n",
      "X_train : (64543, 13)\n",
      "Y_train : (64543, 5)\n",
      "X_test : (21515, 13)\n",
      "Y_test : (21515, 5)\n"
     ]
    }
   ],
   "source": [
    "#######################Tensorflow 코드 시작부분\n",
    "import librosa\n",
    "import pyaudio #마이크를 사용하기 위한 라이브러리\n",
    "import wave\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "##### 변수 설정 부분 #####\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100 #비트레이트 설정\n",
    "CHUNK = int(RATE / 10) # 버퍼 사이즈 1초당 44100비트레이트 이므로 100ms단위\n",
    "RECORD_SECONDS = 5 #녹음할 시간 설정\n",
    "WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "DATA_PATH = \"./data\"\n",
    "X_train = []#train_data 저장할 공간\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "tf_classes = 0\n",
    "def load_wave_generator(path): \n",
    "       \n",
    "    batch_waves = []\n",
    "    labels = []\n",
    "    X_data = []\n",
    "    Y_label = []    \n",
    "    global X_train, X_test, Y_train, Y_test, tf_classes\n",
    "    \n",
    "    folders = os.listdir(path)\n",
    "\n",
    "    for folder in folders:\n",
    "        if not os.path.isdir(path):continue #폴더가 아니면 continue                   \n",
    "        files = os.listdir(path+\"/\"+folder)        \n",
    "        print(\"Foldername :\",folder,\"-\",len(files),\"파일\")\n",
    "        #폴더 이름과 그 폴더에 속하는 파일 갯수 출력\n",
    "        for wav in files:\n",
    "            if not wav.endswith(\".wav\"):continue\n",
    "            else:               \n",
    "                #print(\"Filename :\",wav)#.wav 파일이 아니면 continue\n",
    "                y, sr = librosa.load(path+\"/\"+folder+\"/\"+wav)\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "              \n",
    "                X_data.extend(mfcc)\n",
    "               # print(len(mfcc))\n",
    "                \n",
    "                label = [0 for i in range(len(folders))]\n",
    "                label[tf_classes] = 1\n",
    "                \n",
    "                for i in range(len(mfcc)):\n",
    "                    Y_label.append(label)\n",
    "                #print(Y_label)\n",
    "        tf_classes = tf_classes+1\n",
    "    #end loop\n",
    "    print(\"X_data :\",np.shape(X_data))\n",
    "    print(\"Y_label :\",np.shape(Y_label))\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(np.array(X_data), np.array(Y_label))\n",
    "\n",
    "    xy = (X_train, X_test, Y_train, Y_test)\n",
    "    np.save(\"./data.npy\",xy)\n",
    "\n",
    "load_wave_generator(DATA_PATH)\n",
    "\n",
    "#t = np.array(X_train);\n",
    "#print(\"!!!!!!!!\",t,t.shape,X_train)\n",
    "print(tf_classes,\"개의 클래스!!\")\n",
    "print(\"X_train :\",np.shape(X_train))\n",
    "print(\"Y_train :\",np.shape(Y_train))\n",
    "print(\"X_test :\",np.shape(X_test))\n",
    "print(\"Y_test :\",np.shape(Y_test))\n",
    "####################\n",
    "#clf = LogisticRegression()\n",
    "#clf.fit(X_train, Y_train)\n",
    "####################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'set_random_seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-552bce962e3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m777\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtraining_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'set_random_seed'"
     ]
    }
   ],
   "source": [
    "##################  화자인식 NN 버전 ##################\n",
    "X_train, X_test, Y_train, Y_test = np.load(\"./data.npy\")\n",
    "X_train = X_train.astype(\"float\")\n",
    "X_test = X_test.astype(\"float\")\n",
    "\n",
    "tf.compat.v1.reset_default_graph() \n",
    "tf.set_random_seed(777)\n",
    "learning_rate = 0.001\n",
    "training_epochs = 200\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "sd = 1 / np.sqrt(13) # standard deviation 표준편차(표본표준편차라 1/root(n))\n",
    "\n",
    "#mfcc의 기본은 20\n",
    "# 20ms일 때216은 각 mfcc feature의 열이 216\n",
    "X = tf.placeholder(tf.float32, [None, 13])\n",
    "# \n",
    "Y = tf.placeholder(tf.float32, [None, tf_classes])\n",
    "\n",
    "# W = tf.Variable(tf.random_normal([216, 200]))\n",
    "# b = tf.Variable(tf.random_normal([200]))\n",
    "\n",
    "#1차 히든레이어\n",
    "W1 = tf.get_variable(\"w1\",\n",
    "    #tf.random_normal([216, 180], mean=0, stddev=sd),\n",
    "        shape=[13, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b1\")\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1) # 1차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "# 2차 히든 레이어\n",
    "W2 = tf.get_variable(\"w2\",\n",
    "    #tf.random_normal([180, 150], mean=0, stddev=sd),\n",
    "         shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b2\")\n",
    "L2 = tf.nn.tanh(tf.matmul(L1, W2) + b2) # 2차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "\n",
    "# 3차 히든 레이어\n",
    "W3 = tf.get_variable(\"w3\",\n",
    "    #tf.random_normal([150, 100], mean=0, stddev=sd),\n",
    "            shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b3\")\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3) # 3차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "# 4차 히든 레이어\n",
    "W4 = tf.get_variable(\"w4\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[256, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b4\")\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4) # 4차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
    "\n",
    "# 5차 히든 레이어\n",
    "W5 = tf.get_variable(\"w5\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b5\")\n",
    "L5 = tf.nn.relu(tf.matmul(L4, W5) + b5) # 5차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L5 = tf.nn.dropout(L5, keep_prob = keep_prob)\n",
    "\n",
    "# 6차 히든 레이어\n",
    "W6 = tf.get_variable(\"w6\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b6\")\n",
    "L6 = tf.nn.relu(tf.matmul(L5, W6) + b6) # 6차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L6 = tf.nn.dropout(L6, keep_prob = keep_prob)\n",
    "\n",
    "# 7차 히든 레이어\n",
    "W7 = tf.get_variable(\"w7\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b7\")\n",
    "L7 = tf.nn.relu(tf.matmul(L6, W7) + b7) # 7차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L7 = tf.nn.dropout(L7, keep_prob = keep_prob)\n",
    "\n",
    "# 최종 레이어\n",
    "W8 = tf.get_variable(\"w8\", \n",
    "    #tf.random_normal([50, tf_classes], mean=0, stddev=sd),\n",
    "            shape=[128, tf_classes],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([tf_classes], mean=0, stddev=sd), name=\"b8\")\n",
    "hypothesis = tf.matmul(L7, W8) + b8\n",
    "\n",
    "\n",
    "\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "\n",
    "\n",
    "batch_size=1\n",
    "x_len = len(X_train)\n",
    "#짝수\n",
    "if(x_len%2==0):\n",
    "    batch_size = 2\n",
    "elif(x_len%3==0):\n",
    "    batch_size = 3\n",
    "elif(x_len%4==0):\n",
    "    batch_size = 4\n",
    "else:\n",
    "    batch_size = 1\n",
    "\n",
    "split_X = np.split(X_train,batch_size)\n",
    "split_Y = np.split(Y_train,batch_size)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-760eb6b7f7a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#학습만 반복 코스트 보며 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mavg_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mbatch_xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.504630923\n",
      "Epoch: 0001 cost = 0.505423725\n",
      "Epoch: 0002 cost = 0.506962657\n",
      "Epoch: 0003 cost = 0.503636181\n",
      "Epoch: 0004 cost = 0.506153405\n",
      "Epoch: 0005 cost = 0.504864275\n",
      "Epoch: 0006 cost = 0.501926482\n",
      "Epoch: 0007 cost = 0.502458334\n",
      "Epoch: 0008 cost = 0.500681579\n",
      "Epoch: 0009 cost = 0.501509488\n",
      "Epoch: 0010 cost = 0.502397597\n",
      "Epoch: 0011 cost = 0.501291513\n",
      "Epoch: 0012 cost = 0.495422095\n",
      "Epoch: 0013 cost = 0.499599606\n",
      "Epoch: 0014 cost = 0.499315619\n",
      "Epoch: 0015 cost = 0.499232739\n",
      "Epoch: 0016 cost = 0.505135715\n",
      "Epoch: 0017 cost = 0.499925673\n",
      "Epoch: 0018 cost = 0.494408250\n",
      "Epoch: 0019 cost = 0.497647941\n",
      "Epoch: 0020 cost = 0.498946249\n",
      "Epoch: 0021 cost = 0.498199522\n",
      "Epoch: 0022 cost = 0.496817112\n",
      "Epoch: 0023 cost = 0.496717095\n",
      "Epoch: 0024 cost = 0.493417472\n",
      "Epoch: 0025 cost = 0.496392250\n",
      "Epoch: 0026 cost = 0.496961623\n",
      "Epoch: 0027 cost = 0.496666342\n",
      "Epoch: 0028 cost = 0.495251954\n",
      "Epoch: 0029 cost = 0.495611995\n",
      "Epoch: 0030 cost = 0.496128976\n",
      "Epoch: 0031 cost = 0.495387226\n",
      "Epoch: 0032 cost = 0.491955251\n",
      "Epoch: 0033 cost = 0.494082808\n",
      "Epoch: 0034 cost = 0.498955131\n",
      "Epoch: 0035 cost = 0.496870697\n",
      "Epoch: 0036 cost = 0.493158281\n",
      "Epoch: 0037 cost = 0.491891742\n",
      "Epoch: 0038 cost = 0.492360115\n",
      "Epoch: 0039 cost = 0.493209332\n",
      "Epoch: 0040 cost = 0.493079334\n",
      "Epoch: 0041 cost = 0.491934389\n",
      "Epoch: 0042 cost = 0.489688993\n",
      "Epoch: 0043 cost = 0.490770638\n",
      "Epoch: 0044 cost = 0.489499062\n",
      "Epoch: 0045 cost = 0.487931937\n",
      "Epoch: 0046 cost = 0.487980098\n",
      "Epoch: 0047 cost = 0.490310699\n",
      "Epoch: 0048 cost = 0.489572614\n",
      "Epoch: 0049 cost = 0.490118027\n",
      "Epoch: 0050 cost = 0.486829758\n",
      "Epoch: 0051 cost = 0.489028990\n",
      "Epoch: 0052 cost = 0.484886497\n",
      "Epoch: 0053 cost = 0.487899661\n",
      "Epoch: 0054 cost = 0.484075844\n",
      "Epoch: 0055 cost = 0.484626532\n",
      "Epoch: 0056 cost = 0.484251916\n",
      "Epoch: 0057 cost = 0.480876029\n",
      "Epoch: 0058 cost = 0.483004898\n",
      "Epoch: 0059 cost = 0.482349366\n",
      "Epoch: 0060 cost = 0.482524455\n",
      "Epoch: 0061 cost = 0.480559230\n",
      "Epoch: 0062 cost = 0.481462866\n",
      "Epoch: 0063 cost = 0.483721554\n",
      "Epoch: 0064 cost = 0.480072558\n",
      "Epoch: 0065 cost = 0.477973551\n",
      "Epoch: 0066 cost = 0.480421662\n",
      "Epoch: 0067 cost = 0.480266869\n",
      "Epoch: 0068 cost = 0.479915589\n",
      "Epoch: 0069 cost = 0.478853673\n",
      "Epoch: 0070 cost = 0.479717731\n",
      "Epoch: 0071 cost = 0.484439224\n",
      "Epoch: 0072 cost = 0.479167789\n",
      "Epoch: 0073 cost = 0.478896677\n",
      "Epoch: 0074 cost = 0.480574191\n",
      "Epoch: 0075 cost = 0.477283418\n",
      "Epoch: 0076 cost = 0.480028123\n",
      "Epoch: 0077 cost = 0.475349456\n",
      "Epoch: 0078 cost = 0.476998210\n",
      "Epoch: 0079 cost = 0.476367593\n",
      "Epoch: 0080 cost = 0.480253488\n",
      "Epoch: 0081 cost = 0.476926059\n",
      "Epoch: 0082 cost = 0.478215635\n",
      "Epoch: 0083 cost = 0.479445219\n",
      "Epoch: 0084 cost = 0.474582672\n",
      "Epoch: 0085 cost = 0.477205008\n",
      "Epoch: 0086 cost = 0.477410525\n",
      "Epoch: 0087 cost = 0.473806947\n",
      "Epoch: 0088 cost = 0.472647548\n",
      "Epoch: 0089 cost = 0.473178625\n",
      "Epoch: 0090 cost = 0.474751800\n",
      "Epoch: 0091 cost = 0.475657165\n",
      "Epoch: 0092 cost = 0.475227565\n",
      "Epoch: 0093 cost = 0.472228497\n",
      "Epoch: 0094 cost = 0.472431421\n",
      "Epoch: 0095 cost = 0.473219395\n",
      "Epoch: 0096 cost = 0.472117811\n",
      "Epoch: 0097 cost = 0.471269965\n",
      "Epoch: 0098 cost = 0.469347179\n",
      "Epoch: 0099 cost = 0.475579023\n",
      "Epoch: 0100 cost = 0.474229574\n",
      "Epoch: 0101 cost = 0.471196175\n",
      "Epoch: 0102 cost = 0.471542120\n",
      "Epoch: 0103 cost = 0.471025735\n",
      "Epoch: 0104 cost = 0.470094353\n",
      "Epoch: 0105 cost = 0.466901332\n",
      "Epoch: 0106 cost = 0.470867395\n",
      "Epoch: 0107 cost = 0.469175100\n",
      "Epoch: 0108 cost = 0.472301573\n",
      "Epoch: 0109 cost = 0.466859728\n",
      "Epoch: 0110 cost = 0.467639744\n",
      "Epoch: 0111 cost = 0.469057381\n",
      "Epoch: 0112 cost = 0.468351066\n",
      "Epoch: 0113 cost = 0.465269655\n",
      "Epoch: 0114 cost = 0.464064479\n",
      "Epoch: 0115 cost = 0.469904929\n",
      "Epoch: 0116 cost = 0.463641435\n",
      "Epoch: 0117 cost = 0.462313533\n",
      "Epoch: 0118 cost = 0.464735538\n",
      "Epoch: 0119 cost = 0.468003899\n",
      "Epoch: 0120 cost = 0.469372094\n",
      "Epoch: 0121 cost = 0.464554846\n",
      "Epoch: 0122 cost = 0.468399167\n",
      "Epoch: 0123 cost = 0.466841221\n",
      "Epoch: 0124 cost = 0.467967331\n",
      "Epoch: 0125 cost = 0.465781718\n",
      "Epoch: 0126 cost = 0.463629454\n",
      "Epoch: 0127 cost = 0.468767375\n",
      "Epoch: 0128 cost = 0.468462437\n",
      "Epoch: 0129 cost = 0.464318365\n",
      "Epoch: 0130 cost = 0.460770339\n",
      "Epoch: 0131 cost = 0.465620905\n",
      "Epoch: 0132 cost = 0.463139564\n",
      "Epoch: 0133 cost = 0.457922906\n",
      "Epoch: 0134 cost = 0.465195477\n",
      "Epoch: 0135 cost = 0.462162316\n",
      "Epoch: 0136 cost = 0.462239355\n",
      "Epoch: 0137 cost = 0.463727951\n",
      "Epoch: 0138 cost = 0.462023765\n",
      "Epoch: 0139 cost = 0.461269468\n",
      "Epoch: 0140 cost = 0.457726449\n",
      "Epoch: 0141 cost = 0.459120125\n",
      "Epoch: 0142 cost = 0.458688915\n",
      "Epoch: 0143 cost = 0.459862769\n",
      "Epoch: 0144 cost = 0.457504809\n",
      "Epoch: 0145 cost = 0.460954338\n",
      "Epoch: 0146 cost = 0.459579527\n",
      "Epoch: 0147 cost = 0.460537612\n",
      "Epoch: 0148 cost = 0.462060034\n",
      "Epoch: 0149 cost = 0.459600061\n",
      "Epoch: 0150 cost = 0.459155291\n",
      "Epoch: 0151 cost = 0.455127358\n",
      "Epoch: 0152 cost = 0.457917780\n",
      "Epoch: 0153 cost = 0.457771182\n",
      "Epoch: 0154 cost = 0.458018601\n",
      "Epoch: 0155 cost = 0.459465742\n",
      "Epoch: 0156 cost = 0.454822779\n",
      "Epoch: 0157 cost = 0.460571736\n",
      "Epoch: 0158 cost = 0.458792537\n",
      "Epoch: 0159 cost = 0.454324186\n",
      "Epoch: 0160 cost = 0.459632158\n",
      "Epoch: 0161 cost = 0.455641598\n",
      "Epoch: 0162 cost = 0.455097646\n",
      "Epoch: 0163 cost = 0.456190974\n",
      "Epoch: 0164 cost = 0.454453558\n",
      "Epoch: 0165 cost = 0.450987190\n",
      "Epoch: 0166 cost = 0.456677049\n",
      "Epoch: 0167 cost = 0.455137849\n",
      "Epoch: 0168 cost = 0.453153133\n",
      "Epoch: 0169 cost = 0.450811237\n",
      "Epoch: 0170 cost = 0.454701692\n",
      "Epoch: 0171 cost = 0.453512162\n",
      "Epoch: 0172 cost = 0.451187998\n",
      "Epoch: 0173 cost = 0.450875282\n",
      "Epoch: 0174 cost = 0.453334212\n",
      "Epoch: 0175 cost = 0.450228482\n",
      "Epoch: 0176 cost = 0.454247862\n",
      "Epoch: 0177 cost = 0.454678714\n",
      "Epoch: 0178 cost = 0.453471184\n",
      "Epoch: 0179 cost = 0.450230062\n",
      "Epoch: 0180 cost = 0.448080271\n",
      "Epoch: 0181 cost = 0.451206982\n",
      "Epoch: 0182 cost = 0.450066715\n",
      "Epoch: 0183 cost = 0.450203985\n",
      "Epoch: 0184 cost = 0.448407263\n",
      "Epoch: 0185 cost = 0.449750990\n",
      "Epoch: 0186 cost = 0.445228547\n",
      "Epoch: 0187 cost = 0.448407143\n",
      "Epoch: 0188 cost = 0.447275430\n",
      "Epoch: 0189 cost = 0.449876338\n",
      "Epoch: 0190 cost = 0.445452720\n",
      "Epoch: 0191 cost = 0.444400787\n",
      "Epoch: 0192 cost = 0.446732908\n",
      "Epoch: 0193 cost = 0.442682654\n",
      "Epoch: 0194 cost = 0.446669877\n",
      "Epoch: 0195 cost = 0.445167303\n",
      "Epoch: 0196 cost = 0.449800402\n",
      "Epoch: 0197 cost = 0.448419005\n",
      "Epoch: 0198 cost = 0.443681866\n",
      "Epoch: 0199 cost = 0.444965005\n",
      "Accuracy:  0.8645596\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정 2\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'Saver'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-084227b12ebf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./my_voice_model2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'Saver'"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './my_voice_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(533, 13)\n",
      "(533, 5)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hypothesis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a31543626581>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#print(\"Label :\",sess.run(tf.argmax(Y_test,1)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mcorrect_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"predict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hypothesis' is not defined"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(\"./test_이윤진.wav\")\n",
    "\n",
    "X_test = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "\n",
    "'''\n",
    "0 정유경\n",
    "1 배철수\n",
    "2 이윤진\n",
    "3 강정윤\n",
    "4 임찬주\n",
    "'''\n",
    "label = [0 for i in range(5)]#class가 3개이니까 y_test만드는 과정\n",
    "label[2] = 1\n",
    "Y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    Y_test.append(label)\n",
    "\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))\n",
    "\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "#print(\"Label :\",sess.run(tf.argmax(Y_test,1)))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"predict\")\n",
    "print(pd.value_counts(pd.Series(sess.run(tf.argmax(hypothesis, 1),\n",
    "                                    feed_dict={X: X_test, keep_prob:1}))))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
