{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foldername : 0 - 20 파일\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ImChanjoo\\anaconda3\\envs\\python37\\lib\\site-packages\\librosa\\core\\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
      "C:\\Users\\ImChanjoo\\anaconda3\\envs\\python37\\lib\\site-packages\\librosa\\core\\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foldername : 1 - 20 파일\n",
      "Foldername : 2 - 20 파일\n",
      "Foldername : 3 - 20 파일\n",
      "Foldername : 4 - 20 파일\n",
      "X_data : (53381, 13)\n",
      "Y_label : (53381, 5)\n",
      "5 개의 클래스!!\n",
      "X_train : (40035, 13)\n",
      "Y_train : (40035, 5)\n",
      "X_test : (13346, 13)\n",
      "Y_test : (13346, 5)\n"
     ]
    }
   ],
   "source": [
    "#######################Tensorflow 코드 시작부분\n",
    "import librosa\n",
    "import pyaudio #마이크를 사용하기 위한 라이브러리\n",
    "import wave\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "##### 변수 설정 부분 #####\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100 #비트레이트 설정\n",
    "CHUNK = int(RATE / 10) # 버퍼 사이즈 1초당 44100비트레이트 이므로 100ms단위\n",
    "RECORD_SECONDS = 5 #녹음할 시간 설정\n",
    "WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "DATA_PATH = \"./data\"\n",
    "X_train = []#train_data 저장할 공간\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "tf_classes = 0\n",
    "def load_wave_generator(path): \n",
    "       \n",
    "    batch_waves = []\n",
    "    labels = []\n",
    "    X_data = []\n",
    "    Y_label = []    \n",
    "    global X_train, X_test, Y_train, Y_test, tf_classes\n",
    "    \n",
    "    folders = os.listdir(path)\n",
    "\n",
    "    for folder in folders:\n",
    "        if not os.path.isdir(path):continue #폴더가 아니면 continue                   \n",
    "        files = os.listdir(path+\"/\"+folder)        \n",
    "        print(\"Foldername :\",folder,\"-\",len(files),\"파일\")\n",
    "        #폴더 이름과 그 폴더에 속하는 파일 갯수 출력\n",
    "        for wav in files:\n",
    "            if not wav.endswith(\".wav\"):continue\n",
    "            else:               \n",
    "                #print(\"Filename :\",wav)#.wav 파일이 아니면 continue\n",
    "                y, sr = librosa.load(path+\"/\"+folder+\"/\"+wav)\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "              \n",
    "                X_data.extend(mfcc)\n",
    "               # print(len(mfcc))\n",
    "                \n",
    "                label = [0 for i in range(len(folders))]\n",
    "                label[tf_classes] = 1\n",
    "                \n",
    "                for i in range(len(mfcc)):\n",
    "                    Y_label.append(label)\n",
    "                #print(Y_label)\n",
    "        tf_classes = tf_classes+1\n",
    "    #end loop\n",
    "    print(\"X_data :\",np.shape(X_data))\n",
    "    print(\"Y_label :\",np.shape(Y_label))\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(np.array(X_data), np.array(Y_label))\n",
    "\n",
    "    xy = (X_train, X_test, Y_train, Y_test)\n",
    "    np.save(\"./data.npy\",xy)\n",
    "\n",
    "load_wave_generator(DATA_PATH)\n",
    "\n",
    "#t = np.array(X_train);\n",
    "#print(\"!!!!!!!!\",t,t.shape,X_train)\n",
    "print(tf_classes,\"개의 클래스!!\")\n",
    "print(\"X_train :\",np.shape(X_train))\n",
    "print(\"Y_train :\",np.shape(Y_train))\n",
    "print(\"X_test :\",np.shape(X_test))\n",
    "print(\"Y_test :\",np.shape(Y_test))\n",
    "####################\n",
    "#clf = LogisticRegression()\n",
    "#clf.fit(X_train, Y_train)\n",
    "####################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-8b4eb01eca73>:29: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-3-8b4eb01eca73>:97: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0000 cost = 1.798027953\n",
      "Epoch: 0001 cost = 1.675481478\n",
      "Epoch: 0002 cost = 1.646433512\n",
      "Epoch: 0003 cost = 1.636361877\n",
      "Epoch: 0004 cost = 1.630318324\n",
      "Epoch: 0005 cost = 1.626638532\n",
      "Epoch: 0006 cost = 1.622805635\n",
      "Epoch: 0007 cost = 1.618940711\n",
      "Epoch: 0008 cost = 1.615052263\n",
      "Epoch: 0009 cost = 1.608154058\n",
      "Epoch: 0010 cost = 1.602244059\n",
      "Epoch: 0011 cost = 1.593211055\n",
      "Epoch: 0012 cost = 1.579501192\n",
      "Epoch: 0013 cost = 1.561923385\n",
      "Epoch: 0014 cost = 1.540383697\n",
      "Epoch: 0015 cost = 1.520258705\n",
      "Epoch: 0016 cost = 1.501841625\n",
      "Epoch: 0017 cost = 1.484766523\n",
      "Epoch: 0018 cost = 1.471244017\n",
      "Epoch: 0019 cost = 1.454313318\n",
      "Epoch: 0020 cost = 1.444546262\n",
      "Epoch: 0021 cost = 1.428380648\n",
      "Epoch: 0022 cost = 1.410426617\n",
      "Epoch: 0023 cost = 1.388225993\n",
      "Epoch: 0024 cost = 1.375125488\n",
      "Epoch: 0025 cost = 1.351980607\n",
      "Epoch: 0026 cost = 1.336093545\n",
      "Epoch: 0027 cost = 1.315087398\n",
      "Epoch: 0028 cost = 1.294904590\n",
      "Epoch: 0029 cost = 1.272149483\n",
      "Epoch: 0030 cost = 1.259278854\n",
      "Epoch: 0031 cost = 1.243835330\n",
      "Epoch: 0032 cost = 1.236097534\n",
      "Epoch: 0033 cost = 1.220309774\n",
      "Epoch: 0034 cost = 1.206758618\n",
      "Epoch: 0035 cost = 1.191937288\n",
      "Epoch: 0036 cost = 1.183554769\n",
      "Epoch: 0037 cost = 1.173853397\n",
      "Epoch: 0038 cost = 1.163331191\n",
      "Epoch: 0039 cost = 1.158370018\n",
      "Epoch: 0040 cost = 1.147146344\n",
      "Epoch: 0041 cost = 1.134292920\n",
      "Epoch: 0042 cost = 1.118466496\n",
      "Epoch: 0043 cost = 1.114716013\n",
      "Epoch: 0044 cost = 1.099627415\n",
      "Epoch: 0045 cost = 1.094427546\n",
      "Epoch: 0046 cost = 1.081391732\n",
      "Epoch: 0047 cost = 1.073241393\n",
      "Epoch: 0048 cost = 1.057096958\n",
      "Epoch: 0049 cost = 1.051686247\n",
      "Epoch: 0050 cost = 1.047228654\n",
      "Epoch: 0051 cost = 1.027421991\n",
      "Epoch: 0052 cost = 1.018499176\n",
      "Epoch: 0053 cost = 1.007104596\n",
      "Epoch: 0054 cost = 0.999488135\n",
      "Epoch: 0055 cost = 0.982985318\n",
      "Epoch: 0056 cost = 0.972557724\n",
      "Epoch: 0057 cost = 0.957884133\n",
      "Epoch: 0058 cost = 0.945257664\n",
      "Epoch: 0059 cost = 0.937574446\n",
      "Epoch: 0060 cost = 0.920777678\n",
      "Epoch: 0061 cost = 0.908673426\n",
      "Epoch: 0062 cost = 0.891772111\n",
      "Epoch: 0063 cost = 0.888446172\n",
      "Epoch: 0064 cost = 0.873937746\n",
      "Epoch: 0065 cost = 0.860754569\n",
      "Epoch: 0066 cost = 0.849042833\n",
      "Epoch: 0067 cost = 0.837371131\n",
      "Epoch: 0068 cost = 0.820608576\n",
      "Epoch: 0069 cost = 0.812588473\n",
      "Epoch: 0070 cost = 0.798227350\n",
      "Epoch: 0071 cost = 0.781606793\n",
      "Epoch: 0072 cost = 0.770113190\n",
      "Epoch: 0073 cost = 0.759860933\n",
      "Epoch: 0074 cost = 0.750608623\n",
      "Epoch: 0075 cost = 0.739096681\n",
      "Epoch: 0076 cost = 0.725446602\n",
      "Epoch: 0077 cost = 0.716160138\n",
      "Epoch: 0078 cost = 0.711378634\n",
      "Epoch: 0079 cost = 0.704790056\n",
      "Epoch: 0080 cost = 0.690817773\n",
      "Epoch: 0081 cost = 0.683432778\n",
      "Epoch: 0082 cost = 0.679147720\n",
      "Epoch: 0083 cost = 0.673036774\n",
      "Epoch: 0084 cost = 0.667066296\n",
      "Epoch: 0085 cost = 0.658140043\n",
      "Epoch: 0086 cost = 0.654836794\n",
      "Epoch: 0087 cost = 0.645479421\n",
      "Epoch: 0088 cost = 0.640188436\n",
      "Epoch: 0089 cost = 0.632707536\n",
      "Epoch: 0090 cost = 0.632038991\n",
      "Epoch: 0091 cost = 0.625806610\n",
      "Epoch: 0092 cost = 0.622274915\n",
      "Epoch: 0093 cost = 0.613482217\n",
      "Epoch: 0094 cost = 0.614017109\n",
      "Epoch: 0095 cost = 0.608994663\n",
      "Epoch: 0096 cost = 0.602455338\n",
      "Epoch: 0097 cost = 0.600288729\n",
      "Epoch: 0098 cost = 0.598691940\n",
      "Epoch: 0099 cost = 0.588447591\n",
      "Epoch: 0100 cost = 0.586307605\n",
      "Epoch: 0101 cost = 0.580431143\n",
      "Epoch: 0102 cost = 0.583609502\n",
      "Epoch: 0103 cost = 0.580130835\n",
      "Epoch: 0104 cost = 0.576534986\n",
      "Epoch: 0105 cost = 0.569109102\n",
      "Epoch: 0106 cost = 0.568992496\n",
      "Epoch: 0107 cost = 0.569561640\n",
      "Epoch: 0108 cost = 0.563226899\n",
      "Epoch: 0109 cost = 0.559777101\n",
      "Epoch: 0110 cost = 0.559196671\n",
      "Epoch: 0111 cost = 0.553793192\n",
      "Epoch: 0112 cost = 0.553597391\n",
      "Epoch: 0113 cost = 0.551543951\n",
      "Epoch: 0114 cost = 0.549650510\n",
      "Epoch: 0115 cost = 0.548336565\n",
      "Epoch: 0116 cost = 0.542321404\n",
      "Epoch: 0117 cost = 0.538456619\n",
      "Epoch: 0118 cost = 0.536530058\n",
      "Epoch: 0119 cost = 0.534051398\n",
      "Epoch: 0120 cost = 0.532066981\n",
      "Epoch: 0121 cost = 0.531617125\n",
      "Epoch: 0122 cost = 0.529363692\n",
      "Epoch: 0123 cost = 0.530022403\n",
      "Epoch: 0124 cost = 0.526110888\n",
      "Epoch: 0125 cost = 0.526634316\n",
      "Epoch: 0126 cost = 0.526009579\n",
      "Epoch: 0127 cost = 0.520314197\n",
      "Epoch: 0128 cost = 0.522205691\n",
      "Epoch: 0129 cost = 0.518195907\n",
      "Epoch: 0130 cost = 0.511956771\n",
      "Epoch: 0131 cost = 0.509898702\n",
      "Epoch: 0132 cost = 0.508047978\n",
      "Epoch: 0133 cost = 0.506746133\n",
      "Epoch: 0134 cost = 0.504120390\n",
      "Epoch: 0135 cost = 0.508276651\n",
      "Epoch: 0136 cost = 0.508394500\n",
      "Epoch: 0137 cost = 0.504311055\n",
      "Epoch: 0138 cost = 0.498978317\n",
      "Epoch: 0139 cost = 0.499714196\n",
      "Epoch: 0140 cost = 0.500749598\n",
      "Epoch: 0141 cost = 0.499848296\n",
      "Epoch: 0142 cost = 0.496867508\n",
      "Epoch: 0143 cost = 0.495408058\n",
      "Epoch: 0144 cost = 0.485400458\n",
      "Epoch: 0145 cost = 0.493831466\n",
      "Epoch: 0146 cost = 0.491672426\n",
      "Epoch: 0147 cost = 0.484438578\n",
      "Epoch: 0148 cost = 0.490170350\n",
      "Epoch: 0149 cost = 0.485230078\n",
      "Epoch: 0150 cost = 0.483761708\n",
      "Epoch: 0151 cost = 0.483773430\n",
      "Epoch: 0152 cost = 0.483317822\n",
      "Epoch: 0153 cost = 0.481294751\n",
      "Epoch: 0154 cost = 0.474582483\n",
      "Epoch: 0155 cost = 0.476892948\n",
      "Epoch: 0156 cost = 0.478175173\n",
      "Epoch: 0157 cost = 0.475324988\n",
      "Epoch: 0158 cost = 0.475377242\n",
      "Epoch: 0159 cost = 0.473935346\n",
      "Epoch: 0160 cost = 0.472180784\n",
      "Epoch: 0161 cost = 0.469582409\n",
      "Epoch: 0162 cost = 0.465046753\n",
      "Epoch: 0163 cost = 0.468494415\n",
      "Epoch: 0164 cost = 0.468170126\n",
      "Epoch: 0165 cost = 0.465923498\n",
      "Epoch: 0166 cost = 0.464330375\n",
      "Epoch: 0167 cost = 0.466355453\n",
      "Epoch: 0168 cost = 0.464027872\n",
      "Epoch: 0169 cost = 0.460322102\n",
      "Epoch: 0170 cost = 0.461672743\n",
      "Epoch: 0171 cost = 0.460160176\n",
      "Epoch: 0172 cost = 0.459551295\n",
      "Epoch: 0173 cost = 0.456714650\n",
      "Epoch: 0174 cost = 0.455435505\n",
      "Epoch: 0175 cost = 0.453572879\n",
      "Epoch: 0176 cost = 0.452923159\n",
      "Epoch: 0177 cost = 0.453056981\n",
      "Epoch: 0178 cost = 0.451429208\n",
      "Epoch: 0179 cost = 0.456272066\n",
      "Epoch: 0180 cost = 0.447086434\n",
      "Epoch: 0181 cost = 0.452085416\n",
      "Epoch: 0182 cost = 0.445595513\n",
      "Epoch: 0183 cost = 0.445718497\n",
      "Epoch: 0184 cost = 0.449236383\n",
      "Epoch: 0185 cost = 0.449177682\n",
      "Epoch: 0186 cost = 0.443042994\n",
      "Epoch: 0187 cost = 0.439751883\n",
      "Epoch: 0188 cost = 0.441895038\n",
      "Epoch: 0189 cost = 0.447642873\n",
      "Epoch: 0190 cost = 0.435812523\n",
      "Epoch: 0191 cost = 0.438549519\n",
      "Epoch: 0192 cost = 0.436821232\n",
      "Epoch: 0193 cost = 0.441469451\n",
      "Epoch: 0194 cost = 0.436671406\n",
      "Epoch: 0195 cost = 0.434869627\n",
      "Epoch: 0196 cost = 0.439606299\n",
      "Epoch: 0197 cost = 0.436577499\n",
      "Epoch: 0198 cost = 0.435789704\n",
      "Epoch: 0199 cost = 0.435504685\n",
      "Accuracy:  0.860033\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "##################  화자인식 NN 버전 ##################\n",
    "X_train, X_test, Y_train, Y_test = np.load(\"./data.npy\")\n",
    "X_train = X_train.astype(\"float\")\n",
    "X_test = X_test.astype(\"float\")\n",
    "\n",
    "tf.reset_default_graph() \n",
    "tf.set_random_seed(777)\n",
    "learning_rate = 0.001\n",
    "training_epochs = 200\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "sd = 1 / np.sqrt(13) # standard deviation 표준편차(표본표준편차라 1/root(n))\n",
    "\n",
    "#mfcc의 기본은 20\n",
    "# 20ms일 때216은 각 mfcc feature의 열이 216\n",
    "X = tf.placeholder(tf.float32, [None, 13])\n",
    "# \n",
    "Y = tf.placeholder(tf.float32, [None, tf_classes])\n",
    "\n",
    "# W = tf.Variable(tf.random_normal([216, 200]))\n",
    "# b = tf.Variable(tf.random_normal([200]))\n",
    "\n",
    "#1차 히든레이어\n",
    "W1 = tf.get_variable(\"w1\",\n",
    "    #tf.random_normal([216, 180], mean=0, stddev=sd),\n",
    "        shape=[13, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b1\")\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1) # 1차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "# 2차 히든 레이어\n",
    "W2 = tf.get_variable(\"w2\",\n",
    "    #tf.random_normal([180, 150], mean=0, stddev=sd),\n",
    "         shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b2\")\n",
    "L2 = tf.nn.tanh(tf.matmul(L1, W2) + b2) # 2차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "\n",
    "# 3차 히든 레이어\n",
    "W3 = tf.get_variable(\"w3\",\n",
    "    #tf.random_normal([150, 100], mean=0, stddev=sd),\n",
    "            shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b3\")\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3) # 3차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "# 4차 히든 레이어\n",
    "W4 = tf.get_variable(\"w4\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[256, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b4\")\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4) # 4차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
    "\n",
    "# 5차 히든 레이어\n",
    "W5 = tf.get_variable(\"w5\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b5\")\n",
    "L5 = tf.nn.relu(tf.matmul(L4, W5) + b5) # 5차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L5 = tf.nn.dropout(L5, keep_prob = keep_prob)\n",
    "\n",
    "# 6차 히든 레이어\n",
    "W6 = tf.get_variable(\"w6\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b6\")\n",
    "L6 = tf.nn.relu(tf.matmul(L5, W6) + b6) # 6차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L6 = tf.nn.dropout(L6, keep_prob = keep_prob)\n",
    "\n",
    "# 7차 히든 레이어\n",
    "W7 = tf.get_variable(\"w7\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b7\")\n",
    "L7 = tf.nn.relu(tf.matmul(L6, W7) + b7) # 7차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L7 = tf.nn.dropout(L7, keep_prob = keep_prob)\n",
    "\n",
    "# 최종 레이어\n",
    "W8 = tf.get_variable(\"w8\", \n",
    "    #tf.random_normal([50, tf_classes], mean=0, stddev=sd),\n",
    "            shape=[128, tf_classes],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([tf_classes], mean=0, stddev=sd), name=\"b8\")\n",
    "hypothesis = tf.matmul(L7, W8) + b8\n",
    "\n",
    "\n",
    "\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "\n",
    "\n",
    "batch_size=1\n",
    "x_len = len(X_train)\n",
    "#짝수\n",
    "if(x_len%2==0):\n",
    "    batch_size = 2\n",
    "elif(x_len%3==0):\n",
    "    batch_size = 3\n",
    "elif(x_len%4==0):\n",
    "    batch_size = 4\n",
    "else:\n",
    "    batch_size = 1\n",
    "\n",
    "split_X = np.split(X_train,batch_size)\n",
    "split_Y = np.split(Y_train,batch_size)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.437037985\n",
      "Epoch: 0001 cost = 0.436296304\n",
      "Epoch: 0002 cost = 0.435954452\n",
      "Epoch: 0003 cost = 0.434802016\n",
      "Epoch: 0004 cost = 0.434872727\n",
      "Epoch: 0005 cost = 0.436787675\n",
      "Epoch: 0006 cost = 0.433250864\n",
      "Epoch: 0007 cost = 0.436131130\n",
      "Epoch: 0008 cost = 0.429656784\n",
      "Epoch: 0009 cost = 0.428659687\n",
      "Epoch: 0010 cost = 0.428674777\n",
      "Epoch: 0011 cost = 0.435108910\n",
      "Epoch: 0012 cost = 0.428681542\n",
      "Epoch: 0013 cost = 0.434119185\n",
      "Epoch: 0014 cost = 0.428315024\n",
      "Epoch: 0015 cost = 0.429297219\n",
      "Epoch: 0016 cost = 0.425282190\n",
      "Epoch: 0017 cost = 0.427229534\n",
      "Epoch: 0018 cost = 0.428081522\n",
      "Epoch: 0019 cost = 0.423243483\n",
      "Epoch: 0020 cost = 0.422799269\n",
      "Epoch: 0021 cost = 0.421972990\n",
      "Epoch: 0022 cost = 0.421140432\n",
      "Epoch: 0023 cost = 0.417823027\n",
      "Epoch: 0024 cost = 0.422466695\n",
      "Epoch: 0025 cost = 0.420120875\n",
      "Epoch: 0026 cost = 0.417369306\n",
      "Epoch: 0027 cost = 0.420358807\n",
      "Epoch: 0028 cost = 0.417583297\n",
      "Epoch: 0029 cost = 0.418015242\n",
      "Epoch: 0030 cost = 0.416288843\n",
      "Epoch: 0031 cost = 0.415293058\n",
      "Epoch: 0032 cost = 0.415090869\n",
      "Epoch: 0033 cost = 0.414255053\n",
      "Epoch: 0034 cost = 0.417686055\n",
      "Epoch: 0035 cost = 0.410205752\n",
      "Epoch: 0036 cost = 0.410324434\n",
      "Epoch: 0037 cost = 0.409599106\n",
      "Epoch: 0038 cost = 0.413796475\n",
      "Epoch: 0039 cost = 0.409210891\n",
      "Epoch: 0040 cost = 0.411114484\n",
      "Epoch: 0041 cost = 0.408850749\n",
      "Epoch: 0042 cost = 0.405260891\n",
      "Epoch: 0043 cost = 0.412088195\n",
      "Epoch: 0044 cost = 0.407021503\n",
      "Epoch: 0045 cost = 0.406593253\n",
      "Epoch: 0046 cost = 0.408612758\n",
      "Epoch: 0047 cost = 0.403196573\n",
      "Epoch: 0048 cost = 0.405628175\n",
      "Epoch: 0049 cost = 0.409944485\n",
      "Epoch: 0050 cost = 0.403636177\n",
      "Epoch: 0051 cost = 0.400817474\n",
      "Epoch: 0052 cost = 0.405491104\n",
      "Epoch: 0053 cost = 0.405466805\n",
      "Epoch: 0054 cost = 0.400686204\n",
      "Epoch: 0055 cost = 0.400922835\n",
      "Epoch: 0056 cost = 0.399416139\n",
      "Epoch: 0057 cost = 0.400158763\n",
      "Epoch: 0058 cost = 0.401475718\n",
      "Epoch: 0059 cost = 0.398426890\n",
      "Epoch: 0060 cost = 0.399476667\n",
      "Epoch: 0061 cost = 0.396001448\n",
      "Epoch: 0062 cost = 0.402034491\n",
      "Epoch: 0063 cost = 0.395868003\n",
      "Epoch: 0064 cost = 0.397034129\n",
      "Epoch: 0065 cost = 0.395097454\n",
      "Epoch: 0066 cost = 0.394361456\n",
      "Epoch: 0067 cost = 0.393381029\n",
      "Epoch: 0068 cost = 0.392928928\n",
      "Epoch: 0069 cost = 0.391294718\n",
      "Epoch: 0070 cost = 0.393593858\n",
      "Epoch: 0071 cost = 0.387465676\n",
      "Epoch: 0072 cost = 0.388430655\n",
      "Epoch: 0073 cost = 0.391777933\n",
      "Epoch: 0074 cost = 0.389087886\n",
      "Epoch: 0075 cost = 0.389811883\n",
      "Epoch: 0076 cost = 0.394490262\n",
      "Epoch: 0077 cost = 0.383813550\n",
      "Epoch: 0078 cost = 0.390718917\n",
      "Epoch: 0079 cost = 0.386116823\n",
      "Epoch: 0080 cost = 0.392304331\n",
      "Epoch: 0081 cost = 0.387488166\n",
      "Epoch: 0082 cost = 0.386813432\n",
      "Epoch: 0083 cost = 0.382644325\n",
      "Epoch: 0084 cost = 0.384167145\n",
      "Epoch: 0085 cost = 0.384730915\n",
      "Epoch: 0086 cost = 0.384163568\n",
      "Epoch: 0087 cost = 0.384965460\n",
      "Epoch: 0088 cost = 0.386554539\n",
      "Epoch: 0089 cost = 0.376247178\n",
      "Epoch: 0090 cost = 0.381876896\n",
      "Epoch: 0091 cost = 0.378163218\n",
      "Epoch: 0092 cost = 0.380460451\n",
      "Epoch: 0093 cost = 0.379975508\n",
      "Epoch: 0094 cost = 0.381034295\n",
      "Epoch: 0095 cost = 0.379872839\n",
      "Epoch: 0096 cost = 0.379739940\n",
      "Epoch: 0097 cost = 0.376749386\n",
      "Epoch: 0098 cost = 0.384925316\n",
      "Epoch: 0099 cost = 0.377994398\n",
      "Epoch: 0100 cost = 0.375574350\n",
      "Epoch: 0101 cost = 0.379028092\n",
      "Epoch: 0102 cost = 0.376416753\n",
      "Epoch: 0103 cost = 0.377401759\n",
      "Epoch: 0104 cost = 0.369616727\n",
      "Epoch: 0105 cost = 0.373441021\n",
      "Epoch: 0106 cost = 0.371952573\n",
      "Epoch: 0107 cost = 0.372487684\n",
      "Epoch: 0108 cost = 0.371266951\n",
      "Epoch: 0109 cost = 0.373202145\n",
      "Epoch: 0110 cost = 0.371143897\n",
      "Epoch: 0111 cost = 0.371211807\n",
      "Epoch: 0112 cost = 0.372583648\n",
      "Epoch: 0113 cost = 0.371838252\n",
      "Epoch: 0114 cost = 0.369576722\n",
      "Epoch: 0115 cost = 0.367411723\n",
      "Epoch: 0116 cost = 0.365111758\n",
      "Epoch: 0117 cost = 0.368399928\n",
      "Epoch: 0118 cost = 0.364564180\n",
      "Epoch: 0119 cost = 0.369510760\n",
      "Epoch: 0120 cost = 0.370849152\n",
      "Epoch: 0121 cost = 0.367944717\n",
      "Epoch: 0122 cost = 0.369584193\n",
      "Epoch: 0123 cost = 0.366683036\n",
      "Epoch: 0124 cost = 0.368764669\n",
      "Epoch: 0125 cost = 0.369408091\n",
      "Epoch: 0126 cost = 0.370079746\n",
      "Epoch: 0127 cost = 0.365407765\n",
      "Epoch: 0128 cost = 0.368047933\n",
      "Epoch: 0129 cost = 0.362094055\n",
      "Epoch: 0130 cost = 0.364409924\n",
      "Epoch: 0131 cost = 0.362586419\n",
      "Epoch: 0132 cost = 0.365826865\n",
      "Epoch: 0133 cost = 0.359926909\n",
      "Epoch: 0134 cost = 0.362256656\n",
      "Epoch: 0135 cost = 0.361745059\n",
      "Epoch: 0136 cost = 0.363292933\n",
      "Epoch: 0137 cost = 0.362364580\n",
      "Epoch: 0138 cost = 0.360332350\n",
      "Epoch: 0139 cost = 0.359619528\n",
      "Epoch: 0140 cost = 0.358512312\n",
      "Epoch: 0141 cost = 0.363691141\n",
      "Epoch: 0142 cost = 0.357823521\n",
      "Epoch: 0143 cost = 0.359671642\n",
      "Epoch: 0144 cost = 0.355392824\n",
      "Epoch: 0145 cost = 0.359492163\n",
      "Epoch: 0146 cost = 0.356829484\n",
      "Epoch: 0147 cost = 0.355533183\n",
      "Epoch: 0148 cost = 0.355066150\n",
      "Epoch: 0149 cost = 0.356630623\n",
      "Epoch: 0150 cost = 0.357012411\n",
      "Epoch: 0151 cost = 0.351215810\n",
      "Epoch: 0152 cost = 0.357187758\n",
      "Epoch: 0153 cost = 0.357503831\n",
      "Epoch: 0154 cost = 0.356065551\n",
      "Epoch: 0155 cost = 0.356484493\n",
      "Epoch: 0156 cost = 0.353741735\n",
      "Epoch: 0157 cost = 0.357051293\n",
      "Epoch: 0158 cost = 0.354223271\n",
      "Epoch: 0159 cost = 0.348981520\n",
      "Epoch: 0160 cost = 0.352298409\n",
      "Epoch: 0161 cost = 0.347974350\n",
      "Epoch: 0162 cost = 0.354240398\n",
      "Epoch: 0163 cost = 0.351185660\n",
      "Epoch: 0164 cost = 0.352626026\n",
      "Epoch: 0165 cost = 0.352079580\n",
      "Epoch: 0166 cost = 0.350037475\n",
      "Epoch: 0167 cost = 0.349256665\n",
      "Epoch: 0168 cost = 0.353431722\n",
      "Epoch: 0169 cost = 0.348381917\n",
      "Epoch: 0170 cost = 0.349733353\n",
      "Epoch: 0171 cost = 0.350331833\n",
      "Epoch: 0172 cost = 0.347827951\n",
      "Epoch: 0173 cost = 0.351437489\n",
      "Epoch: 0174 cost = 0.344914198\n",
      "Epoch: 0175 cost = 0.346773913\n",
      "Epoch: 0176 cost = 0.348294030\n",
      "Epoch: 0177 cost = 0.347107768\n",
      "Epoch: 0178 cost = 0.350674927\n",
      "Epoch: 0179 cost = 0.344522794\n",
      "Epoch: 0180 cost = 0.350659271\n",
      "Epoch: 0181 cost = 0.343305757\n",
      "Epoch: 0182 cost = 0.343560964\n",
      "Epoch: 0183 cost = 0.343864312\n",
      "Epoch: 0184 cost = 0.339345068\n",
      "Epoch: 0185 cost = 0.344652305\n",
      "Epoch: 0186 cost = 0.345479012\n",
      "Epoch: 0187 cost = 0.340953877\n",
      "Epoch: 0188 cost = 0.342706472\n",
      "Epoch: 0189 cost = 0.338459243\n",
      "Epoch: 0190 cost = 0.344363093\n",
      "Epoch: 0191 cost = 0.339733074\n",
      "Epoch: 0192 cost = 0.340433160\n",
      "Epoch: 0193 cost = 0.342356831\n",
      "Epoch: 0194 cost = 0.337201516\n",
      "Epoch: 0195 cost = 0.340888530\n",
      "Epoch: 0196 cost = 0.341241697\n",
      "Epoch: 0197 cost = 0.341732512\n",
      "Epoch: 0198 cost = 0.334722459\n",
      "Epoch: 0199 cost = 0.339126070\n",
      "Accuracy:  0.8834857\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.342179437\n",
      "Epoch: 0001 cost = 0.341583321\n",
      "Epoch: 0002 cost = 0.337357183\n",
      "Epoch: 0003 cost = 0.339582404\n",
      "Epoch: 0004 cost = 0.335237980\n",
      "Epoch: 0005 cost = 0.333795577\n",
      "Epoch: 0006 cost = 0.334458719\n",
      "Epoch: 0007 cost = 0.336842299\n",
      "Epoch: 0008 cost = 0.333958189\n",
      "Epoch: 0009 cost = 0.339060406\n",
      "Epoch: 0010 cost = 0.332802971\n",
      "Epoch: 0011 cost = 0.333110809\n",
      "Epoch: 0012 cost = 0.341780494\n",
      "Epoch: 0013 cost = 0.330940922\n",
      "Epoch: 0014 cost = 0.332951794\n",
      "Epoch: 0015 cost = 0.334873617\n",
      "Epoch: 0016 cost = 0.334804495\n",
      "Epoch: 0017 cost = 0.329946031\n",
      "Epoch: 0018 cost = 0.332145631\n",
      "Epoch: 0019 cost = 0.327889989\n",
      "Epoch: 0020 cost = 0.330909232\n",
      "Epoch: 0021 cost = 0.334684600\n",
      "Epoch: 0022 cost = 0.327846517\n",
      "Epoch: 0023 cost = 0.329474638\n",
      "Epoch: 0024 cost = 0.331135372\n",
      "Epoch: 0025 cost = 0.330842763\n",
      "Epoch: 0026 cost = 0.331191162\n",
      "Epoch: 0027 cost = 0.330672264\n",
      "Epoch: 0028 cost = 0.330237895\n",
      "Epoch: 0029 cost = 0.330405672\n",
      "Epoch: 0030 cost = 0.327834646\n",
      "Epoch: 0031 cost = 0.329116126\n",
      "Epoch: 0032 cost = 0.331452062\n",
      "Epoch: 0033 cost = 0.329115172\n",
      "Epoch: 0034 cost = 0.330070366\n",
      "Epoch: 0035 cost = 0.327674776\n",
      "Epoch: 0036 cost = 0.328955442\n",
      "Epoch: 0037 cost = 0.327568024\n",
      "Epoch: 0038 cost = 0.325246880\n",
      "Epoch: 0039 cost = 0.325215320\n",
      "Epoch: 0040 cost = 0.323840350\n",
      "Epoch: 0041 cost = 0.326343775\n",
      "Epoch: 0042 cost = 0.321455727\n",
      "Epoch: 0043 cost = 0.322991033\n",
      "Epoch: 0044 cost = 0.326873134\n",
      "Epoch: 0045 cost = 0.324159950\n",
      "Epoch: 0046 cost = 0.323192199\n",
      "Epoch: 0047 cost = 0.323940009\n",
      "Epoch: 0048 cost = 0.317672521\n",
      "Epoch: 0049 cost = 0.322532535\n",
      "Epoch: 0050 cost = 0.323882759\n",
      "Epoch: 0051 cost = 0.318597098\n",
      "Epoch: 0052 cost = 0.323233436\n",
      "Epoch: 0053 cost = 0.321537286\n",
      "Epoch: 0054 cost = 0.320144673\n",
      "Epoch: 0055 cost = 0.319782197\n",
      "Epoch: 0056 cost = 0.321084867\n",
      "Epoch: 0057 cost = 0.317748408\n",
      "Epoch: 0058 cost = 0.321358810\n",
      "Epoch: 0059 cost = 0.318710824\n",
      "Epoch: 0060 cost = 0.316712479\n",
      "Epoch: 0061 cost = 0.317152450\n",
      "Epoch: 0062 cost = 0.319147239\n",
      "Epoch: 0063 cost = 0.318549603\n",
      "Epoch: 0064 cost = 0.323152741\n",
      "Epoch: 0065 cost = 0.314733426\n",
      "Epoch: 0066 cost = 0.312742591\n",
      "Epoch: 0067 cost = 0.317004184\n",
      "Epoch: 0068 cost = 0.319739858\n",
      "Epoch: 0069 cost = 0.316787014\n",
      "Epoch: 0070 cost = 0.318143378\n",
      "Epoch: 0071 cost = 0.315818508\n",
      "Epoch: 0072 cost = 0.313744843\n",
      "Epoch: 0073 cost = 0.315228224\n",
      "Epoch: 0074 cost = 0.314255406\n",
      "Epoch: 0075 cost = 0.315135906\n",
      "Epoch: 0076 cost = 0.316653093\n",
      "Epoch: 0077 cost = 0.310405612\n",
      "Epoch: 0078 cost = 0.320950687\n",
      "Epoch: 0079 cost = 0.314089795\n",
      "Epoch: 0080 cost = 0.315617760\n",
      "Epoch: 0081 cost = 0.316489567\n",
      "Epoch: 0082 cost = 0.313859135\n",
      "Epoch: 0083 cost = 0.312916676\n",
      "Epoch: 0084 cost = 0.307409426\n",
      "Epoch: 0085 cost = 0.309671144\n",
      "Epoch: 0086 cost = 0.304956555\n",
      "Epoch: 0087 cost = 0.310212215\n",
      "Epoch: 0088 cost = 0.309276233\n",
      "Epoch: 0089 cost = 0.310050388\n",
      "Epoch: 0090 cost = 0.307705084\n",
      "Epoch: 0091 cost = 0.313242833\n",
      "Epoch: 0092 cost = 0.308075120\n",
      "Epoch: 0093 cost = 0.307249616\n",
      "Epoch: 0094 cost = 0.305809319\n",
      "Epoch: 0095 cost = 0.307015797\n",
      "Epoch: 0096 cost = 0.307825069\n",
      "Epoch: 0097 cost = 0.304875503\n",
      "Epoch: 0098 cost = 0.310360481\n",
      "Epoch: 0099 cost = 0.313449591\n",
      "Epoch: 0100 cost = 0.314631253\n",
      "Epoch: 0101 cost = 0.307771156\n",
      "Epoch: 0102 cost = 0.303452879\n",
      "Epoch: 0103 cost = 0.303732276\n",
      "Epoch: 0104 cost = 0.309879035\n",
      "Epoch: 0105 cost = 0.306760708\n",
      "Epoch: 0106 cost = 0.302047551\n",
      "Epoch: 0107 cost = 0.303644607\n",
      "Epoch: 0108 cost = 0.302568167\n",
      "Epoch: 0109 cost = 0.302587678\n",
      "Epoch: 0110 cost = 0.297951380\n",
      "Epoch: 0111 cost = 0.302432477\n",
      "Epoch: 0112 cost = 0.300603539\n",
      "Epoch: 0113 cost = 0.303621729\n",
      "Epoch: 0114 cost = 0.301396002\n",
      "Epoch: 0115 cost = 0.299768289\n",
      "Epoch: 0116 cost = 0.299064984\n",
      "Epoch: 0117 cost = 0.295729478\n",
      "Epoch: 0118 cost = 0.299355716\n",
      "Epoch: 0119 cost = 0.300899049\n",
      "Epoch: 0120 cost = 0.305089613\n",
      "Epoch: 0121 cost = 0.303870360\n",
      "Epoch: 0122 cost = 0.305002451\n",
      "Epoch: 0123 cost = 0.297296415\n",
      "Epoch: 0124 cost = 0.300023745\n",
      "Epoch: 0125 cost = 0.292319000\n",
      "Epoch: 0126 cost = 0.293664734\n",
      "Epoch: 0127 cost = 0.295472254\n",
      "Epoch: 0128 cost = 0.291501294\n",
      "Epoch: 0129 cost = 0.297305266\n",
      "Epoch: 0130 cost = 0.295573046\n",
      "Epoch: 0131 cost = 0.295435816\n",
      "Epoch: 0132 cost = 0.300117304\n",
      "Epoch: 0133 cost = 0.295973579\n",
      "Epoch: 0134 cost = 0.295664827\n",
      "Epoch: 0135 cost = 0.296090504\n",
      "Epoch: 0136 cost = 0.298487176\n",
      "Epoch: 0137 cost = 0.299379210\n",
      "Epoch: 0138 cost = 0.294496238\n",
      "Epoch: 0139 cost = 0.292245577\n",
      "Epoch: 0140 cost = 0.293592582\n",
      "Epoch: 0141 cost = 0.292136351\n",
      "Epoch: 0142 cost = 0.289423098\n",
      "Epoch: 0143 cost = 0.287207544\n",
      "Epoch: 0144 cost = 0.289577127\n",
      "Epoch: 0145 cost = 0.294735094\n",
      "Epoch: 0146 cost = 0.293292403\n",
      "Epoch: 0147 cost = 0.289970448\n",
      "Epoch: 0148 cost = 0.293946246\n",
      "Epoch: 0149 cost = 0.288289656\n",
      "Epoch: 0150 cost = 0.290543268\n",
      "Epoch: 0151 cost = 0.290801366\n",
      "Epoch: 0152 cost = 0.286695302\n",
      "Epoch: 0153 cost = 0.285860300\n",
      "Epoch: 0154 cost = 0.284642984\n",
      "Epoch: 0155 cost = 0.285488268\n",
      "Epoch: 0156 cost = 0.287113955\n",
      "Epoch: 0157 cost = 0.288855811\n",
      "Epoch: 0158 cost = 0.295295884\n",
      "Epoch: 0159 cost = 0.294657032\n",
      "Epoch: 0160 cost = 0.289130867\n",
      "Epoch: 0161 cost = 0.294096698\n",
      "Epoch: 0162 cost = 0.290974915\n",
      "Epoch: 0163 cost = 0.284787963\n",
      "Epoch: 0164 cost = 0.285661370\n",
      "Epoch: 0165 cost = 0.285873393\n",
      "Epoch: 0166 cost = 0.288424055\n",
      "Epoch: 0167 cost = 0.281911820\n",
      "Epoch: 0168 cost = 0.284073621\n",
      "Epoch: 0169 cost = 0.281126767\n",
      "Epoch: 0170 cost = 0.280324181\n",
      "Epoch: 0171 cost = 0.277742724\n",
      "Epoch: 0172 cost = 0.290604661\n",
      "Epoch: 0173 cost = 0.292109489\n",
      "Epoch: 0174 cost = 0.285096804\n",
      "Epoch: 0175 cost = 0.286488364\n",
      "Epoch: 0176 cost = 0.288940181\n",
      "Epoch: 0177 cost = 0.293113073\n",
      "Epoch: 0178 cost = 0.283465316\n",
      "Epoch: 0179 cost = 0.284831633\n",
      "Epoch: 0180 cost = 0.281184832\n",
      "Epoch: 0181 cost = 0.281812280\n",
      "Epoch: 0182 cost = 0.287444095\n",
      "Epoch: 0183 cost = 0.275629222\n",
      "Epoch: 0184 cost = 0.279617131\n",
      "Epoch: 0185 cost = 0.275096069\n",
      "Epoch: 0186 cost = 0.272544444\n",
      "Epoch: 0187 cost = 0.273899178\n",
      "Epoch: 0188 cost = 0.275296201\n",
      "Epoch: 0189 cost = 0.279043287\n",
      "Epoch: 0190 cost = 0.272899409\n",
      "Epoch: 0191 cost = 0.268658489\n",
      "Epoch: 0192 cost = 0.281800548\n",
      "Epoch: 0193 cost = 0.277037342\n",
      "Epoch: 0194 cost = 0.269692928\n",
      "Epoch: 0195 cost = 0.272443473\n",
      "Epoch: 0196 cost = 0.270451715\n",
      "Epoch: 0197 cost = 0.274760624\n",
      "Epoch: 0198 cost = 0.271667163\n",
      "Epoch: 0199 cost = 0.274543295\n",
      "Accuracy:  0.9207253\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정 2\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_voice_model2'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './my_voice_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'librosa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a31543626581>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./test_이윤진.wav\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_mfcc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m '''\n",
      "\u001b[1;31mNameError\u001b[0m: name 'librosa' is not defined"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(\"./test_이윤진.wav\")\n",
    "\n",
    "X_test = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "\n",
    "'''\n",
    "0 정유경\n",
    "1 배철수\n",
    "2 이윤진\n",
    "3 강정윤\n",
    "4 임찬주\n",
    "'''\n",
    "label = [0 for i in range(5)]#class가 3개이니까 y_test만드는 과정\n",
    "label[2] = 1\n",
    "Y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    Y_test.append(label)\n",
    "\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))\n",
    "\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "#print(\"Label :\",sess.run(tf.argmax(Y_test,1)))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"predict\")\n",
    "print(pd.value_counts(pd.Series(sess.run(tf.argmax(hypothesis, 1),\n",
    "                                    feed_dict={X: X_test, keep_prob:1}))))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
