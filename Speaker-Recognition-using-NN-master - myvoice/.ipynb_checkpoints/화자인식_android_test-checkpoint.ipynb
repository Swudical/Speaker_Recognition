{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foldername : 0 - 20 파일\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ImChanjoo\\anaconda3\\envs\\python37\\lib\\site-packages\\librosa\\core\\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
      "C:\\Users\\ImChanjoo\\anaconda3\\envs\\python37\\lib\\site-packages\\librosa\\core\\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foldername : 1 - 20 파일\n",
      "Foldername : 2 - 20 파일\n",
      "Foldername : 3 - 20 파일\n",
      "Foldername : 4 - 20 파일\n",
      "Foldername : 5 - 20 파일\n",
      "X_data : (64734, 20)\n",
      "Y_label : (64734, 6)\n",
      "6 개의 클래스!!\n",
      "X_train : (48550, 20)\n",
      "Y_train : (48550, 6)\n",
      "X_test : (16184, 20)\n",
      "Y_test : (16184, 6)\n"
     ]
    }
   ],
   "source": [
    "#######################Tensorflow 코드 시작부분\n",
    "import librosa\n",
    "import pyaudio #마이크를 사용하기 위한 라이브러리\n",
    "import wave\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "##### 변수 설정 부분 #####\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100 #비트레이트 설정\n",
    "CHUNK = int(RATE / 10) # 버퍼 사이즈 1초당 44100비트레이트 이므로 100ms단위\n",
    "RECORD_SECONDS = 5 #녹음할 시간 설정\n",
    "WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "DATA_PATH = \"./data\"\n",
    "X_train = []#train_data 저장할 공간\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "tf_classes = 0\n",
    "def load_wave_generator(path): \n",
    "       \n",
    "    batch_waves = []\n",
    "    labels = []\n",
    "    X_data = []\n",
    "    Y_label = []    \n",
    "    global X_train, X_test, Y_train, Y_test, tf_classes\n",
    "    \n",
    "    folders = os.listdir(path)\n",
    "\n",
    "    for folder in folders:\n",
    "        if not os.path.isdir(path):continue #폴더가 아니면 continue                   \n",
    "        files = os.listdir(path+\"/\"+folder)        \n",
    "        print(\"Foldername :\",folder,\"-\",len(files),\"파일\")\n",
    "        #폴더 이름과 그 폴더에 속하는 파일 갯수 출력\n",
    "        for wav in files:\n",
    "            if not wav.endswith(\".wav\"):continue\n",
    "            else:               \n",
    "                #print(\"Filename :\",wav)#.wav 파일이 아니면 continue\n",
    "                y, sr = librosa.load(path+\"/\"+folder+\"/\"+wav)\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "              \n",
    "                X_data.extend(mfcc)\n",
    "               # print(len(mfcc))\n",
    "                \n",
    "                label = [0 for i in range(len(folders))]\n",
    "                label[tf_classes] = 1\n",
    "                \n",
    "                for i in range(len(mfcc)):\n",
    "                    Y_label.append(label)\n",
    "                #print(Y_label)\n",
    "        tf_classes = tf_classes+1\n",
    "    #end loop\n",
    "    print(\"X_data :\",np.shape(X_data))\n",
    "    print(\"Y_label :\",np.shape(Y_label))\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(np.array(X_data), np.array(Y_label))\n",
    "\n",
    "    xy = (X_train, X_test, Y_train, Y_test)\n",
    "    np.save(\"./data.npy\",xy)\n",
    "\n",
    "load_wave_generator(DATA_PATH)\n",
    "\n",
    "#t = np.array(X_train);\n",
    "#print(\"!!!!!!!!\",t,t.shape,X_train)\n",
    "print(tf_classes,\"개의 클래스!!\")\n",
    "print(\"X_train :\",np.shape(X_train))\n",
    "print(\"Y_train :\",np.shape(Y_train))\n",
    "print(\"X_test :\",np.shape(X_test))\n",
    "print(\"Y_test :\",np.shape(Y_test))\n",
    "####################\n",
    "#clf = LogisticRegression()\n",
    "#clf.fit(X_train, Y_train)\n",
    "####################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-8429a417d9d8>:31: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-2-8429a417d9d8>:99: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0000 cost = 2.169254541\n",
      "Epoch: 0001 cost = 1.918643832\n",
      "Epoch: 0002 cost = 1.866796792\n",
      "Epoch: 0003 cost = 1.841012597\n",
      "Epoch: 0004 cost = 1.828035295\n",
      "Epoch: 0005 cost = 1.819467723\n",
      "Epoch: 0006 cost = 1.817128658\n",
      "Epoch: 0007 cost = 1.812435925\n",
      "Epoch: 0008 cost = 1.807780683\n",
      "Epoch: 0009 cost = 1.804222882\n",
      "Epoch: 0010 cost = 1.800202429\n",
      "Epoch: 0011 cost = 1.793645918\n",
      "Epoch: 0012 cost = 1.787089586\n",
      "Epoch: 0013 cost = 1.777697921\n",
      "Epoch: 0014 cost = 1.768094659\n",
      "Epoch: 0015 cost = 1.752940536\n",
      "Epoch: 0016 cost = 1.739020348\n",
      "Epoch: 0017 cost = 1.725936472\n",
      "Epoch: 0018 cost = 1.712719142\n",
      "Epoch: 0019 cost = 1.702776372\n",
      "Epoch: 0020 cost = 1.693648100\n",
      "Epoch: 0021 cost = 1.685975432\n",
      "Epoch: 0022 cost = 1.677032232\n",
      "Epoch: 0023 cost = 1.671295047\n",
      "Epoch: 0024 cost = 1.664749026\n",
      "Epoch: 0025 cost = 1.656479895\n",
      "Epoch: 0026 cost = 1.650260031\n",
      "Epoch: 0027 cost = 1.642903149\n",
      "Epoch: 0028 cost = 1.631922662\n",
      "Epoch: 0029 cost = 1.622460544\n",
      "Epoch: 0030 cost = 1.608036816\n",
      "Epoch: 0031 cost = 1.591161430\n",
      "Epoch: 0032 cost = 1.571996033\n",
      "Epoch: 0033 cost = 1.546825945\n",
      "Epoch: 0034 cost = 1.524738610\n",
      "Epoch: 0035 cost = 1.502755046\n",
      "Epoch: 0036 cost = 1.476267040\n",
      "Epoch: 0037 cost = 1.458851695\n",
      "Epoch: 0038 cost = 1.444798470\n",
      "Epoch: 0039 cost = 1.424574077\n",
      "Epoch: 0040 cost = 1.409302413\n",
      "Epoch: 0041 cost = 1.384980083\n",
      "Epoch: 0042 cost = 1.365453780\n",
      "Epoch: 0043 cost = 1.348509967\n",
      "Epoch: 0044 cost = 1.330309689\n",
      "Epoch: 0045 cost = 1.314003289\n",
      "Epoch: 0046 cost = 1.290999949\n",
      "Epoch: 0047 cost = 1.278643429\n",
      "Epoch: 0048 cost = 1.256219208\n",
      "Epoch: 0049 cost = 1.233051419\n",
      "Epoch: 0050 cost = 1.216145575\n",
      "Epoch: 0051 cost = 1.195985794\n",
      "Epoch: 0052 cost = 1.178789377\n",
      "Epoch: 0053 cost = 1.164384842\n",
      "Epoch: 0054 cost = 1.149406016\n",
      "Epoch: 0055 cost = 1.133884132\n",
      "Epoch: 0056 cost = 1.117954016\n",
      "Epoch: 0057 cost = 1.097333491\n",
      "Epoch: 0058 cost = 1.089846015\n",
      "Epoch: 0059 cost = 1.072133958\n",
      "Epoch: 0060 cost = 1.059144616\n",
      "Epoch: 0061 cost = 1.045915782\n",
      "Epoch: 0062 cost = 1.037167370\n",
      "Epoch: 0063 cost = 1.021376073\n",
      "Epoch: 0064 cost = 1.009981394\n",
      "Epoch: 0065 cost = 0.992356122\n",
      "Epoch: 0066 cost = 0.978622794\n",
      "Epoch: 0067 cost = 0.959511071\n",
      "Epoch: 0068 cost = 0.950732261\n",
      "Epoch: 0069 cost = 0.941738099\n",
      "Epoch: 0070 cost = 0.925979942\n",
      "Epoch: 0071 cost = 0.911138207\n",
      "Epoch: 0072 cost = 0.896633655\n",
      "Epoch: 0073 cost = 0.881781429\n",
      "Epoch: 0074 cost = 0.865814835\n",
      "Epoch: 0075 cost = 0.858000547\n",
      "Epoch: 0076 cost = 0.840167433\n",
      "Epoch: 0077 cost = 0.823026031\n",
      "Epoch: 0078 cost = 0.813171268\n",
      "Epoch: 0079 cost = 0.794717014\n",
      "Epoch: 0080 cost = 0.780718625\n",
      "Epoch: 0081 cost = 0.768126518\n",
      "Epoch: 0082 cost = 0.755124271\n",
      "Epoch: 0083 cost = 0.738335252\n",
      "Epoch: 0084 cost = 0.724707365\n",
      "Epoch: 0085 cost = 0.717473358\n",
      "Epoch: 0086 cost = 0.702975154\n",
      "Epoch: 0087 cost = 0.688950449\n",
      "Epoch: 0088 cost = 0.672587037\n",
      "Epoch: 0089 cost = 0.659786403\n",
      "Epoch: 0090 cost = 0.655108958\n",
      "Epoch: 0091 cost = 0.644360989\n",
      "Epoch: 0092 cost = 0.634088725\n",
      "Epoch: 0093 cost = 0.621705681\n",
      "Epoch: 0094 cost = 0.616347522\n",
      "Epoch: 0095 cost = 0.610750437\n",
      "Epoch: 0096 cost = 0.602399170\n",
      "Epoch: 0097 cost = 0.593195558\n",
      "Epoch: 0098 cost = 0.583713293\n",
      "Epoch: 0099 cost = 0.574693918\n",
      "Epoch: 0100 cost = 0.569597065\n",
      "Epoch: 0101 cost = 0.562777907\n",
      "Epoch: 0102 cost = 0.559454262\n",
      "Epoch: 0103 cost = 0.554598719\n",
      "Epoch: 0104 cost = 0.550799131\n",
      "Epoch: 0105 cost = 0.545242846\n",
      "Epoch: 0106 cost = 0.534037739\n",
      "Epoch: 0107 cost = 0.530083090\n",
      "Epoch: 0108 cost = 0.525679946\n",
      "Epoch: 0109 cost = 0.523856848\n",
      "Epoch: 0110 cost = 0.515274346\n",
      "Epoch: 0111 cost = 0.510155588\n",
      "Epoch: 0112 cost = 0.509127557\n",
      "Epoch: 0113 cost = 0.505618989\n",
      "Epoch: 0114 cost = 0.498839974\n",
      "Epoch: 0115 cost = 0.495215461\n",
      "Epoch: 0116 cost = 0.493967980\n",
      "Epoch: 0117 cost = 0.489132166\n",
      "Epoch: 0118 cost = 0.481705979\n",
      "Epoch: 0119 cost = 0.478353843\n",
      "Epoch: 0120 cost = 0.475973532\n",
      "Epoch: 0121 cost = 0.471608341\n",
      "Epoch: 0122 cost = 0.472905993\n",
      "Epoch: 0123 cost = 0.464995712\n",
      "Epoch: 0124 cost = 0.465835363\n",
      "Epoch: 0125 cost = 0.464336738\n",
      "Epoch: 0126 cost = 0.456771523\n",
      "Epoch: 0127 cost = 0.456641480\n",
      "Epoch: 0128 cost = 0.451247051\n",
      "Epoch: 0129 cost = 0.448469445\n",
      "Epoch: 0130 cost = 0.448475376\n",
      "Epoch: 0131 cost = 0.440277502\n",
      "Epoch: 0132 cost = 0.435609326\n",
      "Epoch: 0133 cost = 0.440212965\n",
      "Epoch: 0134 cost = 0.436588660\n",
      "Epoch: 0135 cost = 0.429578945\n",
      "Epoch: 0136 cost = 0.426934063\n",
      "Epoch: 0137 cost = 0.426180601\n",
      "Epoch: 0138 cost = 0.428038180\n",
      "Epoch: 0139 cost = 0.426096782\n",
      "Epoch: 0140 cost = 0.419627592\n",
      "Epoch: 0141 cost = 0.418170258\n",
      "Epoch: 0142 cost = 0.418902114\n",
      "Epoch: 0143 cost = 0.414823577\n",
      "Epoch: 0144 cost = 0.411984265\n",
      "Epoch: 0145 cost = 0.409520268\n",
      "Epoch: 0146 cost = 0.407686576\n",
      "Epoch: 0147 cost = 0.407803670\n",
      "Epoch: 0148 cost = 0.402744070\n",
      "Epoch: 0149 cost = 0.401889071\n",
      "Epoch: 0150 cost = 0.405666664\n",
      "Epoch: 0151 cost = 0.398399279\n",
      "Epoch: 0152 cost = 0.397735879\n",
      "Epoch: 0153 cost = 0.402830392\n",
      "Epoch: 0154 cost = 0.391398072\n",
      "Epoch: 0155 cost = 0.394497156\n",
      "Epoch: 0156 cost = 0.393328160\n",
      "Epoch: 0157 cost = 0.390367612\n",
      "Epoch: 0158 cost = 0.395890936\n",
      "Epoch: 0159 cost = 0.387434751\n",
      "Epoch: 0160 cost = 0.387373537\n",
      "Epoch: 0161 cost = 0.383263916\n",
      "Epoch: 0162 cost = 0.385958850\n",
      "Epoch: 0163 cost = 0.382252470\n",
      "Epoch: 0164 cost = 0.382121414\n",
      "Epoch: 0165 cost = 0.378494084\n",
      "Epoch: 0166 cost = 0.375647232\n",
      "Epoch: 0167 cost = 0.376535952\n",
      "Epoch: 0168 cost = 0.371847883\n",
      "Epoch: 0169 cost = 0.374925613\n",
      "Epoch: 0170 cost = 0.374355093\n",
      "Epoch: 0171 cost = 0.369226515\n",
      "Epoch: 0172 cost = 0.372353479\n",
      "Epoch: 0173 cost = 0.368591562\n",
      "Epoch: 0174 cost = 0.364733070\n",
      "Epoch: 0175 cost = 0.363345534\n",
      "Epoch: 0176 cost = 0.364015222\n",
      "Epoch: 0177 cost = 0.359274119\n",
      "Epoch: 0178 cost = 0.359629735\n",
      "Epoch: 0179 cost = 0.360283494\n",
      "Epoch: 0180 cost = 0.360883102\n",
      "Epoch: 0181 cost = 0.361666113\n",
      "Epoch: 0182 cost = 0.360678032\n",
      "Epoch: 0183 cost = 0.358522192\n",
      "Epoch: 0184 cost = 0.353876233\n",
      "Epoch: 0185 cost = 0.357148871\n",
      "Epoch: 0186 cost = 0.351584375\n",
      "Epoch: 0187 cost = 0.355791897\n",
      "Epoch: 0188 cost = 0.357007623\n",
      "Epoch: 0189 cost = 0.350046411\n",
      "Epoch: 0190 cost = 0.349350587\n",
      "Epoch: 0191 cost = 0.350733936\n",
      "Epoch: 0192 cost = 0.351193726\n",
      "Epoch: 0193 cost = 0.349656582\n",
      "Epoch: 0194 cost = 0.343890637\n",
      "Epoch: 0195 cost = 0.342059001\n",
      "Epoch: 0196 cost = 0.343579471\n",
      "Epoch: 0197 cost = 0.337691963\n",
      "Epoch: 0198 cost = 0.340205953\n",
      "Epoch: 0199 cost = 0.340884477\n",
      "Accuracy:  0.9041646\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "##################  화자인식 NN 버전 ##################\n",
    "X_train, X_test, Y_train, Y_test = np.load(\"./data.npy\")\n",
    "X_train = X_train.astype(\"float\")\n",
    "X_test = X_test.astype(\"float\")\n",
    "\n",
    "# v1\n",
    "tf.reset_default_graph() # 기존에 생성된 graph를 모두 삭제하고, reset시켜 중복되는 것을 막아준다. \n",
    "                         # context가 유지되는 주피터에서는 사용해야한다.\n",
    "tf.set_random_seed(777)\n",
    "learning_rate = 0.001\n",
    "training_epochs = 200\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "sd = 1 / np.sqrt(13) # standard deviation 표준편차(표본표준편차라 1/root(n))\n",
    "\n",
    "#mfcc의 기본은 20\n",
    "# 20ms일 때216은 각 mfcc feature의 열이 216\n",
    "X = tf.placeholder(tf.float32, [None, 20])\n",
    "# \n",
    "Y = tf.placeholder(tf.float32, [None, tf_classes])\n",
    "\n",
    "# W = tf.Variable(tf.random_normal([216, 200]))\n",
    "# b = tf.Variable(tf.random_normal([200]))\n",
    "\n",
    "#1차 히든레이어\n",
    "W1 = tf.get_variable(\"w1\",\n",
    "    #tf.random_normal([216, 180], mean=0, stddev=sd),\n",
    "        shape=[20, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b1\")\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1) # 1차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "# 2차 히든 레이어\n",
    "W2 = tf.get_variable(\"w2\",\n",
    "    #tf.random_normal([180, 150], mean=0, stddev=sd),\n",
    "         shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b2\")\n",
    "L2 = tf.nn.tanh(tf.matmul(L1, W2) + b2) # 2차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "\n",
    "# 3차 히든 레이어\n",
    "W3 = tf.get_variable(\"w3\",\n",
    "    #tf.random_normal([150, 100], mean=0, stddev=sd),\n",
    "            shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b3\")\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3) # 3차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "# 4차 히든 레이어\n",
    "W4 = tf.get_variable(\"w4\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[256, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b4\")\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4) # 4차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
    "\n",
    "# 5차 히든 레이어\n",
    "W5 = tf.get_variable(\"w5\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b5\")\n",
    "L5 = tf.nn.relu(tf.matmul(L4, W5) + b5) # 5차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L5 = tf.nn.dropout(L5, keep_prob = keep_prob)\n",
    "\n",
    "# 6차 히든 레이어\n",
    "W6 = tf.get_variable(\"w6\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b6\")\n",
    "L6 = tf.nn.relu(tf.matmul(L5, W6) + b6) # 6차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L6 = tf.nn.dropout(L6, keep_prob = keep_prob)\n",
    "\n",
    "# 7차 히든 레이어\n",
    "W7 = tf.get_variable(\"w7\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b7\")\n",
    "L7 = tf.nn.relu(tf.matmul(L6, W7) + b7) # 7차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L7 = tf.nn.dropout(L7, keep_prob = keep_prob)\n",
    "\n",
    "# 최종 레이어\n",
    "W8 = tf.get_variable(\"w8\", \n",
    "    #tf.random_normal([50, tf_classes], mean=0, stddev=sd),\n",
    "            shape=[128, tf_classes],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([tf_classes], mean=0, stddev=sd), name=\"b8\")\n",
    "hypothesis = tf.matmul(L7, W8) + b8\n",
    "\n",
    "\n",
    "\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "\n",
    "\n",
    "batch_size=1\n",
    "x_len = len(X_train)\n",
    "#짝수\n",
    "if(x_len%2==0):\n",
    "    batch_size = 2\n",
    "elif(x_len%3==0):\n",
    "    batch_size = 3\n",
    "elif(x_len%4==0):\n",
    "    batch_size = 4\n",
    "else:\n",
    "    batch_size = 1\n",
    "\n",
    "split_X = np.split(X_train,batch_size)\n",
    "split_Y = np.split(Y_train,batch_size)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.344577968\n",
      "Epoch: 0001 cost = 0.340412185\n",
      "Epoch: 0002 cost = 0.335753158\n",
      "Epoch: 0003 cost = 0.339301944\n",
      "Epoch: 0004 cost = 0.335329458\n",
      "Epoch: 0005 cost = 0.335366100\n",
      "Epoch: 0006 cost = 0.335380659\n",
      "Epoch: 0007 cost = 0.334261119\n",
      "Epoch: 0008 cost = 0.335814819\n",
      "Epoch: 0009 cost = 0.336820573\n",
      "Epoch: 0010 cost = 0.333100140\n",
      "Epoch: 0011 cost = 0.329465926\n",
      "Epoch: 0012 cost = 0.334339336\n",
      "Epoch: 0013 cost = 0.329527244\n",
      "Epoch: 0014 cost = 0.327498332\n",
      "Epoch: 0015 cost = 0.327181563\n",
      "Epoch: 0016 cost = 0.327258378\n",
      "Epoch: 0017 cost = 0.324066579\n",
      "Epoch: 0018 cost = 0.326243445\n",
      "Epoch: 0019 cost = 0.326201901\n",
      "Epoch: 0020 cost = 0.323996872\n",
      "Epoch: 0021 cost = 0.328340143\n",
      "Epoch: 0022 cost = 0.321711361\n",
      "Epoch: 0023 cost = 0.319612876\n",
      "Epoch: 0024 cost = 0.321686417\n",
      "Epoch: 0025 cost = 0.321345642\n",
      "Epoch: 0026 cost = 0.319106579\n",
      "Epoch: 0027 cost = 0.319818169\n",
      "Epoch: 0028 cost = 0.320253640\n",
      "Epoch: 0029 cost = 0.318856239\n",
      "Epoch: 0030 cost = 0.315252438\n",
      "Epoch: 0031 cost = 0.318888545\n",
      "Epoch: 0032 cost = 0.314693928\n",
      "Epoch: 0033 cost = 0.316703618\n",
      "Epoch: 0034 cost = 0.316291481\n",
      "Epoch: 0035 cost = 0.314524621\n",
      "Epoch: 0036 cost = 0.314469263\n",
      "Epoch: 0037 cost = 0.309753910\n",
      "Epoch: 0038 cost = 0.315941229\n",
      "Epoch: 0039 cost = 0.312523961\n",
      "Epoch: 0040 cost = 0.309844345\n",
      "Epoch: 0041 cost = 0.309316143\n",
      "Epoch: 0042 cost = 0.307686597\n",
      "Epoch: 0043 cost = 0.311047181\n",
      "Epoch: 0044 cost = 0.308704183\n",
      "Epoch: 0045 cost = 0.307492137\n",
      "Epoch: 0046 cost = 0.309817135\n",
      "Epoch: 0047 cost = 0.305626884\n",
      "Epoch: 0048 cost = 0.303139448\n",
      "Epoch: 0049 cost = 0.303957507\n",
      "Epoch: 0050 cost = 0.303093925\n",
      "Epoch: 0051 cost = 0.304250196\n",
      "Epoch: 0052 cost = 0.301981971\n",
      "Epoch: 0053 cost = 0.306846157\n",
      "Epoch: 0054 cost = 0.300244302\n",
      "Epoch: 0055 cost = 0.303558320\n",
      "Epoch: 0056 cost = 0.300264582\n",
      "Epoch: 0057 cost = 0.302904785\n",
      "Epoch: 0058 cost = 0.299878716\n",
      "Epoch: 0059 cost = 0.301823854\n",
      "Epoch: 0060 cost = 0.298927978\n",
      "Epoch: 0061 cost = 0.298813060\n",
      "Epoch: 0062 cost = 0.299293712\n",
      "Epoch: 0063 cost = 0.293038294\n",
      "Epoch: 0064 cost = 0.294887602\n",
      "Epoch: 0065 cost = 0.298431531\n",
      "Epoch: 0066 cost = 0.295678973\n",
      "Epoch: 0067 cost = 0.292256862\n",
      "Epoch: 0068 cost = 0.294452235\n",
      "Epoch: 0069 cost = 0.293380857\n",
      "Epoch: 0070 cost = 0.297030076\n",
      "Epoch: 0071 cost = 0.291176081\n",
      "Epoch: 0072 cost = 0.298168853\n",
      "Epoch: 0073 cost = 0.293848276\n",
      "Epoch: 0074 cost = 0.291662529\n",
      "Epoch: 0075 cost = 0.290457129\n",
      "Epoch: 0076 cost = 0.289448798\n",
      "Epoch: 0077 cost = 0.289240599\n",
      "Epoch: 0078 cost = 0.291156322\n",
      "Epoch: 0079 cost = 0.289446265\n",
      "Epoch: 0080 cost = 0.290601537\n",
      "Epoch: 0081 cost = 0.292950064\n",
      "Epoch: 0082 cost = 0.289032996\n",
      "Epoch: 0083 cost = 0.287646905\n",
      "Epoch: 0084 cost = 0.286395356\n",
      "Epoch: 0085 cost = 0.285686910\n",
      "Epoch: 0086 cost = 0.285003230\n",
      "Epoch: 0087 cost = 0.285083473\n",
      "Epoch: 0088 cost = 0.285433576\n",
      "Epoch: 0089 cost = 0.286402136\n",
      "Epoch: 0090 cost = 0.284470886\n",
      "Epoch: 0091 cost = 0.284782737\n",
      "Epoch: 0092 cost = 0.283736557\n",
      "Epoch: 0093 cost = 0.285544828\n",
      "Epoch: 0094 cost = 0.281921580\n",
      "Epoch: 0095 cost = 0.281401441\n",
      "Epoch: 0096 cost = 0.278466582\n",
      "Epoch: 0097 cost = 0.281382635\n",
      "Epoch: 0098 cost = 0.280463502\n",
      "Epoch: 0099 cost = 0.280418739\n",
      "Epoch: 0100 cost = 0.281046316\n",
      "Epoch: 0101 cost = 0.277557716\n",
      "Epoch: 0102 cost = 0.280069187\n",
      "Epoch: 0103 cost = 0.280701354\n",
      "Epoch: 0104 cost = 0.277431726\n",
      "Epoch: 0105 cost = 0.276083425\n",
      "Epoch: 0106 cost = 0.279276744\n",
      "Epoch: 0107 cost = 0.280387670\n",
      "Epoch: 0108 cost = 0.276652589\n",
      "Epoch: 0109 cost = 0.272516191\n",
      "Epoch: 0110 cost = 0.275727674\n",
      "Epoch: 0111 cost = 0.276153088\n",
      "Epoch: 0112 cost = 0.276656896\n",
      "Epoch: 0113 cost = 0.272107035\n",
      "Epoch: 0114 cost = 0.272468045\n",
      "Epoch: 0115 cost = 0.273891270\n",
      "Epoch: 0116 cost = 0.273592532\n",
      "Epoch: 0117 cost = 0.273033559\n",
      "Epoch: 0118 cost = 0.272124872\n",
      "Epoch: 0119 cost = 0.273150325\n",
      "Epoch: 0120 cost = 0.273051471\n",
      "Epoch: 0121 cost = 0.271763921\n",
      "Epoch: 0122 cost = 0.274948075\n",
      "Epoch: 0123 cost = 0.270545870\n",
      "Epoch: 0124 cost = 0.271650612\n",
      "Epoch: 0125 cost = 0.274156332\n",
      "Epoch: 0126 cost = 0.267203629\n",
      "Epoch: 0127 cost = 0.269275427\n",
      "Epoch: 0128 cost = 0.265944824\n",
      "Epoch: 0129 cost = 0.272961512\n",
      "Epoch: 0130 cost = 0.268451378\n",
      "Epoch: 0131 cost = 0.271228284\n",
      "Epoch: 0132 cost = 0.267005101\n",
      "Epoch: 0133 cost = 0.269205898\n",
      "Epoch: 0134 cost = 0.264334381\n",
      "Epoch: 0135 cost = 0.266350046\n",
      "Epoch: 0136 cost = 0.263108566\n",
      "Epoch: 0137 cost = 0.267396167\n",
      "Epoch: 0138 cost = 0.262442797\n",
      "Epoch: 0139 cost = 0.264917180\n",
      "Epoch: 0140 cost = 0.267889455\n",
      "Epoch: 0141 cost = 0.262543768\n",
      "Epoch: 0142 cost = 0.265342429\n",
      "Epoch: 0143 cost = 0.260739833\n",
      "Epoch: 0144 cost = 0.264068857\n",
      "Epoch: 0145 cost = 0.264177218\n",
      "Epoch: 0146 cost = 0.265301317\n",
      "Epoch: 0147 cost = 0.263660520\n",
      "Epoch: 0148 cost = 0.261937052\n",
      "Epoch: 0149 cost = 0.263530880\n",
      "Epoch: 0150 cost = 0.260696456\n",
      "Epoch: 0151 cost = 0.259049624\n",
      "Epoch: 0152 cost = 0.259688020\n",
      "Epoch: 0153 cost = 0.261195764\n",
      "Epoch: 0154 cost = 0.262297332\n",
      "Epoch: 0155 cost = 0.257084087\n",
      "Epoch: 0156 cost = 0.258486345\n",
      "Epoch: 0157 cost = 0.252648041\n",
      "Epoch: 0158 cost = 0.257418752\n",
      "Epoch: 0159 cost = 0.258315951\n",
      "Epoch: 0160 cost = 0.256602541\n",
      "Epoch: 0161 cost = 0.256675273\n",
      "Epoch: 0162 cost = 0.255386561\n",
      "Epoch: 0163 cost = 0.255736962\n",
      "Epoch: 0164 cost = 0.255536869\n",
      "Epoch: 0165 cost = 0.258308023\n",
      "Epoch: 0166 cost = 0.256200328\n",
      "Epoch: 0167 cost = 0.253649950\n",
      "Epoch: 0168 cost = 0.252629548\n",
      "Epoch: 0169 cost = 0.255709305\n",
      "Epoch: 0170 cost = 0.253761053\n",
      "Epoch: 0171 cost = 0.253602669\n",
      "Epoch: 0172 cost = 0.251385689\n",
      "Epoch: 0173 cost = 0.249572225\n",
      "Epoch: 0174 cost = 0.250720188\n",
      "Epoch: 0175 cost = 0.250527561\n",
      "Epoch: 0176 cost = 0.253302753\n",
      "Epoch: 0177 cost = 0.250885464\n",
      "Epoch: 0178 cost = 0.251518384\n",
      "Epoch: 0179 cost = 0.254821286\n",
      "Epoch: 0180 cost = 0.248983674\n",
      "Epoch: 0181 cost = 0.250587672\n",
      "Epoch: 0182 cost = 0.246963419\n",
      "Epoch: 0183 cost = 0.252271518\n",
      "Epoch: 0184 cost = 0.250162773\n",
      "Epoch: 0185 cost = 0.253263310\n",
      "Epoch: 0186 cost = 0.253323555\n",
      "Epoch: 0187 cost = 0.249157801\n",
      "Epoch: 0188 cost = 0.249673299\n",
      "Epoch: 0189 cost = 0.254233763\n",
      "Epoch: 0190 cost = 0.249838471\n",
      "Epoch: 0191 cost = 0.250563130\n",
      "Epoch: 0192 cost = 0.246855669\n",
      "Epoch: 0193 cost = 0.246605486\n",
      "Epoch: 0194 cost = 0.245895974\n",
      "Epoch: 0195 cost = 0.247976325\n",
      "Epoch: 0196 cost = 0.248078696\n",
      "Epoch: 0197 cost = 0.247732393\n",
      "Epoch: 0198 cost = 0.247084357\n",
      "Epoch: 0199 cost = 0.245009370\n",
      "Accuracy:  0.92195994\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.247024447\n",
      "Epoch: 0001 cost = 0.243072644\n",
      "Epoch: 0002 cost = 0.248628810\n",
      "Epoch: 0003 cost = 0.247155450\n",
      "Epoch: 0004 cost = 0.241553247\n",
      "Epoch: 0005 cost = 0.245673455\n",
      "Epoch: 0006 cost = 0.246326044\n",
      "Epoch: 0007 cost = 0.245136403\n",
      "Epoch: 0008 cost = 0.240401253\n",
      "Epoch: 0009 cost = 0.242753915\n",
      "Epoch: 0010 cost = 0.241422638\n",
      "Epoch: 0011 cost = 0.242682002\n",
      "Epoch: 0012 cost = 0.239211828\n",
      "Epoch: 0013 cost = 0.245350435\n",
      "Epoch: 0014 cost = 0.238364480\n",
      "Epoch: 0015 cost = 0.243427679\n",
      "Epoch: 0016 cost = 0.243868284\n",
      "Epoch: 0017 cost = 0.240469821\n",
      "Epoch: 0018 cost = 0.238527201\n",
      "Epoch: 0019 cost = 0.239964843\n",
      "Epoch: 0020 cost = 0.245039225\n",
      "Epoch: 0021 cost = 0.241686828\n",
      "Epoch: 0022 cost = 0.238835879\n",
      "Epoch: 0023 cost = 0.241857260\n",
      "Epoch: 0024 cost = 0.240202479\n",
      "Epoch: 0025 cost = 0.238910936\n",
      "Epoch: 0026 cost = 0.240033366\n",
      "Epoch: 0027 cost = 0.239361525\n",
      "Epoch: 0028 cost = 0.239133716\n",
      "Epoch: 0029 cost = 0.235162571\n",
      "Epoch: 0030 cost = 0.240442574\n",
      "Epoch: 0031 cost = 0.236329719\n",
      "Epoch: 0032 cost = 0.238480389\n",
      "Epoch: 0033 cost = 0.240715452\n",
      "Epoch: 0034 cost = 0.241406888\n",
      "Epoch: 0035 cost = 0.248393960\n",
      "Epoch: 0036 cost = 0.240419306\n",
      "Epoch: 0037 cost = 0.238109790\n",
      "Epoch: 0038 cost = 0.240707852\n",
      "Epoch: 0039 cost = 0.241219200\n",
      "Epoch: 0040 cost = 0.235659063\n",
      "Epoch: 0041 cost = 0.238319628\n",
      "Epoch: 0042 cost = 0.236189075\n",
      "Epoch: 0043 cost = 0.236097120\n",
      "Epoch: 0044 cost = 0.233811945\n",
      "Epoch: 0045 cost = 0.232795306\n",
      "Epoch: 0046 cost = 0.238815561\n",
      "Epoch: 0047 cost = 0.234810561\n",
      "Epoch: 0048 cost = 0.235045649\n",
      "Epoch: 0049 cost = 0.234935403\n",
      "Epoch: 0050 cost = 0.236619093\n",
      "Epoch: 0051 cost = 0.237812169\n",
      "Epoch: 0052 cost = 0.235465497\n",
      "Epoch: 0053 cost = 0.233161464\n",
      "Epoch: 0054 cost = 0.233600035\n",
      "Epoch: 0055 cost = 0.235498138\n",
      "Epoch: 0056 cost = 0.232305586\n",
      "Epoch: 0057 cost = 0.231908165\n",
      "Epoch: 0058 cost = 0.232763611\n",
      "Epoch: 0059 cost = 0.231731400\n",
      "Epoch: 0060 cost = 0.231077015\n",
      "Epoch: 0061 cost = 0.230609283\n",
      "Epoch: 0062 cost = 0.230576403\n",
      "Epoch: 0063 cost = 0.232698910\n",
      "Epoch: 0064 cost = 0.230840392\n",
      "Epoch: 0065 cost = 0.230002798\n",
      "Epoch: 0066 cost = 0.231839821\n",
      "Epoch: 0067 cost = 0.231533259\n",
      "Epoch: 0068 cost = 0.231492586\n",
      "Epoch: 0069 cost = 0.231184870\n",
      "Epoch: 0070 cost = 0.226454951\n",
      "Epoch: 0071 cost = 0.229977392\n",
      "Epoch: 0072 cost = 0.229767755\n",
      "Epoch: 0073 cost = 0.229474060\n",
      "Epoch: 0074 cost = 0.227217190\n",
      "Epoch: 0075 cost = 0.227614768\n",
      "Epoch: 0076 cost = 0.228184961\n",
      "Epoch: 0077 cost = 0.229193553\n",
      "Epoch: 0078 cost = 0.230720870\n",
      "Epoch: 0079 cost = 0.226947933\n",
      "Epoch: 0080 cost = 0.227284044\n",
      "Epoch: 0081 cost = 0.230099820\n",
      "Epoch: 0082 cost = 0.230741493\n",
      "Epoch: 0083 cost = 0.229563542\n",
      "Epoch: 0084 cost = 0.227604225\n",
      "Epoch: 0085 cost = 0.227248266\n",
      "Epoch: 0086 cost = 0.226708934\n",
      "Epoch: 0087 cost = 0.227215469\n",
      "Epoch: 0088 cost = 0.223965868\n",
      "Epoch: 0089 cost = 0.225768805\n",
      "Epoch: 0090 cost = 0.224499263\n",
      "Epoch: 0091 cost = 0.225191519\n",
      "Epoch: 0092 cost = 0.226614766\n",
      "Epoch: 0093 cost = 0.221443497\n",
      "Epoch: 0094 cost = 0.223704748\n",
      "Epoch: 0095 cost = 0.223880515\n",
      "Epoch: 0096 cost = 0.222404867\n",
      "Epoch: 0097 cost = 0.225166515\n",
      "Epoch: 0098 cost = 0.224215873\n",
      "Epoch: 0099 cost = 0.220454127\n",
      "Epoch: 0100 cost = 0.223249502\n",
      "Epoch: 0101 cost = 0.225010730\n",
      "Epoch: 0102 cost = 0.221574947\n",
      "Epoch: 0103 cost = 0.222990371\n",
      "Epoch: 0104 cost = 0.225094438\n",
      "Epoch: 0105 cost = 0.225223139\n",
      "Epoch: 0106 cost = 0.223984942\n",
      "Epoch: 0107 cost = 0.221220866\n",
      "Epoch: 0108 cost = 0.225056179\n",
      "Epoch: 0109 cost = 0.221866690\n",
      "Epoch: 0110 cost = 0.222825773\n",
      "Epoch: 0111 cost = 0.223546453\n",
      "Epoch: 0112 cost = 0.221662961\n",
      "Epoch: 0113 cost = 0.219990455\n",
      "Epoch: 0114 cost = 0.220451556\n",
      "Epoch: 0115 cost = 0.220889844\n",
      "Epoch: 0116 cost = 0.222267963\n",
      "Epoch: 0117 cost = 0.225245960\n",
      "Epoch: 0118 cost = 0.221506588\n",
      "Epoch: 0119 cost = 0.222561806\n",
      "Epoch: 0120 cost = 0.222532004\n",
      "Epoch: 0121 cost = 0.218800247\n",
      "Epoch: 0122 cost = 0.221162781\n",
      "Epoch: 0123 cost = 0.218870208\n",
      "Epoch: 0124 cost = 0.218075246\n",
      "Epoch: 0125 cost = 0.217990018\n",
      "Epoch: 0126 cost = 0.222510234\n",
      "Epoch: 0127 cost = 0.218962878\n",
      "Epoch: 0128 cost = 0.220631033\n",
      "Epoch: 0129 cost = 0.220318675\n",
      "Epoch: 0130 cost = 0.221843526\n",
      "Epoch: 0131 cost = 0.217845455\n",
      "Epoch: 0132 cost = 0.217976891\n",
      "Epoch: 0133 cost = 0.214927107\n",
      "Epoch: 0134 cost = 0.213675819\n",
      "Epoch: 0135 cost = 0.216533028\n",
      "Epoch: 0136 cost = 0.214520641\n",
      "Epoch: 0137 cost = 0.213183731\n",
      "Epoch: 0138 cost = 0.216076151\n",
      "Epoch: 0139 cost = 0.213859081\n",
      "Epoch: 0140 cost = 0.212840848\n",
      "Epoch: 0141 cost = 0.212139852\n",
      "Epoch: 0142 cost = 0.216437176\n",
      "Epoch: 0143 cost = 0.214133188\n",
      "Epoch: 0144 cost = 0.213156678\n",
      "Epoch: 0145 cost = 0.215155244\n",
      "Epoch: 0146 cost = 0.220263228\n",
      "Epoch: 0147 cost = 0.217535987\n",
      "Epoch: 0148 cost = 0.213896744\n",
      "Epoch: 0149 cost = 0.217311256\n",
      "Epoch: 0150 cost = 0.213498458\n",
      "Epoch: 0151 cost = 0.214798242\n",
      "Epoch: 0152 cost = 0.215486020\n",
      "Epoch: 0153 cost = 0.216869615\n",
      "Epoch: 0154 cost = 0.214253992\n",
      "Epoch: 0155 cost = 0.213461980\n",
      "Epoch: 0156 cost = 0.215308040\n",
      "Epoch: 0157 cost = 0.215028226\n",
      "Epoch: 0158 cost = 0.213991582\n",
      "Epoch: 0159 cost = 0.215288177\n",
      "Epoch: 0160 cost = 0.212864742\n",
      "Epoch: 0161 cost = 0.210004389\n",
      "Epoch: 0162 cost = 0.210386582\n",
      "Epoch: 0163 cost = 0.211037442\n",
      "Epoch: 0164 cost = 0.210994877\n",
      "Epoch: 0165 cost = 0.211156487\n",
      "Epoch: 0166 cost = 0.209762365\n",
      "Epoch: 0167 cost = 0.212802231\n",
      "Epoch: 0168 cost = 0.209857799\n",
      "Epoch: 0169 cost = 0.209072031\n",
      "Epoch: 0170 cost = 0.211046882\n",
      "Epoch: 0171 cost = 0.210100673\n",
      "Epoch: 0172 cost = 0.211825535\n",
      "Epoch: 0173 cost = 0.212288104\n",
      "Epoch: 0174 cost = 0.210550815\n",
      "Epoch: 0175 cost = 0.211002223\n",
      "Epoch: 0176 cost = 0.212762177\n",
      "Epoch: 0177 cost = 0.215930566\n",
      "Epoch: 0178 cost = 0.209679276\n",
      "Epoch: 0179 cost = 0.213298112\n",
      "Epoch: 0180 cost = 0.209903210\n",
      "Epoch: 0181 cost = 0.212760635\n",
      "Epoch: 0182 cost = 0.210996777\n",
      "Epoch: 0183 cost = 0.210179679\n",
      "Epoch: 0184 cost = 0.208559185\n",
      "Epoch: 0185 cost = 0.211708404\n",
      "Epoch: 0186 cost = 0.212022848\n",
      "Epoch: 0187 cost = 0.213647038\n",
      "Epoch: 0188 cost = 0.210020237\n",
      "Epoch: 0189 cost = 0.210878998\n",
      "Epoch: 0190 cost = 0.212764375\n",
      "Epoch: 0191 cost = 0.206766210\n",
      "Epoch: 0192 cost = 0.206865095\n",
      "Epoch: 0193 cost = 0.207140990\n",
      "Epoch: 0194 cost = 0.203873768\n",
      "Epoch: 0195 cost = 0.207485206\n",
      "Epoch: 0196 cost = 0.205488935\n",
      "Epoch: 0197 cost = 0.207888454\n",
      "Epoch: 0198 cost = 0.209508531\n",
      "Epoch: 0199 cost = 0.208190456\n",
      "Accuracy:  0.92980725\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정 2\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_voice_model2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W100 = tf.Variable(tf.random_normal([1]), name='weight') # 저장할 w 생성\n",
    "saver = tf.train.Saver() # saver 객체 받음\n",
    "sess = tf.Session()\n",
    "saver.save(sess, './my_voice_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(503, 20)\n",
      "(503, 6)\n",
      "predict\n",
      "5    459\n",
      "2     23\n",
      "3     10\n",
      "1      7\n",
      "4      3\n",
      "0      1\n",
      "dtype: int64\n",
      "Accuracy:  0.9125249\n"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(\"./test_한예진.wav\")\n",
    "\n",
    "X_test = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "\n",
    "'''\n",
    "0 정유경\n",
    "1 배철수\n",
    "2 이윤진\n",
    "3 강정윤\n",
    "4 임찬주\n",
    "5 한예진\n",
    "'''\n",
    "label = [0 for i in range(6)]#class가 3개이니까 y_test만드는 과정\n",
    "label[5] = 1\n",
    "Y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    Y_test.append(label)\n",
    "\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"predict\")\n",
    "print(pd.value_counts(pd.Series(sess.run(tf.argmax(hypothesis, 1),\n",
    "                                    feed_dict={X: X_test, keep_prob:1}))))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
