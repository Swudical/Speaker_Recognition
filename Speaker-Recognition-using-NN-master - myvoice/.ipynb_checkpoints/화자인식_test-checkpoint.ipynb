{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foldername : 0 - 35 파일\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ImChanjoo\\anaconda3\\envs\\python37\\lib\\site-packages\\librosa\\core\\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
      "C:\\Users\\ImChanjoo\\anaconda3\\envs\\python37\\lib\\site-packages\\librosa\\core\\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foldername : 1 - 20 파일\n",
      "Foldername : 2 - 35 파일\n",
      "Foldername : 3 - 35 파일\n",
      "Foldername : 4 - 35 파일\n",
      "X_data : (86058, 13)\n",
      "Y_label : (86058, 5)\n",
      "5 개의 클래스!!\n",
      "X_train : (64543, 13)\n",
      "Y_train : (64543, 5)\n",
      "X_test : (21515, 13)\n",
      "Y_test : (21515, 5)\n"
     ]
    }
   ],
   "source": [
    "#######################Tensorflow 코드 시작부분\n",
    "import librosa\n",
    "import pyaudio #마이크를 사용하기 위한 라이브러리\n",
    "import wave\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "##### 변수 설정 부분 #####\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100 #비트레이트 설정\n",
    "CHUNK = int(RATE / 10) # 버퍼 사이즈 1초당 44100비트레이트 이므로 100ms단위\n",
    "RECORD_SECONDS = 5 #녹음할 시간 설정\n",
    "DATA_PATH = \"./data\"\n",
    "X_train = []#train_data 저장할 공간\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "tf_classes = 0\n",
    "\n",
    "def load_wave_generator(path): \n",
    "       \n",
    "    batch_waves = []\n",
    "    labels = []\n",
    "    X_data = []\n",
    "    Y_label = []    \n",
    "    global X_train, X_test, Y_train, Y_test, tf_classes\n",
    "    \n",
    "    folders = os.listdir(path)\n",
    "\n",
    "    for folder in folders:\n",
    "        if not os.path.isdir(path):continue #폴더가 아니면 continue                   \n",
    "        files = os.listdir(path+\"/\"+folder)        \n",
    "        print(\"Foldername :\",folder,\"-\",len(files),\"파일\")\n",
    "        #폴더 이름과 그 폴더에 속하는 파일 갯수 출력\n",
    "        for wav in files:\n",
    "            if not wav.endswith(\".wav\"):continue\n",
    "            else:               \n",
    "                #print(\"Filename :\",wav)#.wav 파일이 아니면 continue\n",
    "                y, sr = librosa.load(path+\"/\"+folder+\"/\"+wav)\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "              \n",
    "                X_data.extend(mfcc)\n",
    "               # print(len(mfcc))\n",
    "                \n",
    "                label = [0 for i in range(len(folders))]\n",
    "                label[tf_classes] = 1\n",
    "                \n",
    "                for i in range(len(mfcc)):\n",
    "                    Y_label.append(label)\n",
    "                #print(Y_label)\n",
    "        tf_classes = tf_classes+1\n",
    "    #end loop\n",
    "    print(\"X_data :\",np.shape(X_data))\n",
    "    print(\"Y_label :\",np.shape(Y_label))\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(np.array(X_data), np.array(Y_label))\n",
    "\n",
    "    xy = (X_train, X_test, Y_train, Y_test)\n",
    "    np.save(\"./data.npy\",xy)\n",
    "\n",
    "load_wave_generator(DATA_PATH)\n",
    "\n",
    "#t = np.array(X_train);\n",
    "#print(\"!!!!!!!!\",t,t.shape,X_train)\n",
    "print(tf_classes,\"개의 클래스!!\")\n",
    "print(\"X_train :\",np.shape(X_train))\n",
    "print(\"Y_train :\",np.shape(Y_train))\n",
    "print(\"X_test :\",np.shape(X_test))\n",
    "print(\"Y_test :\",np.shape(Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 1.891587496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-33cc6c4fd3d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_Y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[0mavg_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;31m#if(epoch%10==0):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##################  화자인식 NN 버전 ##################\n",
    "X_train, X_test, Y_train, Y_test = np.load(\"./data.npy\")\n",
    "X_train = X_train.astype(\"float\")\n",
    "X_test = X_test.astype(\"float\")\n",
    "\n",
    "# v1\n",
    "tf.reset_default_graph() # 기존에 생성된 graph를 모두 삭제하고, reset시켜 중복되는 것을 막아준다. \n",
    "                         # context가 유지되는 주피터에서는 사용해야한다.\n",
    "tf.set_random_seed(777)\n",
    "learning_rate = 0.001\n",
    "training_epochs = 200\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "sd = 1 / np.sqrt(13) # standard deviation 표준편차(표본표준편차라 1/root(n))\n",
    "\n",
    "#mfcc의 기본은 20\n",
    "# 20ms일 때216은 각 mfcc feature의 열이 216\n",
    "X = tf.placeholder(tf.float32, [None, 13])\n",
    "# \n",
    "Y = tf.placeholder(tf.float32, [None, tf_classes])\n",
    "\n",
    "# W = tf.Variable(tf.random_normal([216, 200]))\n",
    "# b = tf.Variable(tf.random_normal([200]))\n",
    "\n",
    "#1차 히든레이어\n",
    "W1 = tf.get_variable(\"w1\",\n",
    "    #tf.random_normal([216, 180], mean=0, stddev=sd),\n",
    "        shape=[13, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b1\")\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1) # 1차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "# 2차 히든 레이어\n",
    "W2 = tf.get_variable(\"w2\",\n",
    "    #tf.random_normal([180, 150], mean=0, stddev=sd),\n",
    "         shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b2\")\n",
    "L2 = tf.nn.tanh(tf.matmul(L1, W2) + b2) # 2차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "\n",
    "# 3차 히든 레이어\n",
    "W3 = tf.get_variable(\"w3\",\n",
    "    #tf.random_normal([150, 100], mean=0, stddev=sd),\n",
    "            shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b3\")\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3) # 3차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "# 4차 히든 레이어\n",
    "W4 = tf.get_variable(\"w4\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[256, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b4\")\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4) # 4차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
    "\n",
    "# 5차 히든 레이어\n",
    "W5 = tf.get_variable(\"w5\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b5\")\n",
    "L5 = tf.nn.relu(tf.matmul(L4, W5) + b5) # 5차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L5 = tf.nn.dropout(L5, keep_prob = keep_prob)\n",
    "\n",
    "# 6차 히든 레이어\n",
    "W6 = tf.get_variable(\"w6\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b6\")\n",
    "L6 = tf.nn.relu(tf.matmul(L5, W6) + b6) # 6차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L6 = tf.nn.dropout(L6, keep_prob = keep_prob)\n",
    "\n",
    "# 7차 히든 레이어\n",
    "W7 = tf.get_variable(\"w7\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b7\")\n",
    "L7 = tf.nn.relu(tf.matmul(L6, W7) + b7) # 7차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L7 = tf.nn.dropout(L7, keep_prob = keep_prob)\n",
    "\n",
    "# 최종 레이어\n",
    "W8 = tf.get_variable(\"w8\", \n",
    "    #tf.random_normal([50, tf_classes], mean=0, stddev=sd),\n",
    "            shape=[128, tf_classes],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([tf_classes], mean=0, stddev=sd), name=\"b8\")\n",
    "hypothesis = tf.matmul(L7, W8) + b8\n",
    "\n",
    "\n",
    "\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "\n",
    "\n",
    "batch_size=1\n",
    "x_len = len(X_train)\n",
    "#짝수\n",
    "if(x_len%2==0):\n",
    "    batch_size = 2\n",
    "elif(x_len%3==0):\n",
    "    batch_size = 3\n",
    "elif(x_len%4==0):\n",
    "    batch_size = 4\n",
    "else:\n",
    "    batch_size = 1\n",
    "\n",
    "split_X = np.split(X_train,batch_size)\n",
    "split_Y = np.split(Y_train,batch_size)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.692580163\n",
      "Epoch: 0001 cost = 0.692815423\n",
      "Epoch: 0002 cost = 0.690020978\n",
      "Epoch: 0003 cost = 0.686582208\n",
      "Epoch: 0004 cost = 0.684986055\n",
      "Epoch: 0005 cost = 0.683878601\n",
      "Epoch: 0006 cost = 0.683177888\n",
      "Epoch: 0007 cost = 0.678895533\n",
      "Epoch: 0008 cost = 0.674513996\n",
      "Epoch: 0009 cost = 0.676676810\n",
      "Epoch: 0010 cost = 0.672628999\n",
      "Epoch: 0011 cost = 0.672415674\n",
      "Epoch: 0012 cost = 0.670185983\n",
      "Epoch: 0013 cost = 0.668587863\n",
      "Epoch: 0014 cost = 0.664874077\n",
      "Epoch: 0015 cost = 0.662770092\n",
      "Epoch: 0016 cost = 0.661118388\n",
      "Epoch: 0017 cost = 0.657376349\n",
      "Epoch: 0018 cost = 0.657347083\n",
      "Epoch: 0019 cost = 0.653322697\n",
      "Epoch: 0020 cost = 0.653171897\n",
      "Epoch: 0021 cost = 0.652682662\n",
      "Epoch: 0022 cost = 0.652137399\n",
      "Epoch: 0023 cost = 0.651151061\n",
      "Epoch: 0024 cost = 0.647391558\n",
      "Epoch: 0025 cost = 0.647123456\n",
      "Epoch: 0026 cost = 0.643362224\n",
      "Epoch: 0027 cost = 0.639058948\n",
      "Epoch: 0028 cost = 0.641514897\n",
      "Epoch: 0029 cost = 0.641856611\n",
      "Epoch: 0030 cost = 0.641013563\n",
      "Epoch: 0031 cost = 0.637905240\n",
      "Epoch: 0032 cost = 0.632306933\n",
      "Epoch: 0033 cost = 0.631678700\n",
      "Epoch: 0034 cost = 0.631667912\n",
      "Epoch: 0035 cost = 0.631807745\n",
      "Epoch: 0036 cost = 0.628197432\n",
      "Epoch: 0037 cost = 0.630656838\n",
      "Epoch: 0038 cost = 0.627069116\n",
      "Epoch: 0039 cost = 0.629553854\n",
      "Epoch: 0040 cost = 0.625286758\n",
      "Epoch: 0041 cost = 0.626337290\n",
      "Epoch: 0042 cost = 0.621794343\n",
      "Epoch: 0043 cost = 0.620072782\n",
      "Epoch: 0044 cost = 0.618234217\n",
      "Epoch: 0045 cost = 0.618545949\n",
      "Epoch: 0046 cost = 0.617414355\n",
      "Epoch: 0047 cost = 0.612557352\n",
      "Epoch: 0048 cost = 0.615749002\n",
      "Epoch: 0049 cost = 0.616079807\n",
      "Epoch: 0050 cost = 0.609954417\n",
      "Epoch: 0051 cost = 0.610139370\n",
      "Epoch: 0052 cost = 0.609491467\n",
      "Epoch: 0053 cost = 0.610724986\n",
      "Epoch: 0054 cost = 0.604579747\n",
      "Epoch: 0055 cost = 0.607077122\n",
      "Epoch: 0056 cost = 0.603478670\n",
      "Epoch: 0057 cost = 0.599684775\n",
      "Epoch: 0058 cost = 0.600252926\n",
      "Epoch: 0059 cost = 0.598654032\n",
      "Epoch: 0060 cost = 0.598534346\n",
      "Epoch: 0061 cost = 0.597528696\n",
      "Epoch: 0062 cost = 0.596173882\n",
      "Epoch: 0063 cost = 0.595733404\n",
      "Epoch: 0064 cost = 0.596097171\n",
      "Epoch: 0065 cost = 0.593869507\n",
      "Epoch: 0066 cost = 0.594028115\n",
      "Epoch: 0067 cost = 0.590737641\n",
      "Epoch: 0068 cost = 0.589501262\n",
      "Epoch: 0069 cost = 0.591014385\n",
      "Epoch: 0070 cost = 0.588463426\n",
      "Epoch: 0071 cost = 0.589541137\n",
      "Epoch: 0072 cost = 0.589045405\n",
      "Epoch: 0073 cost = 0.587202132\n",
      "Epoch: 0074 cost = 0.585903585\n",
      "Epoch: 0075 cost = 0.585268438\n",
      "Epoch: 0076 cost = 0.581963539\n",
      "Epoch: 0077 cost = 0.581668317\n",
      "Epoch: 0078 cost = 0.580673575\n",
      "Epoch: 0079 cost = 0.582173645\n",
      "Epoch: 0080 cost = 0.581573069\n",
      "Epoch: 0081 cost = 0.579104960\n",
      "Epoch: 0082 cost = 0.579770446\n",
      "Epoch: 0083 cost = 0.574826181\n",
      "Epoch: 0084 cost = 0.578230679\n",
      "Epoch: 0085 cost = 0.576377749\n",
      "Epoch: 0086 cost = 0.578427315\n",
      "Epoch: 0087 cost = 0.572482944\n",
      "Epoch: 0088 cost = 0.573325813\n",
      "Epoch: 0089 cost = 0.570952594\n",
      "Epoch: 0090 cost = 0.570239127\n",
      "Epoch: 0091 cost = 0.569925666\n",
      "Epoch: 0092 cost = 0.568298459\n",
      "Epoch: 0093 cost = 0.568558633\n",
      "Epoch: 0094 cost = 0.569766581\n",
      "Epoch: 0095 cost = 0.568863630\n",
      "Epoch: 0096 cost = 0.565712094\n",
      "Epoch: 0097 cost = 0.566451609\n",
      "Epoch: 0098 cost = 0.563417494\n",
      "Epoch: 0099 cost = 0.563659072\n",
      "Epoch: 0100 cost = 0.562951088\n",
      "Epoch: 0101 cost = 0.559566617\n",
      "Epoch: 0102 cost = 0.563433588\n",
      "Epoch: 0103 cost = 0.562481821\n",
      "Epoch: 0104 cost = 0.559088349\n",
      "Epoch: 0105 cost = 0.556174219\n",
      "Epoch: 0106 cost = 0.557052910\n",
      "Epoch: 0107 cost = 0.555787683\n",
      "Epoch: 0108 cost = 0.558449864\n",
      "Epoch: 0109 cost = 0.555449486\n",
      "Epoch: 0110 cost = 0.557091415\n",
      "Epoch: 0111 cost = 0.559367418\n",
      "Epoch: 0112 cost = 0.555276394\n",
      "Epoch: 0113 cost = 0.557365596\n",
      "Epoch: 0114 cost = 0.550206244\n",
      "Epoch: 0115 cost = 0.549595773\n",
      "Epoch: 0116 cost = 0.549393892\n",
      "Epoch: 0117 cost = 0.552369416\n",
      "Epoch: 0118 cost = 0.553488255\n",
      "Epoch: 0119 cost = 0.552060187\n",
      "Epoch: 0120 cost = 0.546146572\n",
      "Epoch: 0121 cost = 0.547883511\n",
      "Epoch: 0122 cost = 0.547155142\n",
      "Epoch: 0123 cost = 0.547620714\n",
      "Epoch: 0124 cost = 0.543636501\n",
      "Epoch: 0125 cost = 0.544339001\n",
      "Epoch: 0126 cost = 0.544206560\n",
      "Epoch: 0127 cost = 0.543945730\n",
      "Epoch: 0128 cost = 0.543608427\n",
      "Epoch: 0129 cost = 0.543144345\n",
      "Epoch: 0130 cost = 0.541269839\n",
      "Epoch: 0131 cost = 0.540849328\n",
      "Epoch: 0132 cost = 0.538813531\n",
      "Epoch: 0133 cost = 0.539689779\n",
      "Epoch: 0134 cost = 0.539423347\n",
      "Epoch: 0135 cost = 0.536660433\n",
      "Epoch: 0136 cost = 0.538645148\n",
      "Epoch: 0137 cost = 0.535684407\n",
      "Epoch: 0138 cost = 0.538434267\n",
      "Epoch: 0139 cost = 0.534299791\n",
      "Epoch: 0140 cost = 0.536857069\n",
      "Epoch: 0141 cost = 0.533568263\n",
      "Epoch: 0142 cost = 0.536876321\n",
      "Epoch: 0143 cost = 0.534955144\n",
      "Epoch: 0144 cost = 0.531532645\n",
      "Epoch: 0145 cost = 0.533770204\n",
      "Epoch: 0146 cost = 0.530689895\n",
      "Epoch: 0147 cost = 0.532344937\n",
      "Epoch: 0148 cost = 0.533355594\n",
      "Epoch: 0149 cost = 0.535106838\n",
      "Epoch: 0150 cost = 0.530351758\n",
      "Epoch: 0151 cost = 0.527768970\n",
      "Epoch: 0152 cost = 0.528383434\n",
      "Epoch: 0153 cost = 0.527003944\n",
      "Epoch: 0154 cost = 0.526267111\n",
      "Epoch: 0155 cost = 0.528616488\n",
      "Epoch: 0156 cost = 0.527373791\n",
      "Epoch: 0157 cost = 0.526949346\n",
      "Epoch: 0158 cost = 0.527091324\n",
      "Epoch: 0159 cost = 0.524726212\n",
      "Epoch: 0160 cost = 0.526669502\n",
      "Epoch: 0161 cost = 0.522683799\n",
      "Epoch: 0162 cost = 0.521773160\n",
      "Epoch: 0163 cost = 0.520800710\n",
      "Epoch: 0164 cost = 0.523683965\n",
      "Epoch: 0165 cost = 0.522932649\n",
      "Epoch: 0166 cost = 0.522348464\n",
      "Epoch: 0167 cost = 0.523379207\n",
      "Epoch: 0168 cost = 0.518985868\n",
      "Epoch: 0169 cost = 0.521566451\n",
      "Epoch: 0170 cost = 0.516183972\n",
      "Epoch: 0171 cost = 0.517596722\n",
      "Epoch: 0172 cost = 0.518805802\n",
      "Epoch: 0173 cost = 0.515324593\n",
      "Epoch: 0174 cost = 0.520121634\n",
      "Epoch: 0175 cost = 0.518154562\n",
      "Epoch: 0176 cost = 0.517059505\n",
      "Epoch: 0177 cost = 0.516356826\n",
      "Epoch: 0178 cost = 0.514743388\n",
      "Epoch: 0179 cost = 0.512690485\n",
      "Epoch: 0180 cost = 0.516609013\n",
      "Epoch: 0181 cost = 0.513744116\n",
      "Epoch: 0182 cost = 0.513915002\n",
      "Epoch: 0183 cost = 0.512722850\n",
      "Epoch: 0184 cost = 0.510580242\n",
      "Epoch: 0185 cost = 0.513125360\n",
      "Epoch: 0186 cost = 0.508789539\n",
      "Epoch: 0187 cost = 0.511700988\n",
      "Epoch: 0188 cost = 0.512407660\n",
      "Epoch: 0189 cost = 0.506038010\n",
      "Epoch: 0190 cost = 0.509926379\n",
      "Epoch: 0191 cost = 0.507762372\n",
      "Epoch: 0192 cost = 0.509724975\n",
      "Epoch: 0193 cost = 0.509872556\n",
      "Epoch: 0194 cost = 0.510470390\n",
      "Epoch: 0195 cost = 0.507423580\n",
      "Epoch: 0196 cost = 0.507739961\n",
      "Epoch: 0197 cost = 0.505190492\n",
      "Epoch: 0198 cost = 0.505006492\n",
      "Epoch: 0199 cost = 0.503773868\n",
      "Accuracy:  0.84787357\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.504630923\n",
      "Epoch: 0001 cost = 0.505423725\n",
      "Epoch: 0002 cost = 0.506962657\n",
      "Epoch: 0003 cost = 0.503636181\n",
      "Epoch: 0004 cost = 0.506153405\n",
      "Epoch: 0005 cost = 0.504864275\n",
      "Epoch: 0006 cost = 0.501926482\n",
      "Epoch: 0007 cost = 0.502458334\n",
      "Epoch: 0008 cost = 0.500681579\n",
      "Epoch: 0009 cost = 0.501509488\n",
      "Epoch: 0010 cost = 0.502397597\n",
      "Epoch: 0011 cost = 0.501291513\n",
      "Epoch: 0012 cost = 0.495422095\n",
      "Epoch: 0013 cost = 0.499599606\n",
      "Epoch: 0014 cost = 0.499315619\n",
      "Epoch: 0015 cost = 0.499232739\n",
      "Epoch: 0016 cost = 0.505135715\n",
      "Epoch: 0017 cost = 0.499925673\n",
      "Epoch: 0018 cost = 0.494408250\n",
      "Epoch: 0019 cost = 0.497647941\n",
      "Epoch: 0020 cost = 0.498946249\n",
      "Epoch: 0021 cost = 0.498199522\n",
      "Epoch: 0022 cost = 0.496817112\n",
      "Epoch: 0023 cost = 0.496717095\n",
      "Epoch: 0024 cost = 0.493417472\n",
      "Epoch: 0025 cost = 0.496392250\n",
      "Epoch: 0026 cost = 0.496961623\n",
      "Epoch: 0027 cost = 0.496666342\n",
      "Epoch: 0028 cost = 0.495251954\n",
      "Epoch: 0029 cost = 0.495611995\n",
      "Epoch: 0030 cost = 0.496128976\n",
      "Epoch: 0031 cost = 0.495387226\n",
      "Epoch: 0032 cost = 0.491955251\n",
      "Epoch: 0033 cost = 0.494082808\n",
      "Epoch: 0034 cost = 0.498955131\n",
      "Epoch: 0035 cost = 0.496870697\n",
      "Epoch: 0036 cost = 0.493158281\n",
      "Epoch: 0037 cost = 0.491891742\n",
      "Epoch: 0038 cost = 0.492360115\n",
      "Epoch: 0039 cost = 0.493209332\n",
      "Epoch: 0040 cost = 0.493079334\n",
      "Epoch: 0041 cost = 0.491934389\n",
      "Epoch: 0042 cost = 0.489688993\n",
      "Epoch: 0043 cost = 0.490770638\n",
      "Epoch: 0044 cost = 0.489499062\n",
      "Epoch: 0045 cost = 0.487931937\n",
      "Epoch: 0046 cost = 0.487980098\n",
      "Epoch: 0047 cost = 0.490310699\n",
      "Epoch: 0048 cost = 0.489572614\n",
      "Epoch: 0049 cost = 0.490118027\n",
      "Epoch: 0050 cost = 0.486829758\n",
      "Epoch: 0051 cost = 0.489028990\n",
      "Epoch: 0052 cost = 0.484886497\n",
      "Epoch: 0053 cost = 0.487899661\n",
      "Epoch: 0054 cost = 0.484075844\n",
      "Epoch: 0055 cost = 0.484626532\n",
      "Epoch: 0056 cost = 0.484251916\n",
      "Epoch: 0057 cost = 0.480876029\n",
      "Epoch: 0058 cost = 0.483004898\n",
      "Epoch: 0059 cost = 0.482349366\n",
      "Epoch: 0060 cost = 0.482524455\n",
      "Epoch: 0061 cost = 0.480559230\n",
      "Epoch: 0062 cost = 0.481462866\n",
      "Epoch: 0063 cost = 0.483721554\n",
      "Epoch: 0064 cost = 0.480072558\n",
      "Epoch: 0065 cost = 0.477973551\n",
      "Epoch: 0066 cost = 0.480421662\n",
      "Epoch: 0067 cost = 0.480266869\n",
      "Epoch: 0068 cost = 0.479915589\n",
      "Epoch: 0069 cost = 0.478853673\n",
      "Epoch: 0070 cost = 0.479717731\n",
      "Epoch: 0071 cost = 0.484439224\n",
      "Epoch: 0072 cost = 0.479167789\n",
      "Epoch: 0073 cost = 0.478896677\n",
      "Epoch: 0074 cost = 0.480574191\n",
      "Epoch: 0075 cost = 0.477283418\n",
      "Epoch: 0076 cost = 0.480028123\n",
      "Epoch: 0077 cost = 0.475349456\n",
      "Epoch: 0078 cost = 0.476998210\n",
      "Epoch: 0079 cost = 0.476367593\n",
      "Epoch: 0080 cost = 0.480253488\n",
      "Epoch: 0081 cost = 0.476926059\n",
      "Epoch: 0082 cost = 0.478215635\n",
      "Epoch: 0083 cost = 0.479445219\n",
      "Epoch: 0084 cost = 0.474582672\n",
      "Epoch: 0085 cost = 0.477205008\n",
      "Epoch: 0086 cost = 0.477410525\n",
      "Epoch: 0087 cost = 0.473806947\n",
      "Epoch: 0088 cost = 0.472647548\n",
      "Epoch: 0089 cost = 0.473178625\n",
      "Epoch: 0090 cost = 0.474751800\n",
      "Epoch: 0091 cost = 0.475657165\n",
      "Epoch: 0092 cost = 0.475227565\n",
      "Epoch: 0093 cost = 0.472228497\n",
      "Epoch: 0094 cost = 0.472431421\n",
      "Epoch: 0095 cost = 0.473219395\n",
      "Epoch: 0096 cost = 0.472117811\n",
      "Epoch: 0097 cost = 0.471269965\n",
      "Epoch: 0098 cost = 0.469347179\n",
      "Epoch: 0099 cost = 0.475579023\n",
      "Epoch: 0100 cost = 0.474229574\n",
      "Epoch: 0101 cost = 0.471196175\n",
      "Epoch: 0102 cost = 0.471542120\n",
      "Epoch: 0103 cost = 0.471025735\n",
      "Epoch: 0104 cost = 0.470094353\n",
      "Epoch: 0105 cost = 0.466901332\n",
      "Epoch: 0106 cost = 0.470867395\n",
      "Epoch: 0107 cost = 0.469175100\n",
      "Epoch: 0108 cost = 0.472301573\n",
      "Epoch: 0109 cost = 0.466859728\n",
      "Epoch: 0110 cost = 0.467639744\n",
      "Epoch: 0111 cost = 0.469057381\n",
      "Epoch: 0112 cost = 0.468351066\n",
      "Epoch: 0113 cost = 0.465269655\n",
      "Epoch: 0114 cost = 0.464064479\n",
      "Epoch: 0115 cost = 0.469904929\n",
      "Epoch: 0116 cost = 0.463641435\n",
      "Epoch: 0117 cost = 0.462313533\n",
      "Epoch: 0118 cost = 0.464735538\n",
      "Epoch: 0119 cost = 0.468003899\n",
      "Epoch: 0120 cost = 0.469372094\n",
      "Epoch: 0121 cost = 0.464554846\n",
      "Epoch: 0122 cost = 0.468399167\n",
      "Epoch: 0123 cost = 0.466841221\n",
      "Epoch: 0124 cost = 0.467967331\n",
      "Epoch: 0125 cost = 0.465781718\n",
      "Epoch: 0126 cost = 0.463629454\n",
      "Epoch: 0127 cost = 0.468767375\n",
      "Epoch: 0128 cost = 0.468462437\n",
      "Epoch: 0129 cost = 0.464318365\n",
      "Epoch: 0130 cost = 0.460770339\n",
      "Epoch: 0131 cost = 0.465620905\n",
      "Epoch: 0132 cost = 0.463139564\n",
      "Epoch: 0133 cost = 0.457922906\n",
      "Epoch: 0134 cost = 0.465195477\n",
      "Epoch: 0135 cost = 0.462162316\n",
      "Epoch: 0136 cost = 0.462239355\n",
      "Epoch: 0137 cost = 0.463727951\n",
      "Epoch: 0138 cost = 0.462023765\n",
      "Epoch: 0139 cost = 0.461269468\n",
      "Epoch: 0140 cost = 0.457726449\n",
      "Epoch: 0141 cost = 0.459120125\n",
      "Epoch: 0142 cost = 0.458688915\n",
      "Epoch: 0143 cost = 0.459862769\n",
      "Epoch: 0144 cost = 0.457504809\n",
      "Epoch: 0145 cost = 0.460954338\n",
      "Epoch: 0146 cost = 0.459579527\n",
      "Epoch: 0147 cost = 0.460537612\n",
      "Epoch: 0148 cost = 0.462060034\n",
      "Epoch: 0149 cost = 0.459600061\n",
      "Epoch: 0150 cost = 0.459155291\n",
      "Epoch: 0151 cost = 0.455127358\n",
      "Epoch: 0152 cost = 0.457917780\n",
      "Epoch: 0153 cost = 0.457771182\n",
      "Epoch: 0154 cost = 0.458018601\n",
      "Epoch: 0155 cost = 0.459465742\n",
      "Epoch: 0156 cost = 0.454822779\n",
      "Epoch: 0157 cost = 0.460571736\n",
      "Epoch: 0158 cost = 0.458792537\n",
      "Epoch: 0159 cost = 0.454324186\n",
      "Epoch: 0160 cost = 0.459632158\n",
      "Epoch: 0161 cost = 0.455641598\n",
      "Epoch: 0162 cost = 0.455097646\n",
      "Epoch: 0163 cost = 0.456190974\n",
      "Epoch: 0164 cost = 0.454453558\n",
      "Epoch: 0165 cost = 0.450987190\n",
      "Epoch: 0166 cost = 0.456677049\n",
      "Epoch: 0167 cost = 0.455137849\n",
      "Epoch: 0168 cost = 0.453153133\n",
      "Epoch: 0169 cost = 0.450811237\n",
      "Epoch: 0170 cost = 0.454701692\n",
      "Epoch: 0171 cost = 0.453512162\n",
      "Epoch: 0172 cost = 0.451187998\n",
      "Epoch: 0173 cost = 0.450875282\n",
      "Epoch: 0174 cost = 0.453334212\n",
      "Epoch: 0175 cost = 0.450228482\n",
      "Epoch: 0176 cost = 0.454247862\n",
      "Epoch: 0177 cost = 0.454678714\n",
      "Epoch: 0178 cost = 0.453471184\n",
      "Epoch: 0179 cost = 0.450230062\n",
      "Epoch: 0180 cost = 0.448080271\n",
      "Epoch: 0181 cost = 0.451206982\n",
      "Epoch: 0182 cost = 0.450066715\n",
      "Epoch: 0183 cost = 0.450203985\n",
      "Epoch: 0184 cost = 0.448407263\n",
      "Epoch: 0185 cost = 0.449750990\n",
      "Epoch: 0186 cost = 0.445228547\n",
      "Epoch: 0187 cost = 0.448407143\n",
      "Epoch: 0188 cost = 0.447275430\n",
      "Epoch: 0189 cost = 0.449876338\n",
      "Epoch: 0190 cost = 0.445452720\n",
      "Epoch: 0191 cost = 0.444400787\n",
      "Epoch: 0192 cost = 0.446732908\n",
      "Epoch: 0193 cost = 0.442682654\n",
      "Epoch: 0194 cost = 0.446669877\n",
      "Epoch: 0195 cost = 0.445167303\n",
      "Epoch: 0196 cost = 0.449800402\n",
      "Epoch: 0197 cost = 0.448419005\n",
      "Epoch: 0198 cost = 0.443681866\n",
      "Epoch: 0199 cost = 0.444965005\n",
      "Accuracy:  0.8645596\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정 2\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_voice_model2'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './my_voice_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(533, 13)\n",
      "(533, 5)\n",
      "predict\n",
      "2    509\n",
      "3     11\n",
      "1      6\n",
      "4      5\n",
      "0      2\n",
      "dtype: int64\n",
      "Accuracy:  0.95497185\n"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(\"./test_이윤진.wav\")\n",
    "\n",
    "X_test = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "\n",
    "'''\n",
    "0 정유경\n",
    "1 배철수\n",
    "2 이윤진\n",
    "3 강정윤\n",
    "4 임찬주\n",
    "'''\n",
    "label = [0 for i in range(5)]#class가 3개이니까 y_test만드는 과정\n",
    "label[2] = 1\n",
    "Y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    Y_test.append(label)\n",
    "\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))\n",
    "\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "#print(\"Label :\",sess.run(tf.argmax(Y_test,1)))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"predict\")\n",
    "print(pd.value_counts(pd.Series(sess.run(tf.argmax(hypothesis, 1),\n",
    "                                    feed_dict={X: X_test, keep_prob:1}))))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
