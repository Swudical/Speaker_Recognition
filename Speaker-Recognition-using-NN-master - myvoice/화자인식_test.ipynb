{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foldername : 0 - 20 파일\n",
      "Foldername : 1 - 20 파일\n",
      "Foldername : 2 - 20 파일\n",
      "Foldername : 3 - 20 파일\n",
      "Foldername : 4 - 20 파일\n",
      "X_data : (54178, 20)\n",
      "Y_label : (54178, 5)\n",
      "5 개의 클래스!!\n",
      "X_train : (40633, 20)\n",
      "Y_train : (40633, 5)\n",
      "X_test : (13545, 20)\n",
      "Y_test : (13545, 5)\n"
     ]
    }
   ],
   "source": [
    "#######################Tensorflow 코드 시작부분\n",
    "import librosa\n",
    "import pyaudio #마이크를 사용하기 위한 라이브러리\n",
    "import wave\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "##### 변수 설정 부분 #####\n",
    "DATA_PATH = \"./data\"\n",
    "X_train = []#train_data 저장할 공간\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "tf_classes = 0\n",
    "\n",
    "def load_wave_generator(path): \n",
    "       \n",
    "    batch_waves = []\n",
    "    labels = []\n",
    "    X_data = []\n",
    "    Y_label = []    \n",
    "    global X_train, X_test, Y_train, Y_test, tf_classes\n",
    "    \n",
    "    folders = os.listdir(path)\n",
    "\n",
    "    for folder in folders:\n",
    "        if not os.path.isdir(path):continue #폴더가 아니면 continue                   \n",
    "        files = os.listdir(path+\"/\"+folder)        \n",
    "        print(\"Foldername :\",folder,\"-\",len(files),\"파일\")\n",
    "        #폴더 이름과 그 폴더에 속하는 파일 갯수 출력\n",
    "        for wav in files:\n",
    "            if not wav.endswith(\".wav\"):continue\n",
    "            else:               \n",
    "                #print(\"Filename :\",wav)#.wav 파일이 아니면 continue\n",
    "                y, sr = librosa.load(path+\"/\"+folder+\"/\"+wav)\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "                \n",
    "                X_data.extend(mfcc)\n",
    "               # print(len(mfcc))\n",
    "                \n",
    "                label = [0 for i in range(len(folders))]\n",
    "                label[tf_classes] = 1\n",
    "                \n",
    "                for i in range(len(mfcc)):\n",
    "                    Y_label.append(label)\n",
    "                #print(Y_label)\n",
    "        tf_classes = tf_classes+1\n",
    "    #end loop\n",
    "    print(\"X_data :\",np.shape(X_data))\n",
    "    print(\"Y_label :\",np.shape(Y_label))\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(np.array(X_data), np.array(Y_label))\n",
    "\n",
    "    xy = (X_train, X_test, Y_train, Y_test)\n",
    "    np.save(\"./data.npy\",xy)\n",
    "\n",
    "load_wave_generator(DATA_PATH)\n",
    "\n",
    "#t = np.array(X_train);\n",
    "#print(\"!!!!!!!!\",t,t.shape,X_train)\n",
    "print(tf_classes,\"개의 클래스!!\")\n",
    "print(\"X_train :\",np.shape(X_train))\n",
    "print(\"Y_train :\",np.shape(Y_train))\n",
    "print(\"X_test :\",np.shape(X_test))\n",
    "print(\"Y_test :\",np.shape(Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 1.878391266\n",
      "Epoch: 0001 cost = 1.734129310\n",
      "Epoch: 0002 cost = 1.693499923\n",
      "Epoch: 0003 cost = 1.675191641\n",
      "Epoch: 0004 cost = 1.657896996\n",
      "Epoch: 0005 cost = 1.646477103\n",
      "Epoch: 0006 cost = 1.637713671\n",
      "Epoch: 0007 cost = 1.635173202\n",
      "Epoch: 0008 cost = 1.626531243\n",
      "Epoch: 0009 cost = 1.627596855\n",
      "Epoch: 0010 cost = 1.622805476\n",
      "Epoch: 0011 cost = 1.620999455\n",
      "Epoch: 0012 cost = 1.620400190\n",
      "Epoch: 0013 cost = 1.618559718\n",
      "Epoch: 0014 cost = 1.614760518\n",
      "Epoch: 0015 cost = 1.613138080\n",
      "Epoch: 0016 cost = 1.610566735\n",
      "Epoch: 0017 cost = 1.609061956\n",
      "Epoch: 0018 cost = 1.606838346\n",
      "Epoch: 0019 cost = 1.600846648\n",
      "Epoch: 0020 cost = 1.598018527\n",
      "Epoch: 0021 cost = 1.594308496\n",
      "Epoch: 0022 cost = 1.588436961\n",
      "Epoch: 0023 cost = 1.579231143\n",
      "Epoch: 0024 cost = 1.572331786\n",
      "Epoch: 0025 cost = 1.561043382\n",
      "Epoch: 0026 cost = 1.550191045\n",
      "Epoch: 0027 cost = 1.540086150\n",
      "Epoch: 0028 cost = 1.528746605\n",
      "Epoch: 0029 cost = 1.519275784\n",
      "Epoch: 0030 cost = 1.509343624\n",
      "Epoch: 0031 cost = 1.502238750\n",
      "Epoch: 0032 cost = 1.492956400\n",
      "Epoch: 0033 cost = 1.488041043\n",
      "Epoch: 0034 cost = 1.482755184\n",
      "Epoch: 0035 cost = 1.472632647\n",
      "Epoch: 0036 cost = 1.465787649\n",
      "Epoch: 0037 cost = 1.460375547\n",
      "Epoch: 0038 cost = 1.454396844\n",
      "Epoch: 0039 cost = 1.445201755\n",
      "Epoch: 0040 cost = 1.437372804\n",
      "Epoch: 0041 cost = 1.427656889\n",
      "Epoch: 0042 cost = 1.414859176\n",
      "Epoch: 0043 cost = 1.403368711\n",
      "Epoch: 0044 cost = 1.394846439\n",
      "Epoch: 0045 cost = 1.379984617\n",
      "Epoch: 0046 cost = 1.365675688\n",
      "Epoch: 0047 cost = 1.354715228\n",
      "Epoch: 0048 cost = 1.345488071\n",
      "Epoch: 0049 cost = 1.329143286\n",
      "Epoch: 0050 cost = 1.317514539\n",
      "Epoch: 0051 cost = 1.303912520\n",
      "Epoch: 0052 cost = 1.289541483\n",
      "Epoch: 0053 cost = 1.274303675\n",
      "Epoch: 0054 cost = 1.264294982\n",
      "Epoch: 0055 cost = 1.245827317\n",
      "Epoch: 0056 cost = 1.234875321\n",
      "Epoch: 0057 cost = 1.220365882\n",
      "Epoch: 0058 cost = 1.209323645\n",
      "Epoch: 0059 cost = 1.198263884\n",
      "Epoch: 0060 cost = 1.190926075\n",
      "Epoch: 0061 cost = 1.174911022\n",
      "Epoch: 0062 cost = 1.163039446\n",
      "Epoch: 0063 cost = 1.148126602\n",
      "Epoch: 0064 cost = 1.141267896\n",
      "Epoch: 0065 cost = 1.124974847\n",
      "Epoch: 0066 cost = 1.117841363\n",
      "Epoch: 0067 cost = 1.107240200\n",
      "Epoch: 0068 cost = 1.097812891\n",
      "Epoch: 0069 cost = 1.085750818\n",
      "Epoch: 0070 cost = 1.077254653\n",
      "Epoch: 0071 cost = 1.068868160\n",
      "Epoch: 0072 cost = 1.060858607\n",
      "Epoch: 0073 cost = 1.047349811\n",
      "Epoch: 0074 cost = 1.043779254\n",
      "Epoch: 0075 cost = 1.034689188\n",
      "Epoch: 0076 cost = 1.025172591\n",
      "Epoch: 0077 cost = 1.015566230\n",
      "Epoch: 0078 cost = 1.001675487\n",
      "Epoch: 0079 cost = 1.002471685\n",
      "Epoch: 0080 cost = 0.992478788\n",
      "Epoch: 0081 cost = 0.989243448\n",
      "Epoch: 0082 cost = 0.976976335\n",
      "Epoch: 0083 cost = 0.975109994\n",
      "Epoch: 0084 cost = 0.964950740\n",
      "Epoch: 0085 cost = 0.955105960\n",
      "Epoch: 0086 cost = 0.951635122\n",
      "Epoch: 0087 cost = 0.943200052\n",
      "Epoch: 0088 cost = 0.930961192\n",
      "Epoch: 0089 cost = 0.928979576\n",
      "Epoch: 0090 cost = 0.920596898\n",
      "Epoch: 0091 cost = 0.919589102\n",
      "Epoch: 0092 cost = 0.910024762\n",
      "Epoch: 0093 cost = 0.898956537\n",
      "Epoch: 0094 cost = 0.893882632\n",
      "Epoch: 0095 cost = 0.888626337\n",
      "Epoch: 0096 cost = 0.883034110\n",
      "Epoch: 0097 cost = 0.874351084\n",
      "Epoch: 0098 cost = 0.873026431\n",
      "Epoch: 0099 cost = 0.861390293\n",
      "Epoch: 0100 cost = 0.855796158\n",
      "Epoch: 0101 cost = 0.849408627\n",
      "Epoch: 0102 cost = 0.843983412\n",
      "Epoch: 0103 cost = 0.836963654\n",
      "Epoch: 0104 cost = 0.833117068\n",
      "Epoch: 0105 cost = 0.823634326\n",
      "Epoch: 0106 cost = 0.820505440\n",
      "Epoch: 0107 cost = 0.808877826\n",
      "Epoch: 0108 cost = 0.800825298\n",
      "Epoch: 0109 cost = 0.794648051\n",
      "Epoch: 0110 cost = 0.791151226\n",
      "Epoch: 0111 cost = 0.782367170\n",
      "Epoch: 0112 cost = 0.768131554\n",
      "Epoch: 0113 cost = 0.762348056\n",
      "Epoch: 0114 cost = 0.762735248\n",
      "Epoch: 0115 cost = 0.753295064\n",
      "Epoch: 0116 cost = 0.744593680\n",
      "Epoch: 0117 cost = 0.738313437\n",
      "Epoch: 0118 cost = 0.733927608\n",
      "Epoch: 0119 cost = 0.720549762\n",
      "Epoch: 0120 cost = 0.715599656\n",
      "Epoch: 0121 cost = 0.704838514\n",
      "Epoch: 0122 cost = 0.701151550\n",
      "Epoch: 0123 cost = 0.695163131\n",
      "Epoch: 0124 cost = 0.683356285\n",
      "Epoch: 0125 cost = 0.676882565\n",
      "Epoch: 0126 cost = 0.673309445\n",
      "Epoch: 0127 cost = 0.659022272\n",
      "Epoch: 0128 cost = 0.657632351\n",
      "Epoch: 0129 cost = 0.651820183\n",
      "Epoch: 0130 cost = 0.644011617\n",
      "Epoch: 0131 cost = 0.633740485\n",
      "Epoch: 0132 cost = 0.626028895\n",
      "Epoch: 0133 cost = 0.618196428\n",
      "Epoch: 0134 cost = 0.615498602\n",
      "Epoch: 0135 cost = 0.607488632\n",
      "Epoch: 0136 cost = 0.599733949\n",
      "Epoch: 0137 cost = 0.595732391\n",
      "Epoch: 0138 cost = 0.589924157\n",
      "Epoch: 0139 cost = 0.590404332\n",
      "Epoch: 0140 cost = 0.587635159\n",
      "Epoch: 0141 cost = 0.578493893\n",
      "Epoch: 0142 cost = 0.572783351\n",
      "Epoch: 0143 cost = 0.564202249\n",
      "Epoch: 0144 cost = 0.564477980\n",
      "Epoch: 0145 cost = 0.558368802\n",
      "Epoch: 0146 cost = 0.550797105\n",
      "Epoch: 0147 cost = 0.553666234\n",
      "Epoch: 0148 cost = 0.543708980\n",
      "Epoch: 0149 cost = 0.540922105\n",
      "Epoch: 0150 cost = 0.537339449\n",
      "Epoch: 0151 cost = 0.536025524\n",
      "Epoch: 0152 cost = 0.530757546\n",
      "Epoch: 0153 cost = 0.528837025\n",
      "Epoch: 0154 cost = 0.523500562\n",
      "Epoch: 0155 cost = 0.516658664\n",
      "Epoch: 0156 cost = 0.515544057\n",
      "Epoch: 0157 cost = 0.507767618\n",
      "Epoch: 0158 cost = 0.504751682\n",
      "Epoch: 0159 cost = 0.505125523\n",
      "Epoch: 0160 cost = 0.500977218\n",
      "Epoch: 0161 cost = 0.500097692\n",
      "Epoch: 0162 cost = 0.497538656\n",
      "Epoch: 0163 cost = 0.494355381\n",
      "Epoch: 0164 cost = 0.491268933\n",
      "Epoch: 0165 cost = 0.487270683\n",
      "Epoch: 0166 cost = 0.485122055\n",
      "Epoch: 0167 cost = 0.477917701\n",
      "Epoch: 0168 cost = 0.482414752\n",
      "Epoch: 0169 cost = 0.473486096\n",
      "Epoch: 0170 cost = 0.474920809\n",
      "Epoch: 0171 cost = 0.467808932\n",
      "Epoch: 0172 cost = 0.468040615\n",
      "Epoch: 0173 cost = 0.463215709\n",
      "Epoch: 0174 cost = 0.464481562\n",
      "Epoch: 0175 cost = 0.458551824\n",
      "Epoch: 0176 cost = 0.452739894\n",
      "Epoch: 0177 cost = 0.456512690\n",
      "Epoch: 0178 cost = 0.450229377\n",
      "Epoch: 0179 cost = 0.450177938\n",
      "Epoch: 0180 cost = 0.448951721\n",
      "Epoch: 0181 cost = 0.444041222\n",
      "Epoch: 0182 cost = 0.444336385\n",
      "Epoch: 0183 cost = 0.436234772\n",
      "Epoch: 0184 cost = 0.439149767\n",
      "Epoch: 0185 cost = 0.435944527\n",
      "Epoch: 0186 cost = 0.429556578\n",
      "Epoch: 0187 cost = 0.425170690\n",
      "Epoch: 0188 cost = 0.428472012\n",
      "Epoch: 0189 cost = 0.427329123\n",
      "Epoch: 0190 cost = 0.423384517\n",
      "Epoch: 0191 cost = 0.421153456\n",
      "Epoch: 0192 cost = 0.415373743\n",
      "Epoch: 0193 cost = 0.423022807\n",
      "Epoch: 0194 cost = 0.415022373\n",
      "Epoch: 0195 cost = 0.410535693\n",
      "Epoch: 0196 cost = 0.414497942\n",
      "Epoch: 0197 cost = 0.407812685\n",
      "Epoch: 0198 cost = 0.408930629\n",
      "Epoch: 0199 cost = 0.403020769\n",
      "Accuracy:  0.87929124\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "##################  화자인식 NN 버전 ##################\n",
    "X_train, X_test, Y_train, Y_test = np.load(\"./data.npy\")\n",
    "X_train = X_train.astype(\"float\")\n",
    "X_test = X_test.astype(\"float\")\n",
    "\n",
    "# v1\n",
    "tf.reset_default_graph() # 기존에 생성된 graph를 모두 삭제하고, reset시켜 중복되는 것을 막아준다. \n",
    "                         # context가 유지되는 주피터에서는 사용해야한다.\n",
    "tf.set_random_seed(777)\n",
    "learning_rate = 0.001\n",
    "training_epochs = 200\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "sd = 1 / np.sqrt(20) # standard deviation 표준편차(표본표준편차라 1/root(n))\n",
    "\n",
    "#mfcc의 기본은 20\n",
    "# 20ms일 때216은 각 mfcc feature의 열이 216\n",
    "X = tf.placeholder(tf.float32, [None, 20])\n",
    "# \n",
    "Y = tf.placeholder(tf.float32, [None, tf_classes])\n",
    "\n",
    "# W = tf.Variable(tf.random_normal([216, 200]))\n",
    "# b = tf.Variable(tf.random_normal([200]))\n",
    "\n",
    "#1차 히든레이어\n",
    "W1 = tf.get_variable(\"w1\",\n",
    "    #tf.random_normal([216, 180], mean=0, stddev=sd),\n",
    "        shape=[20, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b1\")\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1) # 1차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "# 2차 히든 레이어\n",
    "W2 = tf.get_variable(\"w2\",\n",
    "    #tf.random_normal([180, 150], mean=0, stddev=sd),\n",
    "         shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b2\")\n",
    "L2 = tf.nn.tanh(tf.matmul(L1, W2) + b2) # 2차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "\n",
    "# 3차 히든 레이어\n",
    "W3 = tf.get_variable(\"w3\",\n",
    "    #tf.random_normal([150, 100], mean=0, stddev=sd),\n",
    "            shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b3\")\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3) # 3차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "# 4차 히든 레이어\n",
    "W4 = tf.get_variable(\"w4\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[256, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b4\")\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4) # 4차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
    "\n",
    "# 5차 히든 레이어\n",
    "W5 = tf.get_variable(\"w5\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b5\")\n",
    "L5 = tf.nn.relu(tf.matmul(L4, W5) + b5) # 5차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L5 = tf.nn.dropout(L5, keep_prob = keep_prob)\n",
    "\n",
    "# 6차 히든 레이어\n",
    "W6 = tf.get_variable(\"w6\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b6\")\n",
    "L6 = tf.nn.relu(tf.matmul(L5, W6) + b6) # 6차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L6 = tf.nn.dropout(L6, keep_prob = keep_prob)\n",
    "\n",
    "# 7차 히든 레이어\n",
    "W7 = tf.get_variable(\"w7\",\n",
    "    #tf.random_normal([100, 50], mean=0, stddev=sd),\n",
    "             shape=[128, 128],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b7\")\n",
    "L7 = tf.nn.relu(tf.matmul(L6, W7) + b7) # 7차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L7 = tf.nn.dropout(L7, keep_prob = keep_prob)\n",
    "\n",
    "# 최종 레이어\n",
    "W8 = tf.get_variable(\"w8\", \n",
    "    #tf.random_normal([50, tf_classes], mean=0, stddev=sd),\n",
    "            shape=[128, tf_classes],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([tf_classes], mean=0, stddev=sd), name=\"b8\")\n",
    "hypothesis = tf.matmul(L7, W8) + b8\n",
    "\n",
    "\n",
    "\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "\n",
    "\n",
    "batch_size=1\n",
    "x_len = len(X_train)\n",
    "#짝수\n",
    "if(x_len%2==0):\n",
    "    batch_size = 2\n",
    "elif(x_len%3==0):\n",
    "    batch_size = 3\n",
    "elif(x_len%4==0):\n",
    "    batch_size = 4\n",
    "else:\n",
    "    batch_size = 1\n",
    "\n",
    "split_X = np.split(X_train,batch_size)\n",
    "split_Y = np.split(Y_train,batch_size)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.407995820\n",
      "Epoch: 0001 cost = 0.401891917\n",
      "Epoch: 0002 cost = 0.399526179\n",
      "Epoch: 0003 cost = 0.398920059\n",
      "Epoch: 0004 cost = 0.395492345\n",
      "Epoch: 0005 cost = 0.393142670\n",
      "Epoch: 0006 cost = 0.394861847\n",
      "Epoch: 0007 cost = 0.390815854\n",
      "Epoch: 0008 cost = 0.388725936\n",
      "Epoch: 0009 cost = 0.386350244\n",
      "Epoch: 0010 cost = 0.386486977\n",
      "Epoch: 0011 cost = 0.383720458\n",
      "Epoch: 0012 cost = 0.379267454\n",
      "Epoch: 0013 cost = 0.382202148\n",
      "Epoch: 0014 cost = 0.375690430\n",
      "Epoch: 0015 cost = 0.376492321\n",
      "Epoch: 0016 cost = 0.382158220\n",
      "Epoch: 0017 cost = 0.377438933\n",
      "Epoch: 0018 cost = 0.375201941\n",
      "Epoch: 0019 cost = 0.371289164\n",
      "Epoch: 0020 cost = 0.368444264\n",
      "Epoch: 0021 cost = 0.368222177\n",
      "Epoch: 0022 cost = 0.366998792\n",
      "Epoch: 0023 cost = 0.373307526\n",
      "Epoch: 0024 cost = 0.367742032\n",
      "Epoch: 0025 cost = 0.365212321\n",
      "Epoch: 0026 cost = 0.362505466\n",
      "Epoch: 0027 cost = 0.361817151\n",
      "Epoch: 0028 cost = 0.362353921\n",
      "Epoch: 0029 cost = 0.360128850\n",
      "Epoch: 0030 cost = 0.355395585\n",
      "Epoch: 0031 cost = 0.358767420\n",
      "Epoch: 0032 cost = 0.352597862\n",
      "Epoch: 0033 cost = 0.356350809\n",
      "Epoch: 0034 cost = 0.355759859\n",
      "Epoch: 0035 cost = 0.352633417\n",
      "Epoch: 0036 cost = 0.349031270\n",
      "Epoch: 0037 cost = 0.348121673\n",
      "Epoch: 0038 cost = 0.347001135\n",
      "Epoch: 0039 cost = 0.347794890\n",
      "Epoch: 0040 cost = 0.349692404\n",
      "Epoch: 0041 cost = 0.344855487\n",
      "Epoch: 0042 cost = 0.347127944\n",
      "Epoch: 0043 cost = 0.346588045\n",
      "Epoch: 0044 cost = 0.343490541\n",
      "Epoch: 0045 cost = 0.345603108\n",
      "Epoch: 0046 cost = 0.341352403\n",
      "Epoch: 0047 cost = 0.340318799\n",
      "Epoch: 0048 cost = 0.338645577\n",
      "Epoch: 0049 cost = 0.341147363\n",
      "Epoch: 0050 cost = 0.335685998\n",
      "Epoch: 0051 cost = 0.339394301\n",
      "Epoch: 0052 cost = 0.336587459\n",
      "Epoch: 0053 cost = 0.330918193\n",
      "Epoch: 0054 cost = 0.333125591\n",
      "Epoch: 0055 cost = 0.332971692\n",
      "Epoch: 0056 cost = 0.332817495\n",
      "Epoch: 0057 cost = 0.330707669\n",
      "Epoch: 0058 cost = 0.331711322\n",
      "Epoch: 0059 cost = 0.329872817\n",
      "Epoch: 0060 cost = 0.328666329\n",
      "Epoch: 0061 cost = 0.329105347\n",
      "Epoch: 0062 cost = 0.331213027\n",
      "Epoch: 0063 cost = 0.324197263\n",
      "Epoch: 0064 cost = 0.326559007\n",
      "Epoch: 0065 cost = 0.324200660\n",
      "Epoch: 0066 cost = 0.321991533\n",
      "Epoch: 0067 cost = 0.323300272\n",
      "Epoch: 0068 cost = 0.321741343\n",
      "Epoch: 0069 cost = 0.322483689\n",
      "Epoch: 0070 cost = 0.323522538\n",
      "Epoch: 0071 cost = 0.317785084\n",
      "Epoch: 0072 cost = 0.319214970\n",
      "Epoch: 0073 cost = 0.317238063\n",
      "Epoch: 0074 cost = 0.316239595\n",
      "Epoch: 0075 cost = 0.316181093\n",
      "Epoch: 0076 cost = 0.315966487\n",
      "Epoch: 0077 cost = 0.319126785\n",
      "Epoch: 0078 cost = 0.316931546\n",
      "Epoch: 0079 cost = 0.319490075\n",
      "Epoch: 0080 cost = 0.313179940\n",
      "Epoch: 0081 cost = 0.311270148\n",
      "Epoch: 0082 cost = 0.311926037\n",
      "Epoch: 0083 cost = 0.309007138\n",
      "Epoch: 0084 cost = 0.308159083\n",
      "Epoch: 0085 cost = 0.308994323\n",
      "Epoch: 0086 cost = 0.307666391\n",
      "Epoch: 0087 cost = 0.309067011\n",
      "Epoch: 0088 cost = 0.306815505\n",
      "Epoch: 0089 cost = 0.307141572\n",
      "Epoch: 0090 cost = 0.305876553\n",
      "Epoch: 0091 cost = 0.305973500\n",
      "Epoch: 0092 cost = 0.305106044\n",
      "Epoch: 0093 cost = 0.304903984\n",
      "Epoch: 0094 cost = 0.302230805\n",
      "Epoch: 0095 cost = 0.302710056\n",
      "Epoch: 0096 cost = 0.302394092\n",
      "Epoch: 0097 cost = 0.298821628\n",
      "Epoch: 0098 cost = 0.300848097\n",
      "Epoch: 0099 cost = 0.297580451\n",
      "Epoch: 0100 cost = 0.300114691\n",
      "Epoch: 0101 cost = 0.296954006\n",
      "Epoch: 0102 cost = 0.298915446\n",
      "Epoch: 0103 cost = 0.296818763\n",
      "Epoch: 0104 cost = 0.295103312\n",
      "Epoch: 0105 cost = 0.293486029\n",
      "Epoch: 0106 cost = 0.299110085\n",
      "Epoch: 0107 cost = 0.295418084\n",
      "Epoch: 0108 cost = 0.291755915\n",
      "Epoch: 0109 cost = 0.293157905\n",
      "Epoch: 0110 cost = 0.294992089\n",
      "Epoch: 0111 cost = 0.295530856\n",
      "Epoch: 0112 cost = 0.295150429\n",
      "Epoch: 0113 cost = 0.291945875\n",
      "Epoch: 0114 cost = 0.288352698\n",
      "Epoch: 0115 cost = 0.293440729\n",
      "Epoch: 0116 cost = 0.293217629\n",
      "Epoch: 0117 cost = 0.291694134\n",
      "Epoch: 0118 cost = 0.287273824\n",
      "Epoch: 0119 cost = 0.289646626\n",
      "Epoch: 0120 cost = 0.291135579\n",
      "Epoch: 0121 cost = 0.284502923\n",
      "Epoch: 0122 cost = 0.285310477\n",
      "Epoch: 0123 cost = 0.290632039\n",
      "Epoch: 0124 cost = 0.290106148\n",
      "Epoch: 0125 cost = 0.288402110\n",
      "Epoch: 0126 cost = 0.285087466\n",
      "Epoch: 0127 cost = 0.285836011\n",
      "Epoch: 0128 cost = 0.284642935\n",
      "Epoch: 0129 cost = 0.282264620\n",
      "Epoch: 0130 cost = 0.285917878\n",
      "Epoch: 0131 cost = 0.282632232\n",
      "Epoch: 0132 cost = 0.282095701\n",
      "Epoch: 0133 cost = 0.284560055\n",
      "Epoch: 0134 cost = 0.286166906\n",
      "Epoch: 0135 cost = 0.280122578\n",
      "Epoch: 0136 cost = 0.277712107\n",
      "Epoch: 0137 cost = 0.279338598\n",
      "Epoch: 0138 cost = 0.280837387\n",
      "Epoch: 0139 cost = 0.281265587\n",
      "Epoch: 0140 cost = 0.279920757\n",
      "Epoch: 0141 cost = 0.280826688\n",
      "Epoch: 0142 cost = 0.278331429\n",
      "Epoch: 0143 cost = 0.278446168\n",
      "Epoch: 0144 cost = 0.276441425\n",
      "Epoch: 0145 cost = 0.277035058\n",
      "Epoch: 0146 cost = 0.275695860\n",
      "Epoch: 0147 cost = 0.278654307\n",
      "Epoch: 0148 cost = 0.269965053\n",
      "Epoch: 0149 cost = 0.274557292\n",
      "Epoch: 0150 cost = 0.275869697\n",
      "Epoch: 0151 cost = 0.268652767\n",
      "Epoch: 0152 cost = 0.272716433\n",
      "Epoch: 0153 cost = 0.270607710\n",
      "Epoch: 0154 cost = 0.270647138\n",
      "Epoch: 0155 cost = 0.271083504\n",
      "Epoch: 0156 cost = 0.270968914\n",
      "Epoch: 0157 cost = 0.273971856\n",
      "Epoch: 0158 cost = 0.272347599\n",
      "Epoch: 0159 cost = 0.271401197\n",
      "Epoch: 0160 cost = 0.269317627\n",
      "Epoch: 0161 cost = 0.269163728\n",
      "Epoch: 0162 cost = 0.267765671\n",
      "Epoch: 0163 cost = 0.270316541\n",
      "Epoch: 0164 cost = 0.267025203\n",
      "Epoch: 0165 cost = 0.272807658\n",
      "Epoch: 0166 cost = 0.269166797\n",
      "Epoch: 0167 cost = 0.269484758\n",
      "Epoch: 0168 cost = 0.262728393\n",
      "Epoch: 0169 cost = 0.272742301\n",
      "Epoch: 0170 cost = 0.265086412\n",
      "Epoch: 0171 cost = 0.265384674\n",
      "Epoch: 0172 cost = 0.261778027\n",
      "Epoch: 0173 cost = 0.264543623\n",
      "Epoch: 0174 cost = 0.265484512\n",
      "Epoch: 0175 cost = 0.267261446\n",
      "Epoch: 0176 cost = 0.264245152\n",
      "Epoch: 0177 cost = 0.265430629\n",
      "Epoch: 0178 cost = 0.261169046\n",
      "Epoch: 0179 cost = 0.267033666\n",
      "Epoch: 0180 cost = 0.259594083\n",
      "Epoch: 0181 cost = 0.259943962\n",
      "Epoch: 0182 cost = 0.265379936\n",
      "Epoch: 0183 cost = 0.259152681\n",
      "Epoch: 0184 cost = 0.262424737\n",
      "Epoch: 0185 cost = 0.261886150\n",
      "Epoch: 0186 cost = 0.257553846\n",
      "Epoch: 0187 cost = 0.253861159\n",
      "Epoch: 0188 cost = 0.259099513\n",
      "Epoch: 0189 cost = 0.257141411\n",
      "Epoch: 0190 cost = 0.258300006\n",
      "Epoch: 0191 cost = 0.256749064\n",
      "Epoch: 0192 cost = 0.257260054\n",
      "Epoch: 0193 cost = 0.259850353\n",
      "Epoch: 0194 cost = 0.256721944\n",
      "Epoch: 0195 cost = 0.258443952\n",
      "Epoch: 0196 cost = 0.257887304\n",
      "Epoch: 0197 cost = 0.255417556\n",
      "Epoch: 0198 cost = 0.256847560\n",
      "Epoch: 0199 cost = 0.258568913\n",
      "Accuracy:  0.9131045\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.251638025\n",
      "Epoch: 0001 cost = 0.254545093\n",
      "Epoch: 0002 cost = 0.253473490\n",
      "Epoch: 0003 cost = 0.254916757\n",
      "Epoch: 0004 cost = 0.255413055\n",
      "Epoch: 0005 cost = 0.249968037\n",
      "Epoch: 0006 cost = 0.255346507\n",
      "Epoch: 0007 cost = 0.252484292\n",
      "Epoch: 0008 cost = 0.253185779\n",
      "Epoch: 0009 cost = 0.251344293\n",
      "Epoch: 0010 cost = 0.249671608\n",
      "Epoch: 0011 cost = 0.253867328\n",
      "Epoch: 0012 cost = 0.250390291\n",
      "Epoch: 0013 cost = 0.251148999\n",
      "Epoch: 0014 cost = 0.252305448\n",
      "Epoch: 0015 cost = 0.253163755\n",
      "Epoch: 0016 cost = 0.246926621\n",
      "Epoch: 0017 cost = 0.251121789\n",
      "Epoch: 0018 cost = 0.248001501\n",
      "Epoch: 0019 cost = 0.251560181\n",
      "Epoch: 0020 cost = 0.247300550\n",
      "Epoch: 0021 cost = 0.248516560\n",
      "Epoch: 0022 cost = 0.248166442\n",
      "Epoch: 0023 cost = 0.248024672\n",
      "Epoch: 0024 cost = 0.247982278\n",
      "Epoch: 0025 cost = 0.243786588\n",
      "Epoch: 0026 cost = 0.246818632\n",
      "Epoch: 0027 cost = 0.246295586\n",
      "Epoch: 0028 cost = 0.244948179\n",
      "Epoch: 0029 cost = 0.244970649\n",
      "Epoch: 0030 cost = 0.248966575\n",
      "Epoch: 0031 cost = 0.244330108\n",
      "Epoch: 0032 cost = 0.247686461\n",
      "Epoch: 0033 cost = 0.247000635\n",
      "Epoch: 0034 cost = 0.246198609\n",
      "Epoch: 0035 cost = 0.242512047\n",
      "Epoch: 0036 cost = 0.248623684\n",
      "Epoch: 0037 cost = 0.243427575\n",
      "Epoch: 0038 cost = 0.249057829\n",
      "Epoch: 0039 cost = 0.245069772\n",
      "Epoch: 0040 cost = 0.243815884\n",
      "Epoch: 0041 cost = 0.243725270\n",
      "Epoch: 0042 cost = 0.241890177\n",
      "Epoch: 0043 cost = 0.241046011\n",
      "Epoch: 0044 cost = 0.243944034\n",
      "Epoch: 0045 cost = 0.243786603\n",
      "Epoch: 0046 cost = 0.241134778\n",
      "Epoch: 0047 cost = 0.239241689\n",
      "Epoch: 0048 cost = 0.239806041\n",
      "Epoch: 0049 cost = 0.240063980\n",
      "Epoch: 0050 cost = 0.238800526\n",
      "Epoch: 0051 cost = 0.240275517\n",
      "Epoch: 0052 cost = 0.239281923\n",
      "Epoch: 0053 cost = 0.238379210\n",
      "Epoch: 0054 cost = 0.236309901\n",
      "Epoch: 0055 cost = 0.239011824\n",
      "Epoch: 0056 cost = 0.241489366\n",
      "Epoch: 0057 cost = 0.237010255\n",
      "Epoch: 0058 cost = 0.236917317\n",
      "Epoch: 0059 cost = 0.242670268\n",
      "Epoch: 0060 cost = 0.240337864\n",
      "Epoch: 0061 cost = 0.236095101\n",
      "Epoch: 0062 cost = 0.236635655\n",
      "Epoch: 0063 cost = 0.236831397\n",
      "Epoch: 0064 cost = 0.235545874\n",
      "Epoch: 0065 cost = 0.236773849\n",
      "Epoch: 0066 cost = 0.234760016\n",
      "Epoch: 0067 cost = 0.233723372\n",
      "Epoch: 0068 cost = 0.239527062\n",
      "Epoch: 0069 cost = 0.234912083\n",
      "Epoch: 0070 cost = 0.233666152\n",
      "Epoch: 0071 cost = 0.236599267\n",
      "Epoch: 0072 cost = 0.235845834\n",
      "Epoch: 0073 cost = 0.231936350\n",
      "Epoch: 0074 cost = 0.236013234\n",
      "Epoch: 0075 cost = 0.236228839\n",
      "Epoch: 0076 cost = 0.234008431\n",
      "Epoch: 0077 cost = 0.233226806\n",
      "Epoch: 0078 cost = 0.233733878\n",
      "Epoch: 0079 cost = 0.236368448\n",
      "Epoch: 0080 cost = 0.231304884\n",
      "Epoch: 0081 cost = 0.234396756\n",
      "Epoch: 0082 cost = 0.236741260\n",
      "Epoch: 0083 cost = 0.234514400\n",
      "Epoch: 0084 cost = 0.230940491\n",
      "Epoch: 0085 cost = 0.228440166\n",
      "Epoch: 0086 cost = 0.230598092\n",
      "Epoch: 0087 cost = 0.229287639\n",
      "Epoch: 0088 cost = 0.229864642\n",
      "Epoch: 0089 cost = 0.226828337\n",
      "Epoch: 0090 cost = 0.231854156\n",
      "Epoch: 0091 cost = 0.232385293\n",
      "Epoch: 0092 cost = 0.230836004\n",
      "Epoch: 0093 cost = 0.229036152\n",
      "Epoch: 0094 cost = 0.231355876\n",
      "Epoch: 0095 cost = 0.226981267\n",
      "Epoch: 0096 cost = 0.228876710\n",
      "Epoch: 0097 cost = 0.229957283\n",
      "Epoch: 0098 cost = 0.231192917\n",
      "Epoch: 0099 cost = 0.229207024\n",
      "Epoch: 0100 cost = 0.225894168\n",
      "Epoch: 0101 cost = 0.225142822\n",
      "Epoch: 0102 cost = 0.226227805\n",
      "Epoch: 0103 cost = 0.226221651\n",
      "Epoch: 0104 cost = 0.223605320\n",
      "Epoch: 0105 cost = 0.223661944\n",
      "Epoch: 0106 cost = 0.228666648\n",
      "Epoch: 0107 cost = 0.225749731\n",
      "Epoch: 0108 cost = 0.223974213\n",
      "Epoch: 0109 cost = 0.224766389\n",
      "Epoch: 0110 cost = 0.220335603\n",
      "Epoch: 0111 cost = 0.227052957\n",
      "Epoch: 0112 cost = 0.224521309\n",
      "Epoch: 0113 cost = 0.220415398\n",
      "Epoch: 0114 cost = 0.223462626\n",
      "Epoch: 0115 cost = 0.222353324\n",
      "Epoch: 0116 cost = 0.224303111\n",
      "Epoch: 0117 cost = 0.220963657\n",
      "Epoch: 0118 cost = 0.219871178\n",
      "Epoch: 0119 cost = 0.219611242\n",
      "Epoch: 0120 cost = 0.224851444\n",
      "Epoch: 0121 cost = 0.222586527\n",
      "Epoch: 0122 cost = 0.224337697\n",
      "Epoch: 0123 cost = 0.222290963\n",
      "Epoch: 0124 cost = 0.221916825\n",
      "Epoch: 0125 cost = 0.219965816\n",
      "Epoch: 0126 cost = 0.223776296\n",
      "Epoch: 0127 cost = 0.217534378\n",
      "Epoch: 0128 cost = 0.221666932\n",
      "Epoch: 0129 cost = 0.220166981\n",
      "Epoch: 0130 cost = 0.217334747\n",
      "Epoch: 0131 cost = 0.218269810\n",
      "Epoch: 0132 cost = 0.224188358\n",
      "Epoch: 0133 cost = 0.220003426\n",
      "Epoch: 0134 cost = 0.220723450\n",
      "Epoch: 0135 cost = 0.219774127\n",
      "Epoch: 0136 cost = 0.218661815\n",
      "Epoch: 0137 cost = 0.216718867\n",
      "Epoch: 0138 cost = 0.219142660\n",
      "Epoch: 0139 cost = 0.217856035\n",
      "Epoch: 0140 cost = 0.217481211\n",
      "Epoch: 0141 cost = 0.221644521\n",
      "Epoch: 0142 cost = 0.217348263\n",
      "Epoch: 0143 cost = 0.217100427\n",
      "Epoch: 0144 cost = 0.214238390\n",
      "Epoch: 0145 cost = 0.219276533\n",
      "Epoch: 0146 cost = 0.215334326\n",
      "Epoch: 0147 cost = 0.221584290\n",
      "Epoch: 0148 cost = 0.214273572\n",
      "Epoch: 0149 cost = 0.217848957\n",
      "Epoch: 0150 cost = 0.219045907\n",
      "Epoch: 0151 cost = 0.212542281\n",
      "Epoch: 0152 cost = 0.214638904\n",
      "Epoch: 0153 cost = 0.215728432\n",
      "Epoch: 0154 cost = 0.217272431\n",
      "Epoch: 0155 cost = 0.217278779\n",
      "Epoch: 0156 cost = 0.216699228\n",
      "Epoch: 0157 cost = 0.211598232\n",
      "Epoch: 0158 cost = 0.214839518\n",
      "Epoch: 0159 cost = 0.212456480\n",
      "Epoch: 0160 cost = 0.214816615\n",
      "Epoch: 0161 cost = 0.213171631\n",
      "Epoch: 0162 cost = 0.212947130\n",
      "Epoch: 0163 cost = 0.210845590\n",
      "Epoch: 0164 cost = 0.216679737\n",
      "Epoch: 0165 cost = 0.210822761\n",
      "Epoch: 0166 cost = 0.213205844\n",
      "Epoch: 0167 cost = 0.210726142\n",
      "Epoch: 0168 cost = 0.213486180\n",
      "Epoch: 0169 cost = 0.213000983\n",
      "Epoch: 0170 cost = 0.210098892\n",
      "Epoch: 0171 cost = 0.211004987\n",
      "Epoch: 0172 cost = 0.214538619\n",
      "Epoch: 0173 cost = 0.209464177\n",
      "Epoch: 0174 cost = 0.210068688\n",
      "Epoch: 0175 cost = 0.210308269\n",
      "Epoch: 0176 cost = 0.210742295\n",
      "Epoch: 0177 cost = 0.210782871\n",
      "Epoch: 0178 cost = 0.212488919\n",
      "Epoch: 0179 cost = 0.208802775\n",
      "Epoch: 0180 cost = 0.211677447\n",
      "Epoch: 0181 cost = 0.211701751\n",
      "Epoch: 0182 cost = 0.209148496\n",
      "Epoch: 0183 cost = 0.207299605\n",
      "Epoch: 0184 cost = 0.208421066\n",
      "Epoch: 0185 cost = 0.209261671\n",
      "Epoch: 0186 cost = 0.210556880\n",
      "Epoch: 0187 cost = 0.206713378\n",
      "Epoch: 0188 cost = 0.207185239\n",
      "Epoch: 0189 cost = 0.204795778\n",
      "Epoch: 0190 cost = 0.205859661\n",
      "Epoch: 0191 cost = 0.208735481\n",
      "Epoch: 0192 cost = 0.205515534\n",
      "Epoch: 0193 cost = 0.204439953\n",
      "Epoch: 0194 cost = 0.207936510\n",
      "Epoch: 0195 cost = 0.209716976\n",
      "Epoch: 0196 cost = 0.208726808\n",
      "Epoch: 0197 cost = 0.206410080\n",
      "Epoch: 0198 cost = 0.204457119\n",
      "Epoch: 0199 cost = 0.212610170\n",
      "Accuracy:  0.86164623\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정 2\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_voice_model2'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './my_voice_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(582, 20)\n",
      "(582, 5)\n",
      "predict\n",
      "0    548\n",
      "1     20\n",
      "4     10\n",
      "3      3\n",
      "2      1\n",
      "dtype: int64\n",
      "Accuracy:  0.9415808\n"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(\"./test_정유경.wav\")\n",
    "\n",
    "X_test = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "\n",
    "'''\n",
    "0 정유경\n",
    "1 배철수\n",
    "2 이윤진\n",
    "3 강정윤\n",
    "4 임찬주\n",
    "'''\n",
    "label = [0 for i in range(5)]#class가 3개이니까 y_test만드는 과정\n",
    "label[0] = 1\n",
    "Y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    Y_test.append(label)\n",
    "\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))\n",
    "\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "#print(\"Label :\",sess.run(tf.argmax(Y_test,1)))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"predict\")\n",
    "print(pd.value_counts(pd.Series(sess.run(tf.argmax(hypothesis, 1),\n",
    "                                    feed_dict={X: X_test, keep_prob:1}))))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
