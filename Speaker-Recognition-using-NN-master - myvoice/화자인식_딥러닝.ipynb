{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foldername : 0 - 20 파일\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ImChanjoo\\anaconda3\\envs\\python37\\lib\\site-packages\\librosa\\core\\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
      "C:\\Users\\ImChanjoo\\anaconda3\\envs\\python37\\lib\\site-packages\\librosa\\core\\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foldername : 1 - 20 파일\n",
      "Foldername : 2 - 20 파일\n",
      "Foldername : 3 - 20 파일\n",
      "Foldername : 4 - 20 파일\n",
      "X_data : (54264, 20)\n",
      "Y_label : (54264, 5)\n",
      "5 개의 클래스!!\n",
      "X_train : (40698, 20)\n",
      "Y_train : (40698, 5)\n",
      "X_test : (13566, 20)\n",
      "Y_test : (13566, 5)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import wave\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "##### 변수 설정 부분 #####\n",
    "DATA_PATH = \"./data\"\n",
    "X_train = [] #train_data 저장할 공간\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "tf_classes = 0\n",
    "\n",
    "def load_wave_generator(path): \n",
    "       \n",
    "    batch_waves = []\n",
    "    labels = []\n",
    "    X_data = []\n",
    "    Y_label = []    \n",
    "    global X_train, X_test, Y_train, Y_test, tf_classes\n",
    "    \n",
    "    folders = os.listdir(path)\n",
    "\n",
    "    for folder in folders:\n",
    "        if not os.path.isdir(path):continue # 폴더가 아니면 continue                   \n",
    "        files = os.listdir(path+\"/\"+folder)        \n",
    "        print(\"Foldername :\",folder,\"-\",len(files),\"파일\")\n",
    "        \n",
    "        # 폴더 이름과 그 폴더에 속하는 파일 갯수 출력\n",
    "        for wav in files:\n",
    "            if not wav.endswith(\".wav\"):continue\n",
    "            else:               \n",
    "                y, sr = librosa.load(path+\"/\"+folder+\"/\"+wav)\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "              \n",
    "                X_data.extend(mfcc)\n",
    "                \n",
    "                label = [0 for i in range(len(folders))]\n",
    "                label[tf_classes] = 1\n",
    "                \n",
    "                for i in range(len(mfcc)):\n",
    "                    Y_label.append(label)\n",
    "                    \n",
    "        tf_classes = tf_classes+1\n",
    "   \n",
    "    print(\"X_data :\",np.shape(X_data))\n",
    "    print(\"Y_label :\",np.shape(Y_label))\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(np.array(X_data), np.array(Y_label))\n",
    "\n",
    "    xy = (X_train, X_test, Y_train, Y_test)\n",
    "    np.save(\"./data.npy\",xy)\n",
    "    \n",
    "\n",
    "load_wave_generator(DATA_PATH)\n",
    "\n",
    "\n",
    "print(tf_classes,\"개의 클래스!!\")\n",
    "print(\"X_train :\",np.shape(X_train))\n",
    "print(\"Y_train :\",np.shape(Y_train))\n",
    "print(\"X_test :\",np.shape(X_test))\n",
    "print(\"Y_test :\",np.shape(Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-7a0dc2f6bb36>:24: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-2-7a0dc2f6bb36>:69: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0000 cost = 1.806586623\n",
      "Epoch: 0001 cost = 1.687602818\n",
      "Epoch: 0002 cost = 1.654644847\n",
      "Epoch: 0003 cost = 1.636134982\n",
      "Epoch: 0004 cost = 1.633174717\n",
      "Epoch: 0005 cost = 1.624652088\n",
      "Epoch: 0006 cost = 1.619630754\n",
      "Epoch: 0007 cost = 1.618709266\n",
      "Epoch: 0008 cost = 1.612985969\n",
      "Epoch: 0009 cost = 1.608114600\n",
      "Epoch: 0010 cost = 1.602930188\n",
      "Epoch: 0011 cost = 1.597488165\n",
      "Epoch: 0012 cost = 1.586576939\n",
      "Epoch: 0013 cost = 1.574141026\n",
      "Epoch: 0014 cost = 1.558693826\n",
      "Epoch: 0015 cost = 1.542039454\n",
      "Epoch: 0016 cost = 1.524231613\n",
      "Epoch: 0017 cost = 1.512255609\n",
      "Epoch: 0018 cost = 1.501982450\n",
      "Epoch: 0019 cost = 1.493688047\n",
      "Epoch: 0020 cost = 1.489730060\n",
      "Epoch: 0021 cost = 1.481895506\n",
      "Epoch: 0022 cost = 1.476798892\n",
      "Epoch: 0023 cost = 1.467703462\n",
      "Epoch: 0024 cost = 1.459524930\n",
      "Epoch: 0025 cost = 1.448302865\n",
      "Epoch: 0026 cost = 1.434907556\n",
      "Epoch: 0027 cost = 1.421953976\n",
      "Epoch: 0028 cost = 1.408429682\n",
      "Epoch: 0029 cost = 1.392135024\n",
      "Epoch: 0030 cost = 1.383483708\n",
      "Epoch: 0031 cost = 1.368316948\n",
      "Epoch: 0032 cost = 1.359155595\n",
      "Epoch: 0033 cost = 1.347428560\n",
      "Epoch: 0034 cost = 1.329527438\n",
      "Epoch: 0035 cost = 1.319303215\n",
      "Epoch: 0036 cost = 1.308984816\n",
      "Epoch: 0037 cost = 1.297021449\n",
      "Epoch: 0038 cost = 1.285638094\n",
      "Epoch: 0039 cost = 1.277096868\n",
      "Epoch: 0040 cost = 1.268744171\n",
      "Epoch: 0041 cost = 1.259771645\n",
      "Epoch: 0042 cost = 1.247784972\n",
      "Epoch: 0043 cost = 1.243097126\n",
      "Epoch: 0044 cost = 1.232418120\n",
      "Epoch: 0045 cost = 1.222695053\n",
      "Epoch: 0046 cost = 1.214526296\n",
      "Epoch: 0047 cost = 1.202083826\n",
      "Epoch: 0048 cost = 1.192073286\n",
      "Epoch: 0049 cost = 1.178422093\n",
      "Epoch: 0050 cost = 1.166300058\n",
      "Epoch: 0051 cost = 1.158669174\n",
      "Epoch: 0052 cost = 1.142235816\n",
      "Epoch: 0053 cost = 1.133266389\n",
      "Epoch: 0054 cost = 1.123798966\n",
      "Epoch: 0055 cost = 1.110317588\n",
      "Epoch: 0056 cost = 1.099435329\n",
      "Epoch: 0057 cost = 1.091495156\n",
      "Epoch: 0058 cost = 1.077876747\n",
      "Epoch: 0059 cost = 1.062335551\n",
      "Epoch: 0060 cost = 1.051617026\n",
      "Epoch: 0061 cost = 1.040310144\n",
      "Epoch: 0062 cost = 1.028918862\n",
      "Epoch: 0063 cost = 1.017952085\n",
      "Epoch: 0064 cost = 1.003551543\n",
      "Epoch: 0065 cost = 0.990894020\n",
      "Epoch: 0066 cost = 0.980480641\n",
      "Epoch: 0067 cost = 0.967091978\n",
      "Epoch: 0068 cost = 0.949847221\n",
      "Epoch: 0069 cost = 0.940612406\n",
      "Epoch: 0070 cost = 0.919522196\n",
      "Epoch: 0071 cost = 0.915820777\n",
      "Epoch: 0072 cost = 0.898765504\n",
      "Epoch: 0073 cost = 0.889249504\n",
      "Epoch: 0074 cost = 0.871465147\n",
      "Epoch: 0075 cost = 0.859364718\n",
      "Epoch: 0076 cost = 0.847164154\n",
      "Epoch: 0077 cost = 0.837394148\n",
      "Epoch: 0078 cost = 0.819781750\n",
      "Epoch: 0079 cost = 0.816607237\n",
      "Epoch: 0080 cost = 0.796865940\n",
      "Epoch: 0081 cost = 0.788878828\n",
      "Epoch: 0082 cost = 0.775948018\n",
      "Epoch: 0083 cost = 0.758961856\n",
      "Epoch: 0084 cost = 0.748737693\n",
      "Epoch: 0085 cost = 0.733174443\n",
      "Epoch: 0086 cost = 0.721203744\n",
      "Epoch: 0087 cost = 0.705843806\n",
      "Epoch: 0088 cost = 0.692492932\n",
      "Epoch: 0089 cost = 0.678958535\n",
      "Epoch: 0090 cost = 0.667820066\n",
      "Epoch: 0091 cost = 0.656593591\n",
      "Epoch: 0092 cost = 0.645584434\n",
      "Epoch: 0093 cost = 0.627814561\n",
      "Epoch: 0094 cost = 0.624582797\n",
      "Epoch: 0095 cost = 0.610658795\n",
      "Epoch: 0096 cost = 0.598444104\n",
      "Epoch: 0097 cost = 0.591318160\n",
      "Epoch: 0098 cost = 0.581058472\n",
      "Epoch: 0099 cost = 0.572599292\n",
      "Epoch: 0100 cost = 0.562711984\n",
      "Epoch: 0101 cost = 0.556749791\n",
      "Epoch: 0102 cost = 0.549794942\n",
      "Epoch: 0103 cost = 0.544864655\n",
      "Epoch: 0104 cost = 0.538049608\n",
      "Epoch: 0105 cost = 0.531752408\n",
      "Epoch: 0106 cost = 0.522219121\n",
      "Epoch: 0107 cost = 0.521453053\n",
      "Epoch: 0108 cost = 0.515935749\n",
      "Epoch: 0109 cost = 0.511743486\n",
      "Epoch: 0110 cost = 0.502653137\n",
      "Epoch: 0111 cost = 0.501039892\n",
      "Epoch: 0112 cost = 0.492492795\n",
      "Epoch: 0113 cost = 0.493091881\n",
      "Epoch: 0114 cost = 0.484930709\n",
      "Epoch: 0115 cost = 0.480850324\n",
      "Epoch: 0116 cost = 0.476835489\n",
      "Epoch: 0117 cost = 0.467587501\n",
      "Epoch: 0118 cost = 0.468854025\n",
      "Epoch: 0119 cost = 0.464151904\n",
      "Epoch: 0120 cost = 0.461465657\n",
      "Epoch: 0121 cost = 0.457891628\n",
      "Epoch: 0122 cost = 0.449641302\n",
      "Epoch: 0123 cost = 0.451872304\n",
      "Epoch: 0124 cost = 0.449099585\n",
      "Epoch: 0125 cost = 0.447867051\n",
      "Epoch: 0126 cost = 0.440448359\n",
      "Epoch: 0127 cost = 0.439954773\n",
      "Epoch: 0128 cost = 0.434656888\n",
      "Epoch: 0129 cost = 0.431435958\n",
      "Epoch: 0130 cost = 0.429933801\n",
      "Epoch: 0131 cost = 0.426661193\n",
      "Epoch: 0132 cost = 0.424133554\n",
      "Epoch: 0133 cost = 0.420382693\n",
      "Epoch: 0134 cost = 0.419797674\n",
      "Epoch: 0135 cost = 0.414845720\n",
      "Epoch: 0136 cost = 0.413727656\n",
      "Epoch: 0137 cost = 0.410090357\n",
      "Epoch: 0138 cost = 0.412427157\n",
      "Epoch: 0139 cost = 0.409673646\n",
      "Epoch: 0140 cost = 0.402995631\n",
      "Epoch: 0141 cost = 0.408020049\n",
      "Epoch: 0142 cost = 0.399347693\n",
      "Epoch: 0143 cost = 0.401395217\n",
      "Epoch: 0144 cost = 0.392976284\n",
      "Epoch: 0145 cost = 0.394675985\n",
      "Epoch: 0146 cost = 0.394750372\n",
      "Epoch: 0147 cost = 0.391669348\n",
      "Epoch: 0148 cost = 0.389320016\n",
      "Epoch: 0149 cost = 0.385953158\n",
      "Epoch: 0150 cost = 0.384917080\n",
      "Epoch: 0151 cost = 0.385617673\n",
      "Epoch: 0152 cost = 0.385571450\n",
      "Epoch: 0153 cost = 0.382608771\n",
      "Epoch: 0154 cost = 0.380897805\n",
      "Epoch: 0155 cost = 0.378072202\n",
      "Epoch: 0156 cost = 0.378536686\n",
      "Epoch: 0157 cost = 0.377526045\n",
      "Epoch: 0158 cost = 0.375758588\n",
      "Epoch: 0159 cost = 0.368656546\n",
      "Epoch: 0160 cost = 0.374555483\n",
      "Epoch: 0161 cost = 0.368552655\n",
      "Epoch: 0162 cost = 0.366594344\n",
      "Epoch: 0163 cost = 0.361852720\n",
      "Epoch: 0164 cost = 0.364066124\n",
      "Epoch: 0165 cost = 0.364287958\n",
      "Epoch: 0166 cost = 0.358543873\n",
      "Epoch: 0167 cost = 0.359081253\n",
      "Epoch: 0168 cost = 0.354737222\n",
      "Epoch: 0169 cost = 0.356977835\n",
      "Epoch: 0170 cost = 0.361513510\n",
      "Epoch: 0171 cost = 0.355844349\n",
      "Epoch: 0172 cost = 0.354209974\n",
      "Epoch: 0173 cost = 0.351402923\n",
      "Epoch: 0174 cost = 0.353360415\n",
      "Epoch: 0175 cost = 0.351002410\n",
      "Epoch: 0176 cost = 0.344968691\n",
      "Epoch: 0177 cost = 0.346329406\n",
      "Epoch: 0178 cost = 0.347368225\n",
      "Epoch: 0179 cost = 0.345707059\n",
      "Epoch: 0180 cost = 0.346653640\n",
      "Epoch: 0181 cost = 0.341440096\n",
      "Epoch: 0182 cost = 0.339573383\n",
      "Epoch: 0183 cost = 0.340827614\n",
      "Epoch: 0184 cost = 0.336227626\n",
      "Epoch: 0185 cost = 0.335160449\n",
      "Epoch: 0186 cost = 0.331878066\n",
      "Epoch: 0187 cost = 0.332731277\n",
      "Epoch: 0188 cost = 0.335972339\n",
      "Epoch: 0189 cost = 0.333109915\n",
      "Epoch: 0190 cost = 0.334157348\n",
      "Epoch: 0191 cost = 0.330601886\n",
      "Epoch: 0192 cost = 0.327885553\n",
      "Epoch: 0193 cost = 0.331867158\n",
      "Epoch: 0194 cost = 0.324936882\n",
      "Epoch: 0195 cost = 0.328931838\n",
      "Epoch: 0196 cost = 0.325523168\n",
      "Epoch: 0197 cost = 0.323650941\n",
      "Epoch: 0198 cost = 0.321279094\n",
      "Epoch: 0199 cost = 0.323271766\n",
      "Accuracy:  0.90483564\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "##################  화자인식 NN 버전 ##################\n",
    "X_train, X_test, Y_train, Y_test = np.load(\"./data.npy\")\n",
    "X_train = X_train.astype(\"float\")\n",
    "X_test = X_test.astype(\"float\")\n",
    "\n",
    "# v1\n",
    "tf.reset_default_graph() # 기존에 생성된 graph를 모두 삭제하고, reset시켜 중복되는 것을 막아준다. \n",
    "                         # context가 유지되는 주피터에서는 사용해야한다.\n",
    "tf.set_random_seed(777)\n",
    "learning_rate = 0.001\n",
    "training_epochs = 200\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "sd = 1 / np.sqrt(20) # standard deviation 표준편차(표본표준편차라 1/root(n))\n",
    "\n",
    "#mfcc의 기본은 20\n",
    "# 20ms일 때216은 각 mfcc feature의 열이 216\n",
    "X = tf.placeholder(tf.float32, [None, 20])\n",
    "Y = tf.placeholder(tf.float32, [None, tf_classes])\n",
    "\n",
    "#1차 히든레이어\n",
    "W1 = tf.get_variable(\"w1\",shape=[20, 256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b1\")\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1) # 1차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "# 2차 히든 레이어\n",
    "W2 = tf.get_variable(\"w2\", shape=[256, 256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b2\")\n",
    "L2 = tf.nn.tanh(tf.matmul(L1, W2) + b2) # 2차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "\n",
    "# 3차 히든 레이어\n",
    "W3 = tf.get_variable(\"w3\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256], mean=0, stddev=sd), name=\"b3\")\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3) # 3차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "# 4차 히든 레이어\n",
    "W4 = tf.get_variable(\"w4\", shape=[256, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b4\")\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4) # 4차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
    "\n",
    "# 5차 히든 레이어\n",
    "W5 = tf.get_variable(\"w5\", shape=[128, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b5\")\n",
    "L5 = tf.nn.relu(tf.matmul(L4, W5) + b5) # 5차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L5 = tf.nn.dropout(L5, keep_prob = keep_prob)\n",
    "\n",
    "# 6차 히든 레이어\n",
    "W6 = tf.get_variable(\"w6\", shape=[128, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b6\")\n",
    "L6 = tf.nn.relu(tf.matmul(L5, W6) + b6) # 6차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L6 = tf.nn.dropout(L6, keep_prob = keep_prob)\n",
    "\n",
    "# 7차 히든 레이어\n",
    "W7 = tf.get_variable(\"w7\", shape=[128, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([128], mean=0, stddev=sd), name=\"b7\")\n",
    "L7 = tf.nn.relu(tf.matmul(L6, W7) + b7) # 7차 히든레이어는 'Relu' 함수를 쓴다.\n",
    "L7 = tf.nn.dropout(L7, keep_prob = keep_prob)\n",
    "\n",
    "# 최종 레이어\n",
    "W8 = tf.get_variable(\"w8\", shape=[128, tf_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([tf_classes], mean=0, stddev=sd), name=\"b8\")\n",
    "hypothesis = tf.matmul(L7, W8) + b8\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "\n",
    "\n",
    "batch_size=1\n",
    "x_len = len(X_train)\n",
    "\n",
    "if(x_len%2==0):\n",
    "    batch_size = 2\n",
    "elif(x_len%3==0):\n",
    "    batch_size = 3\n",
    "elif(x_len%4==0):\n",
    "    batch_size = 4\n",
    "else:\n",
    "    batch_size = 1\n",
    "\n",
    "split_X = np.split(X_train,batch_size)\n",
    "split_Y = np.split(Y_train,batch_size)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.320977494\n",
      "Epoch: 0001 cost = 0.318241999\n",
      "Epoch: 0002 cost = 0.323594540\n",
      "Epoch: 0003 cost = 0.318467900\n",
      "Epoch: 0004 cost = 0.315076470\n",
      "Epoch: 0005 cost = 0.319164157\n",
      "Epoch: 0006 cost = 0.314166978\n",
      "Epoch: 0007 cost = 0.316449389\n",
      "Epoch: 0008 cost = 0.312153250\n",
      "Epoch: 0009 cost = 0.314399913\n",
      "Epoch: 0010 cost = 0.309893131\n",
      "Epoch: 0011 cost = 0.315916792\n",
      "Epoch: 0012 cost = 0.308318987\n",
      "Epoch: 0013 cost = 0.310395285\n",
      "Epoch: 0014 cost = 0.311116755\n",
      "Epoch: 0015 cost = 0.309693947\n",
      "Epoch: 0016 cost = 0.305347294\n",
      "Epoch: 0017 cost = 0.305240914\n",
      "Epoch: 0018 cost = 0.302133217\n",
      "Epoch: 0019 cost = 0.309332758\n",
      "Epoch: 0020 cost = 0.307028860\n",
      "Epoch: 0021 cost = 0.304714024\n",
      "Epoch: 0022 cost = 0.304688767\n",
      "Epoch: 0023 cost = 0.303506508\n",
      "Epoch: 0024 cost = 0.306443810\n",
      "Epoch: 0025 cost = 0.300378606\n",
      "Epoch: 0026 cost = 0.300659299\n",
      "Epoch: 0027 cost = 0.302543953\n",
      "Epoch: 0028 cost = 0.301655203\n",
      "Epoch: 0029 cost = 0.297092944\n",
      "Epoch: 0030 cost = 0.299972668\n",
      "Epoch: 0031 cost = 0.295625404\n",
      "Epoch: 0032 cost = 0.297424480\n",
      "Epoch: 0033 cost = 0.292066917\n",
      "Epoch: 0034 cost = 0.295803457\n",
      "Epoch: 0035 cost = 0.293659970\n",
      "Epoch: 0036 cost = 0.293091774\n",
      "Epoch: 0037 cost = 0.293876082\n",
      "Epoch: 0038 cost = 0.289044559\n",
      "Epoch: 0039 cost = 0.290264517\n",
      "Epoch: 0040 cost = 0.293097422\n",
      "Epoch: 0041 cost = 0.291857556\n",
      "Epoch: 0042 cost = 0.286551923\n",
      "Epoch: 0043 cost = 0.287036031\n",
      "Epoch: 0044 cost = 0.288283989\n",
      "Epoch: 0045 cost = 0.286258757\n",
      "Epoch: 0046 cost = 0.287584484\n",
      "Epoch: 0047 cost = 0.284541070\n",
      "Epoch: 0048 cost = 0.286127746\n",
      "Epoch: 0049 cost = 0.283847794\n",
      "Epoch: 0050 cost = 0.282065481\n",
      "Epoch: 0051 cost = 0.284723788\n",
      "Epoch: 0052 cost = 0.283841610\n",
      "Epoch: 0053 cost = 0.278052717\n",
      "Epoch: 0054 cost = 0.282759607\n",
      "Epoch: 0055 cost = 0.282229632\n",
      "Epoch: 0056 cost = 0.278601393\n",
      "Epoch: 0057 cost = 0.280623242\n",
      "Epoch: 0058 cost = 0.280775592\n",
      "Epoch: 0059 cost = 0.281560257\n",
      "Epoch: 0060 cost = 0.276640654\n",
      "Epoch: 0061 cost = 0.273970395\n",
      "Epoch: 0062 cost = 0.276194856\n",
      "Epoch: 0063 cost = 0.274667025\n",
      "Epoch: 0064 cost = 0.278802007\n",
      "Epoch: 0065 cost = 0.274922282\n",
      "Epoch: 0066 cost = 0.273649693\n",
      "Epoch: 0067 cost = 0.275733799\n",
      "Epoch: 0068 cost = 0.271306247\n",
      "Epoch: 0069 cost = 0.270815820\n",
      "Epoch: 0070 cost = 0.269252092\n",
      "Epoch: 0071 cost = 0.268883571\n",
      "Epoch: 0072 cost = 0.268456027\n",
      "Epoch: 0073 cost = 0.270412594\n",
      "Epoch: 0074 cost = 0.270061493\n",
      "Epoch: 0075 cost = 0.267245322\n",
      "Epoch: 0076 cost = 0.269388273\n",
      "Epoch: 0077 cost = 0.268105224\n",
      "Epoch: 0078 cost = 0.270883024\n",
      "Epoch: 0079 cost = 0.269449368\n",
      "Epoch: 0080 cost = 0.268303916\n",
      "Epoch: 0081 cost = 0.266601756\n",
      "Epoch: 0082 cost = 0.264969647\n",
      "Epoch: 0083 cost = 0.267623752\n",
      "Epoch: 0084 cost = 0.262130961\n",
      "Epoch: 0085 cost = 0.264881164\n",
      "Epoch: 0086 cost = 0.268965706\n",
      "Epoch: 0087 cost = 0.262101054\n",
      "Epoch: 0088 cost = 0.263831392\n",
      "Epoch: 0089 cost = 0.264739528\n",
      "Epoch: 0090 cost = 0.260143846\n",
      "Epoch: 0091 cost = 0.259351388\n",
      "Epoch: 0092 cost = 0.261766434\n",
      "Epoch: 0093 cost = 0.260784447\n",
      "Epoch: 0094 cost = 0.259178758\n",
      "Epoch: 0095 cost = 0.261777773\n",
      "Epoch: 0096 cost = 0.256396532\n",
      "Epoch: 0097 cost = 0.259325445\n",
      "Epoch: 0098 cost = 0.256977469\n",
      "Epoch: 0099 cost = 0.261497140\n",
      "Epoch: 0100 cost = 0.254486866\n",
      "Epoch: 0101 cost = 0.256518945\n",
      "Epoch: 0102 cost = 0.256845251\n",
      "Epoch: 0103 cost = 0.252083741\n",
      "Epoch: 0104 cost = 0.259930521\n",
      "Epoch: 0105 cost = 0.252837807\n",
      "Epoch: 0106 cost = 0.253500462\n",
      "Epoch: 0107 cost = 0.258815482\n",
      "Epoch: 0108 cost = 0.253686421\n",
      "Epoch: 0109 cost = 0.255602971\n",
      "Epoch: 0110 cost = 0.250999913\n",
      "Epoch: 0111 cost = 0.248386361\n",
      "Epoch: 0112 cost = 0.254159272\n",
      "Epoch: 0113 cost = 0.252088182\n",
      "Epoch: 0114 cost = 0.251842044\n",
      "Epoch: 0115 cost = 0.253053144\n",
      "Epoch: 0116 cost = 0.249239117\n",
      "Epoch: 0117 cost = 0.250276648\n",
      "Epoch: 0118 cost = 0.249994703\n",
      "Epoch: 0119 cost = 0.251318529\n",
      "Epoch: 0120 cost = 0.247504897\n",
      "Epoch: 0121 cost = 0.250161238\n",
      "Epoch: 0122 cost = 0.246672459\n",
      "Epoch: 0123 cost = 0.249853119\n",
      "Epoch: 0124 cost = 0.247323997\n",
      "Epoch: 0125 cost = 0.253104329\n",
      "Epoch: 0126 cost = 0.249085926\n",
      "Epoch: 0127 cost = 0.245508023\n",
      "Epoch: 0128 cost = 0.244609013\n",
      "Epoch: 0129 cost = 0.244316727\n",
      "Epoch: 0130 cost = 0.245668806\n",
      "Epoch: 0131 cost = 0.244728021\n",
      "Epoch: 0132 cost = 0.244715750\n",
      "Epoch: 0133 cost = 0.246106178\n",
      "Epoch: 0134 cost = 0.244513862\n",
      "Epoch: 0135 cost = 0.244618408\n",
      "Epoch: 0136 cost = 0.246144965\n",
      "Epoch: 0137 cost = 0.245233454\n",
      "Epoch: 0138 cost = 0.245651178\n",
      "Epoch: 0139 cost = 0.241401687\n",
      "Epoch: 0140 cost = 0.242586255\n",
      "Epoch: 0141 cost = 0.245295517\n",
      "Epoch: 0142 cost = 0.243252568\n",
      "Epoch: 0143 cost = 0.238655537\n",
      "Epoch: 0144 cost = 0.243893944\n",
      "Epoch: 0145 cost = 0.240518853\n",
      "Epoch: 0146 cost = 0.240528308\n",
      "Epoch: 0147 cost = 0.240043804\n",
      "Epoch: 0148 cost = 0.236901842\n",
      "Epoch: 0149 cost = 0.236115739\n",
      "Epoch: 0150 cost = 0.241616435\n",
      "Epoch: 0151 cost = 0.237698548\n",
      "Epoch: 0152 cost = 0.235345967\n",
      "Epoch: 0153 cost = 0.233989097\n",
      "Epoch: 0154 cost = 0.234433465\n",
      "Epoch: 0155 cost = 0.242365330\n",
      "Epoch: 0156 cost = 0.239215724\n",
      "Epoch: 0157 cost = 0.234569564\n",
      "Epoch: 0158 cost = 0.239926167\n",
      "Epoch: 0159 cost = 0.237772696\n",
      "Epoch: 0160 cost = 0.235962346\n",
      "Epoch: 0161 cost = 0.233089641\n",
      "Epoch: 0162 cost = 0.231082663\n",
      "Epoch: 0163 cost = 0.234619692\n",
      "Epoch: 0164 cost = 0.235549450\n",
      "Epoch: 0165 cost = 0.235326476\n",
      "Epoch: 0166 cost = 0.231084809\n",
      "Epoch: 0167 cost = 0.232441589\n",
      "Epoch: 0168 cost = 0.232048333\n",
      "Epoch: 0169 cost = 0.235029764\n",
      "Epoch: 0170 cost = 0.231718160\n",
      "Epoch: 0171 cost = 0.231642365\n",
      "Epoch: 0172 cost = 0.232700258\n",
      "Epoch: 0173 cost = 0.236289315\n",
      "Epoch: 0174 cost = 0.234251522\n",
      "Epoch: 0175 cost = 0.228579700\n",
      "Epoch: 0176 cost = 0.228249334\n",
      "Epoch: 0177 cost = 0.231502831\n",
      "Epoch: 0178 cost = 0.232758999\n",
      "Epoch: 0179 cost = 0.229546539\n",
      "Epoch: 0180 cost = 0.225036554\n",
      "Epoch: 0181 cost = 0.233616494\n",
      "Epoch: 0182 cost = 0.229937039\n",
      "Epoch: 0183 cost = 0.229935303\n",
      "Epoch: 0184 cost = 0.228026867\n",
      "Epoch: 0185 cost = 0.230019048\n",
      "Epoch: 0186 cost = 0.229311287\n",
      "Epoch: 0187 cost = 0.228398815\n",
      "Epoch: 0188 cost = 0.226145521\n",
      "Epoch: 0189 cost = 0.226677440\n",
      "Epoch: 0190 cost = 0.227215841\n",
      "Epoch: 0191 cost = 0.228275731\n",
      "Epoch: 0192 cost = 0.225191131\n",
      "Epoch: 0193 cost = 0.225017063\n",
      "Epoch: 0194 cost = 0.222178556\n",
      "Epoch: 0195 cost = 0.224642314\n",
      "Epoch: 0196 cost = 0.229636580\n",
      "Epoch: 0197 cost = 0.223722957\n",
      "Epoch: 0198 cost = 0.224878721\n",
      "Epoch: 0199 cost = 0.225517213\n",
      "Accuracy:  0.9226006\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 cost = 0.225638971\n",
      "Epoch: 0001 cost = 0.222127765\n",
      "Epoch: 0002 cost = 0.226730965\n",
      "Epoch: 0003 cost = 0.226748623\n",
      "Epoch: 0004 cost = 0.223086998\n",
      "Epoch: 0005 cost = 0.222402737\n",
      "Epoch: 0006 cost = 0.223859757\n",
      "Epoch: 0007 cost = 0.222078249\n",
      "Epoch: 0008 cost = 0.221331477\n",
      "Epoch: 0009 cost = 0.218945146\n",
      "Epoch: 0010 cost = 0.222047679\n",
      "Epoch: 0011 cost = 0.220785998\n",
      "Epoch: 0012 cost = 0.221992075\n",
      "Epoch: 0013 cost = 0.218559809\n",
      "Epoch: 0014 cost = 0.222764201\n",
      "Epoch: 0015 cost = 0.219104469\n",
      "Epoch: 0016 cost = 0.223196648\n",
      "Epoch: 0017 cost = 0.217967391\n",
      "Epoch: 0018 cost = 0.223534048\n",
      "Epoch: 0019 cost = 0.223019280\n",
      "Epoch: 0020 cost = 0.219170161\n",
      "Epoch: 0021 cost = 0.219099492\n",
      "Epoch: 0022 cost = 0.220327310\n",
      "Epoch: 0023 cost = 0.220915020\n",
      "Epoch: 0024 cost = 0.215582646\n",
      "Epoch: 0025 cost = 0.216632172\n",
      "Epoch: 0026 cost = 0.217530042\n",
      "Epoch: 0027 cost = 0.217409648\n",
      "Epoch: 0028 cost = 0.215097569\n",
      "Epoch: 0029 cost = 0.218768761\n",
      "Epoch: 0030 cost = 0.218093969\n",
      "Epoch: 0031 cost = 0.215680078\n",
      "Epoch: 0032 cost = 0.220194280\n",
      "Epoch: 0033 cost = 0.213660106\n",
      "Epoch: 0034 cost = 0.211005740\n",
      "Epoch: 0035 cost = 0.213891864\n",
      "Epoch: 0036 cost = 0.217765711\n",
      "Epoch: 0037 cost = 0.214671835\n",
      "Epoch: 0038 cost = 0.214583524\n",
      "Epoch: 0039 cost = 0.223978184\n",
      "Epoch: 0040 cost = 0.217155650\n",
      "Epoch: 0041 cost = 0.215793639\n",
      "Epoch: 0042 cost = 0.216822587\n",
      "Epoch: 0043 cost = 0.213224508\n",
      "Epoch: 0044 cost = 0.219042443\n",
      "Epoch: 0045 cost = 0.215280548\n",
      "Epoch: 0046 cost = 0.214680791\n",
      "Epoch: 0047 cost = 0.209628463\n",
      "Epoch: 0048 cost = 0.213021755\n",
      "Epoch: 0049 cost = 0.215218924\n",
      "Epoch: 0050 cost = 0.214120768\n",
      "Epoch: 0051 cost = 0.212495714\n",
      "Epoch: 0052 cost = 0.208806090\n",
      "Epoch: 0053 cost = 0.212101668\n",
      "Epoch: 0054 cost = 0.216583535\n",
      "Epoch: 0055 cost = 0.213735886\n",
      "Epoch: 0056 cost = 0.211774282\n",
      "Epoch: 0057 cost = 0.212541901\n",
      "Epoch: 0058 cost = 0.208690904\n",
      "Epoch: 0059 cost = 0.209443934\n",
      "Epoch: 0060 cost = 0.212400295\n",
      "Epoch: 0061 cost = 0.209171697\n",
      "Epoch: 0062 cost = 0.212241314\n",
      "Epoch: 0063 cost = 0.210719146\n",
      "Epoch: 0064 cost = 0.213728197\n",
      "Epoch: 0065 cost = 0.210676856\n",
      "Epoch: 0066 cost = 0.206862494\n",
      "Epoch: 0067 cost = 0.208216548\n",
      "Epoch: 0068 cost = 0.212838888\n",
      "Epoch: 0069 cost = 0.212328590\n",
      "Epoch: 0070 cost = 0.214183941\n",
      "Epoch: 0071 cost = 0.207663208\n",
      "Epoch: 0072 cost = 0.212165549\n",
      "Epoch: 0073 cost = 0.208732672\n",
      "Epoch: 0074 cost = 0.208647832\n",
      "Epoch: 0075 cost = 0.207956284\n",
      "Epoch: 0076 cost = 0.207953602\n",
      "Epoch: 0077 cost = 0.204183951\n",
      "Epoch: 0078 cost = 0.205883250\n",
      "Epoch: 0079 cost = 0.205109559\n",
      "Epoch: 0080 cost = 0.204016328\n",
      "Epoch: 0081 cost = 0.202875964\n",
      "Epoch: 0082 cost = 0.208398715\n",
      "Epoch: 0083 cost = 0.207608625\n",
      "Epoch: 0084 cost = 0.204909228\n",
      "Epoch: 0085 cost = 0.207166925\n",
      "Epoch: 0086 cost = 0.203745857\n",
      "Epoch: 0087 cost = 0.207587235\n",
      "Epoch: 0088 cost = 0.204538696\n",
      "Epoch: 0089 cost = 0.202821627\n",
      "Epoch: 0090 cost = 0.206762217\n",
      "Epoch: 0091 cost = 0.207745366\n",
      "Epoch: 0092 cost = 0.201600827\n",
      "Epoch: 0093 cost = 0.206313454\n",
      "Epoch: 0094 cost = 0.208559468\n",
      "Epoch: 0095 cost = 0.205172837\n",
      "Epoch: 0096 cost = 0.207291961\n",
      "Epoch: 0097 cost = 0.206194438\n",
      "Epoch: 0098 cost = 0.201647855\n",
      "Epoch: 0099 cost = 0.198764458\n",
      "Epoch: 0100 cost = 0.200473994\n",
      "Epoch: 0101 cost = 0.205815725\n",
      "Epoch: 0102 cost = 0.203275137\n",
      "Epoch: 0103 cost = 0.200281300\n",
      "Epoch: 0104 cost = 0.203867838\n",
      "Epoch: 0105 cost = 0.205078185\n",
      "Epoch: 0106 cost = 0.202532940\n",
      "Epoch: 0107 cost = 0.198485300\n",
      "Epoch: 0108 cost = 0.200141028\n",
      "Epoch: 0109 cost = 0.200523071\n",
      "Epoch: 0110 cost = 0.200618401\n",
      "Epoch: 0111 cost = 0.197679706\n",
      "Epoch: 0112 cost = 0.200693250\n",
      "Epoch: 0113 cost = 0.200377382\n",
      "Epoch: 0114 cost = 0.203693658\n",
      "Epoch: 0115 cost = 0.202749923\n",
      "Epoch: 0116 cost = 0.198951796\n",
      "Epoch: 0117 cost = 0.197679989\n",
      "Epoch: 0118 cost = 0.198569626\n",
      "Epoch: 0119 cost = 0.199463092\n",
      "Epoch: 0120 cost = 0.199079044\n",
      "Epoch: 0121 cost = 0.200938292\n",
      "Epoch: 0122 cost = 0.199075617\n",
      "Epoch: 0123 cost = 0.198905692\n",
      "Epoch: 0124 cost = 0.200880654\n",
      "Epoch: 0125 cost = 0.199295431\n",
      "Epoch: 0126 cost = 0.199450344\n",
      "Epoch: 0127 cost = 0.202781029\n",
      "Epoch: 0128 cost = 0.201090820\n",
      "Epoch: 0129 cost = 0.196459644\n",
      "Epoch: 0130 cost = 0.200255558\n",
      "Epoch: 0131 cost = 0.201239645\n",
      "Epoch: 0132 cost = 0.197754890\n",
      "Epoch: 0133 cost = 0.200913087\n",
      "Epoch: 0134 cost = 0.197491340\n",
      "Epoch: 0135 cost = 0.199848853\n",
      "Epoch: 0136 cost = 0.198538087\n",
      "Epoch: 0137 cost = 0.197775222\n",
      "Epoch: 0138 cost = 0.196446858\n",
      "Epoch: 0139 cost = 0.198477484\n",
      "Epoch: 0140 cost = 0.196307503\n",
      "Epoch: 0141 cost = 0.193723179\n",
      "Epoch: 0142 cost = 0.194577642\n",
      "Epoch: 0143 cost = 0.196311742\n",
      "Epoch: 0144 cost = 0.196712963\n",
      "Epoch: 0145 cost = 0.195416406\n",
      "Epoch: 0146 cost = 0.193490341\n",
      "Epoch: 0147 cost = 0.191966154\n",
      "Epoch: 0148 cost = 0.195732646\n",
      "Epoch: 0149 cost = 0.189874738\n",
      "Epoch: 0150 cost = 0.193681911\n",
      "Epoch: 0151 cost = 0.190231964\n",
      "Epoch: 0152 cost = 0.197535142\n",
      "Epoch: 0153 cost = 0.191507295\n",
      "Epoch: 0154 cost = 0.191977620\n",
      "Epoch: 0155 cost = 0.188867927\n",
      "Epoch: 0156 cost = 0.190775320\n",
      "Epoch: 0157 cost = 0.193675078\n",
      "Epoch: 0158 cost = 0.190402620\n",
      "Epoch: 0159 cost = 0.193261608\n",
      "Epoch: 0160 cost = 0.198754132\n",
      "Epoch: 0161 cost = 0.195599690\n",
      "Epoch: 0162 cost = 0.193851523\n",
      "Epoch: 0163 cost = 0.191355579\n",
      "Epoch: 0164 cost = 0.190091401\n",
      "Epoch: 0165 cost = 0.189342797\n",
      "Epoch: 0166 cost = 0.191517219\n",
      "Epoch: 0167 cost = 0.190268271\n",
      "Epoch: 0168 cost = 0.189187460\n",
      "Epoch: 0169 cost = 0.188353896\n",
      "Epoch: 0170 cost = 0.190976866\n",
      "Epoch: 0171 cost = 0.192244783\n",
      "Epoch: 0172 cost = 0.193696119\n",
      "Epoch: 0173 cost = 0.191652089\n",
      "Epoch: 0174 cost = 0.193479799\n",
      "Epoch: 0175 cost = 0.191894539\n",
      "Epoch: 0176 cost = 0.190123215\n",
      "Epoch: 0177 cost = 0.190561116\n",
      "Epoch: 0178 cost = 0.191584371\n",
      "Epoch: 0179 cost = 0.193527155\n",
      "Epoch: 0180 cost = 0.187635012\n",
      "Epoch: 0181 cost = 0.188682623\n",
      "Epoch: 0182 cost = 0.192737445\n",
      "Epoch: 0183 cost = 0.193559565\n",
      "Epoch: 0184 cost = 0.189184472\n",
      "Epoch: 0185 cost = 0.186402738\n",
      "Epoch: 0186 cost = 0.189597867\n",
      "Epoch: 0187 cost = 0.187114365\n",
      "Epoch: 0188 cost = 0.189212225\n",
      "Epoch: 0189 cost = 0.191010766\n",
      "Epoch: 0190 cost = 0.189854965\n",
      "Epoch: 0191 cost = 0.189570919\n",
      "Epoch: 0192 cost = 0.188446857\n",
      "Epoch: 0193 cost = 0.186986461\n",
      "Epoch: 0194 cost = 0.192102075\n",
      "Epoch: 0195 cost = 0.188737914\n",
      "Epoch: 0196 cost = 0.184632607\n",
      "Epoch: 0197 cost = 0.189664878\n",
      "Epoch: 0198 cost = 0.188511714\n",
      "Epoch: 0199 cost = 0.183983162\n",
      "Accuracy:  0.928424\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "#학습만 반복 코스트 보며 설정 2\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(batch_size):\n",
    "        batch_xs = split_X[i]\n",
    "        batch_ys = split_Y[i]\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / batch_size\n",
    "        #if(epoch%10==0):\n",
    "    print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_voice_model2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './my_voice_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ImChanjoo\\anaconda3\\envs\\python37\\lib\\site-packages\\librosa\\core\\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(533, 20)\n",
      "(533, 5)\n",
      "predict\n",
      "2    521\n",
      "1      7\n",
      "3      3\n",
      "4      2\n",
      "dtype: int64\n",
      "Accuracy:  0.97748595\n"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(\"./test_이윤진.wav\")\n",
    "\n",
    "X_test = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=int(sr*0.01),n_fft=int(sr*0.02)).T\n",
    "\n",
    "'''\n",
    "0 정유경\n",
    "1 배철수\n",
    "2 이윤진\n",
    "3 강정윤\n",
    "4 임찬주\n",
    "'''\n",
    "label = [0 for i in range(5)]#class가 3개이니까 y_test만드는 과정\n",
    "label[2] = 1\n",
    "Y_test = []\n",
    "for i in range(len(X_test)):\n",
    "    Y_test.append(label)\n",
    "\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))\n",
    "\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n",
    "#print(\"Label :\",sess.run(tf.argmax(Y_test,1)))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"predict\")\n",
    "print(pd.value_counts(pd.Series(sess.run(tf.argmax(hypothesis, 1),\n",
    "                                    feed_dict={X: X_test, keep_prob:1}))))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: X_test, Y:Y_test, keep_prob:1}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
